<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_113.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><span>[114]</span><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_115.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit d4b06c2d4cce466e2d62163c0a954e1b2ce96f8b
Author: Nickolai Zeldovich &lt;nickolai@csail.mit.edu&gt;
Date:   Sat Dec 15 06:34:37 2012 -0500

    kvm: fix i8254 counter 0 wraparound
    
    The kvm i8254 emulation for counter 0 (but not for counters 1 and 2)
    has at least two bugs in mode 0:
    
    1. The OUT bit, computed by pit_get_out(), is never set high.
    
    2. The counter value, computed by pit_get_count(), wraps back around to
       the initial counter value, rather than wrapping back to 0xFFFF
       (which is the behavior described in the comment in __kpit_elapsed,
       the behavior implemented by qemu, and the behavior observed on AMD
       hardware).
    
    The bug stems from __kpit_elapsed computing the elapsed time mod the
    initial counter value (stored as nanoseconds in ps-&gt;period).  This is both
    unnecessary (none of the callers of kpit_elapsed expect the value to be
    at most the initial counter value) and incorrect (it causes pit_get_count
    to appear to wrap around to the initial counter value rather than 0xFFFF).
    Removing this mod from __kpit_elapsed fixes both of the above bugs.
    
    Signed-off-by: Nickolai Zeldovich &lt;nickolai@csail.mit.edu&gt;
    Reviewed-by: Marcelo Tosatti &lt;mtosatti@redhat.com&gt;
    Signed-off-by: Gleb Natapov &lt;gleb@redhat.com&gt;

diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index 11300d2fa714..c1d30b2fc9bb 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -122,7 +122,6 @@ static s64 __kpit_elapsed(struct kvm *kvm)
 	 */
 	remaining = hrtimer_get_remaining(&amp;ps-&gt;timer);
 	elapsed = ps-&gt;period - ktime_to_ns(remaining);
-	elapsed = mod_64(elapsed, ps-&gt;period);
 
 	return elapsed;
 }</pre><hr><pre>commit bd6e07e72f37f34535bec7eebc807e5fcfe37b43
Author: Ilia Mirkin &lt;imirkin@alum.mit.edu&gt;
Date:   Sun Mar 7 12:48:53 2021 -0500

    drm/nouveau/kms/nv04: use vzalloc for nv04_display
    
    The struct is giant, and triggers an order-7 allocation (512K). There is
    no reason for this to be kmalloc-type memory, so switch to vmalloc. This
    should help loading nouveau on low-memory and/or long-running systems.
    
    Reported-by: Nathan E. Egge &lt;unlord@xiph.org&gt;
    Signed-off-by: Ilia Mirkin &lt;imirkin@alum.mit.edu&gt;
    Cc: stable@vger.kernel.org
    Signed-off-by: Ben Skeggs &lt;bskeggs@redhat.com&gt;
    Reviewed-by: Karol Herbst &lt;kherbst@redhat.com&gt;
    Signed-off-by: Karol Herbst &lt;kherbst@redhat.com&gt;
    Link: https://gitlab.freedesktop.org/drm/nouveau/-/merge_requests/10

diff --git a/drivers/gpu/drm/nouveau/dispnv04/disp.c b/drivers/gpu/drm/nouveau/dispnv04/disp.c
index 7739f46470d3..99fee4d8cd31 100644
--- a/drivers/gpu/drm/nouveau/dispnv04/disp.c
+++ b/drivers/gpu/drm/nouveau/dispnv04/disp.c
@@ -205,7 +205,7 @@ nv04_display_destroy(struct drm_device *dev)
 	nvif_notify_dtor(&amp;disp-&gt;flip);
 
 	nouveau_display(dev)-&gt;priv = NULL;
-	kfree(disp);
+	vfree(disp);
 
 	nvif_object_unmap(&amp;drm-&gt;client.device.object);
 }
@@ -223,7 +223,7 @@ nv04_display_create(struct drm_device *dev)
 	struct nv04_display *disp;
 	int i, ret;
 
-	disp = kzalloc(sizeof(*disp), GFP_KERNEL);
+	disp = vzalloc(sizeof(*disp));
 	if (!disp)
 		return -ENOMEM;
 </pre><hr><pre>commit 4af0e6e39b7ed77796a41537db91d717fedd0ac3
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Wed Nov 11 11:09:46 2020 -0500

    x86/mm: Remove duplicate definition of _PAGE_PAT_LARGE
    
    _PAGE_PAT_LARGE is already defined next to _PAGE_PAT. Remove the
    duplicate.
    
    Fixes: 4efb56649132 ("x86/mm: Tabulate the page table encoding definitions")
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Borislav Petkov &lt;bp@suse.de&gt;
    Acked-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
    Link: https://lkml.kernel.org/r/20201111160946.147341-2-nivedita@alum.mit.edu

diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
index 394757ee030a..f24d7ef8fffa 100644
--- a/arch/x86/include/asm/pgtable_types.h
+++ b/arch/x86/include/asm/pgtable_types.h
@@ -177,8 +177,6 @@ enum page_cache_mode {
 #define __pgprot(x)		((pgprot_t) { (x) } )
 #define __pg(x)			__pgprot(x)
 
-#define _PAGE_PAT_LARGE		(_AT(pteval_t, 1) &lt;&lt; _PAGE_BIT_PAT_LARGE)
-
 #define PAGE_NONE	     __pg(   0|   0|   0|___A|   0|   0|   0|___G)
 #define PAGE_SHARED	     __pg(__PP|__RW|_USR|___A|__NX|   0|   0|   0)
 #define PAGE_SHARED_EXEC     __pg(__PP|__RW|_USR|___A|   0|   0|   0|   0)</pre><hr><pre>commit 29ac40cbed2bc06fa218ca25d7f5e280d3d08a25
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Wed Nov 11 11:09:45 2020 -0500

    x86/mm/mem_encrypt: Fix definition of PMD_FLAGS_DEC_WP
    
    The PAT bit is in different locations for 4k and 2M/1G page table
    entries.
    
    Add a definition for _PAGE_LARGE_CACHE_MASK to represent the three
    caching bits (PWT, PCD, PAT), similar to _PAGE_CACHE_MASK for 4k pages,
    and use it in the definition of PMD_FLAGS_DEC_WP to get the correct PAT
    index for write-protected pages.
    
    Fixes: 6ebcb060713f ("x86/mm: Add support to encrypt the kernel in-place")
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Borislav Petkov &lt;bp@suse.de&gt;
    Tested-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20201111160946.147341-1-nivedita@alum.mit.edu

diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h
index 816b31c68550..394757ee030a 100644
--- a/arch/x86/include/asm/pgtable_types.h
+++ b/arch/x86/include/asm/pgtable_types.h
@@ -155,6 +155,7 @@ enum page_cache_mode {
 #define _PAGE_ENC		(_AT(pteval_t, sme_me_mask))
 
 #define _PAGE_CACHE_MASK	(_PAGE_PWT | _PAGE_PCD | _PAGE_PAT)
+#define _PAGE_LARGE_CACHE_MASK	(_PAGE_PWT | _PAGE_PCD | _PAGE_PAT_LARGE)
 
 #define _PAGE_NOCACHE		(cachemode2protval(_PAGE_CACHE_MODE_UC))
 #define _PAGE_CACHE_WP		(cachemode2protval(_PAGE_CACHE_MODE_WP))
diff --git a/arch/x86/mm/mem_encrypt_identity.c b/arch/x86/mm/mem_encrypt_identity.c
index 733b983f3a89..6c5eb6f3f14f 100644
--- a/arch/x86/mm/mem_encrypt_identity.c
+++ b/arch/x86/mm/mem_encrypt_identity.c
@@ -45,8 +45,8 @@
 #define PMD_FLAGS_LARGE		(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)
 
 #define PMD_FLAGS_DEC		PMD_FLAGS_LARGE
-#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \
-				 (_PAGE_PAT | _PAGE_PWT))
+#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_LARGE_CACHE_MASK) | \
+				 (_PAGE_PAT_LARGE | _PAGE_PWT))
 
 #define PMD_FLAGS_ENC		(PMD_FLAGS_LARGE | _PAGE_ENC)
 </pre><hr><pre>commit 262bd5724afdefd4c48a260d6100e78cc43ee06b
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Fri Nov 27 12:13:24 2020 -0500

    x86/cpu/amd: Remove dead code for TSEG region remapping
    
    Commit
    
      26bfa5f89486 ("x86, amd: Cleanup init_amd")
    
    moved the code that remaps the TSEG region using 4k pages from
    init_amd() to bsp_init_amd().
    
    However, bsp_init_amd() is executed well before the direct mapping is
    actually created:
    
      setup_arch()
        -&gt; early_cpu_init()
          -&gt; early_identify_cpu()
            -&gt; this_cpu-&gt;c_bsp_init()
              -&gt; bsp_init_amd()
        ...
        -&gt; init_mem_mapping()
    
    So the change effectively disabled the 4k remapping, because
    pfn_range_is_mapped() is always false at this point.
    
    It has been over six years since the commit, and no-one seems to have
    noticed this, so just remove the code. The original code was also
    incomplete, since it doesn't check how large the TSEG address range
    actually is, so it might remap only part of it in any case.
    
    Hygon has copied the incorrect version, so the code has never run on it
    since the cpu support was added two years ago. Remove it from there as
    well.
    
    Committer notes:
    
    This workaround is incomplete anyway:
    
    1. The code must check MSRC001_0113.TValid (SMM TSeg Mask MSR) first, to
    check whether the TSeg address range is enabled.
    
    2. The code must check whether the range is not 2M aligned - if it is,
    there's nothing to work around.
    
    3. In all the BIOSes tested, the TSeg range is in a e820 reserved area
    and those are not mapped anymore, after
    
      66520ebc2df3 ("x86, mm: Only direct map addresses that are marked as E820_RAM")
    
    which means, there's nothing to be worked around either.
    
    So let's rip it out.
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Borislav Petkov &lt;bp@suse.de&gt;
    Link: https://lkml.kernel.org/r/20201127171324.1846019-1-nivedita@alum.mit.edu

diff --git a/arch/x86/kernel/cpu/amd.c b/arch/x86/kernel/cpu/amd.c
index 1f71c7616917..f8ca66f3d861 100644
--- a/arch/x86/kernel/cpu/amd.c
+++ b/arch/x86/kernel/cpu/amd.c
@@ -23,7 +23,6 @@
 
 #ifdef CONFIG_X86_64
 # include &lt;asm/mmconfig.h&gt;
-# include &lt;asm/set_memory.h&gt;
 #endif
 
 #include "cpu.h"
@@ -509,26 +508,6 @@ static void early_init_amd_mc(struct cpuinfo_x86 *c)
 
 static void bsp_init_amd(struct cpuinfo_x86 *c)
 {
-
-#ifdef CONFIG_X86_64
-	if (c-&gt;x86 &gt;= 0xf) {
-		unsigned long long tseg;
-
-		/*
-		 * Split up direct mapping around the TSEG SMM area.
-		 * Don't do it for gbpages because there seems very little
-		 * benefit in doing so.
-		 */
-		if (!rdmsrl_safe(MSR_K8_TSEG_ADDR, &amp;tseg)) {
-			unsigned long pfn = tseg &gt;&gt; PAGE_SHIFT;
-
-			pr_debug("tseg: %010llx\n", tseg);
-			if (pfn_range_is_mapped(pfn, pfn + 1))
-				set_memory_4k((unsigned long)__va(tseg), 1);
-		}
-	}
-#endif
-
 	if (cpu_has(c, X86_FEATURE_CONSTANT_TSC)) {
 
 		if (c-&gt;x86 &gt; 0x10 ||
diff --git a/arch/x86/kernel/cpu/hygon.c b/arch/x86/kernel/cpu/hygon.c
index dc0840aae26c..ae59115d18f9 100644
--- a/arch/x86/kernel/cpu/hygon.c
+++ b/arch/x86/kernel/cpu/hygon.c
@@ -14,9 +14,6 @@
 #include &lt;asm/cacheinfo.h&gt;
 #include &lt;asm/spec-ctrl.h&gt;
 #include &lt;asm/delay.h&gt;
-#ifdef CONFIG_X86_64
-# include &lt;asm/set_memory.h&gt;
-#endif
 
 #include "cpu.h"
 
@@ -203,23 +200,6 @@ static void early_init_hygon_mc(struct cpuinfo_x86 *c)
 
 static void bsp_init_hygon(struct cpuinfo_x86 *c)
 {
-#ifdef CONFIG_X86_64
-	unsigned long long tseg;
-
-	/*
-	 * Split up direct mapping around the TSEG SMM area.
-	 * Don't do it for gbpages because there seems very little
-	 * benefit in doing so.
-	 */
-	if (!rdmsrl_safe(MSR_K8_TSEG_ADDR, &amp;tseg)) {
-		unsigned long pfn = tseg &gt;&gt; PAGE_SHIFT;
-
-		pr_debug("tseg: %010llx\n", tseg);
-		if (pfn_range_is_mapped(pfn, pfn + 1))
-			set_memory_4k((unsigned long)__va(tseg), 1);
-	}
-#endif
-
 	if (cpu_has(c, X86_FEATURE_CONSTANT_TSC)) {
 		u64 val;
 </pre><hr><pre>commit 31d8546033053b98de00846ede8088bdbe38651d
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Mon Oct 5 11:12:08 2020 -0400

    x86/head/64: Remove unused GET_CR2_INTO() macro
    
    Commit
    
      4b47cdbda6f1 ("x86/head/64: Move early exception dispatch to C code")
    
    removed the usage of GET_CR2_INTO().
    
    Drop the definition as well, and related definitions in paravirt.h and
    asm-offsets.h
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Borislav Petkov &lt;bp@suse.de&gt;
    Link: https://lkml.kernel.org/r/20201005151208.2212886-3-nivedita@alum.mit.edu

diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h
index d25cc6830e89..f8dce11d2bc1 100644
--- a/arch/x86/include/asm/paravirt.h
+++ b/arch/x86/include/asm/paravirt.h
@@ -812,17 +812,6 @@ extern void default_banner(void);
 #endif /* CONFIG_PARAVIRT_XXL */
 #endif	/* CONFIG_X86_64 */
 
-#ifdef CONFIG_PARAVIRT_XXL
-
-#define GET_CR2_INTO_AX							\
-	PARA_SITE(PARA_PATCH(PV_MMU_read_cr2),				\
-		  ANNOTATE_RETPOLINE_SAFE;				\
-		  call PARA_INDIRECT(pv_ops+PV_MMU_read_cr2);		\
-		 )
-
-#endif /* CONFIG_PARAVIRT_XXL */
-
-
 #endif /* __ASSEMBLY__ */
 #else  /* CONFIG_PARAVIRT */
 # define default_banner x86_init_noop
diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c
index 70b7154f4bdd..60b9f42ce3c1 100644
--- a/arch/x86/kernel/asm-offsets.c
+++ b/arch/x86/kernel/asm-offsets.c
@@ -66,7 +66,6 @@ static void __used common(void)
 	OFFSET(PV_IRQ_irq_disable, paravirt_patch_template, irq.irq_disable);
 	OFFSET(PV_IRQ_irq_enable, paravirt_patch_template, irq.irq_enable);
 	OFFSET(PV_CPU_iret, paravirt_patch_template, cpu.iret);
-	OFFSET(PV_MMU_read_cr2, paravirt_patch_template, mmu.read_cr2);
 #endif
 
 #ifdef CONFIG_XEN
diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S
index 7eb2a1c87969..2215d4cff38b 100644
--- a/arch/x86/kernel/head_64.S
+++ b/arch/x86/kernel/head_64.S
@@ -26,15 +26,6 @@
 #include &lt;asm/nospec-branch.h&gt;
 #include &lt;asm/fixmap.h&gt;
 
-#ifdef CONFIG_PARAVIRT_XXL
-#include &lt;asm/asm-offsets.h&gt;
-#include &lt;asm/paravirt.h&gt;
-#define GET_CR2_INTO(reg) GET_CR2_INTO_AX ; _ASM_MOV %_ASM_AX, reg
-#else
-#define INTERRUPT_RETURN iretq
-#define GET_CR2_INTO(reg) _ASM_MOV %cr2, reg
-#endif
-
 /*
  * We are not able to switch in one step to the final KERNEL ADDRESS SPACE
  * because we need identity-mapped pages.</pre><hr><pre>commit 0ac317e89791b76055ef11b952625ef77a1d2eba
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Mon Oct 5 11:12:07 2020 -0400

    x86/boot: Remove unused finalize_identity_maps()
    
    Commit
    
      8570978ea030 ("x86/boot/compressed/64: Don't pre-map memory in KASLR code")
    
    removed all the references to finalize_identity_maps(), but neglected to
    delete the actual function. Remove it.
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Borislav Petkov &lt;bp@suse.de&gt;
    Link: https://lkml.kernel.org/r/20201005151208.2212886-2-nivedita@alum.mit.edu

diff --git a/arch/x86/boot/compressed/ident_map_64.c b/arch/x86/boot/compressed/ident_map_64.c
index a5e5db6ada3c..6bf20223dc0f 100644
--- a/arch/x86/boot/compressed/ident_map_64.c
+++ b/arch/x86/boot/compressed/ident_map_64.c
@@ -167,16 +167,6 @@ void initialize_identity_maps(void *rmode)
 	write_cr3(top_level_pgt);
 }
 
-/*
- * This switches the page tables to the new level4 that has been built
- * via calls to add_identity_map() above. If booted via startup_32(),
- * this is effectively a no-op.
- */
-void finalize_identity_maps(void)
-{
-	write_cr3(top_level_pgt);
-}
-
 static pte_t *split_large_pmd(struct x86_mapping_info *info,
 			      pmd_t *pmdp, unsigned long __address)
 {</pre><hr><pre>commit 3347acc6fcd4ee71ad18a9ff9d9dac176b517329
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Fri Nov 13 22:51:59 2020 -0800

    compiler.h: fix barrier_data() on clang
    
    Commit 815f0ddb346c ("include/linux/compiler*.h: make compiler-*.h
    mutually exclusive") neglected to copy barrier_data() from
    compiler-gcc.h into compiler-clang.h.
    
    The definition in compiler-gcc.h was really to work around clang's more
    aggressive optimization, so this broke barrier_data() on clang, and
    consequently memzero_explicit() as well.
    
    For example, this results in at least the memzero_explicit() call in
    lib/crypto/sha256.c:sha256_transform() being optimized away by clang.
    
    Fix this by moving the definition of barrier_data() into compiler.h.
    
    Also move the gcc/clang definition of barrier() into compiler.h,
    __memory_barrier() is icc-specific (and barrier() is already defined
    using it in compiler-intel.h) and doesn't belong in compiler.h.
    
    [rdunlap@infradead.org: fix ALPHA builds when SMP is not enabled]
    
    Link: https://lkml.kernel.org/r/20201101231835.4589-1-rdunlap@infradead.org
    Fixes: 815f0ddb346c ("include/linux/compiler*.h: make compiler-*.h mutually exclusive")
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Randy Dunlap &lt;rdunlap@infradead.org&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Tested-by: Nick Desaulniers &lt;ndesaulniers@google.com&gt;
    Reviewed-by: Nick Desaulniers &lt;ndesaulniers@google.com&gt;
    Reviewed-by: Kees Cook &lt;keescook@chromium.org&gt;
    Cc: &lt;stable@vger.kernel.org&gt;
    Link: https://lkml.kernel.org/r/20201014212631.207844-1-nivedita@alum.mit.edu
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h
index 798027bb89be..640f09479bdf 100644
--- a/include/asm-generic/barrier.h
+++ b/include/asm-generic/barrier.h
@@ -13,6 +13,7 @@
 
 #ifndef __ASSEMBLY__
 
+#include &lt;linux/compiler.h&gt;
 #include &lt;asm/rwonce.h&gt;
 
 #ifndef nop
diff --git a/include/linux/compiler-clang.h b/include/linux/compiler-clang.h
index 230604e7f057..dd7233c48bf3 100644
--- a/include/linux/compiler-clang.h
+++ b/include/linux/compiler-clang.h
@@ -60,12 +60,6 @@
 #define COMPILER_HAS_GENERIC_BUILTIN_OVERFLOW 1
 #endif
 
-/* The following are for compatibility with GCC, from compiler-gcc.h,
- * and may be redefined here because they should not be shared with other
- * compilers, like ICC.
- */
-#define barrier() __asm__ __volatile__("" : : : "memory")
-
 #if __has_feature(shadow_call_stack)
 # define __noscs	__attribute__((__no_sanitize__("shadow-call-stack")))
 #endif
diff --git a/include/linux/compiler-gcc.h b/include/linux/compiler-gcc.h
index 5deb37024574..74c6c0486eed 100644
--- a/include/linux/compiler-gcc.h
+++ b/include/linux/compiler-gcc.h
@@ -15,25 +15,6 @@
 # error Sorry, your version of GCC is too old - please use 4.9 or newer.
 #endif
 
-/* Optimization barrier */
-
-/* The "volatile" is due to gcc bugs */
-#define barrier() __asm__ __volatile__("": : :"memory")
-/*
- * This version is i.e. to prevent dead stores elimination on @ptr
- * where gcc and llvm may behave differently when otherwise using
- * normal barrier(): while gcc behavior gets along with a normal
- * barrier(), llvm needs an explicit input variable to be assumed
- * clobbered. The issue is as follows: while the inline asm might
- * access any memory it wants, the compiler could have fit all of
- * @ptr into memory registers instead, and since @ptr never escaped
- * from that, it proved that the inline asm wasn't touching any of
- * it. This version works well with both compilers, i.e. we're telling
- * the compiler that the inline asm absolutely may see the contents
- * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495
- */
-#define barrier_data(ptr) __asm__ __volatile__("": :"r"(ptr) :"memory")
-
 /*
  * This macro obfuscates arithmetic on a variable address so that gcc
  * shouldn't recognize the original var, and make assumptions about it.
diff --git a/include/linux/compiler.h b/include/linux/compiler.h
index e512f5505dad..b8fe0c23cfff 100644
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -80,11 +80,25 @@ void ftrace_likely_update(struct ftrace_likely_data *f, int val,
 
 /* Optimization barrier */
 #ifndef barrier
-# define barrier() __memory_barrier()
+/* The "volatile" is due to gcc bugs */
+# define barrier() __asm__ __volatile__("": : :"memory")
 #endif
 
 #ifndef barrier_data
-# define barrier_data(ptr) barrier()
+/*
+ * This version is i.e. to prevent dead stores elimination on @ptr
+ * where gcc and llvm may behave differently when otherwise using
+ * normal barrier(): while gcc behavior gets along with a normal
+ * barrier(), llvm needs an explicit input variable to be assumed
+ * clobbered. The issue is as follows: while the inline asm might
+ * access any memory it wants, the compiler could have fit all of
+ * @ptr into memory registers instead, and since @ptr never escaped
+ * from that, it proved that the inline asm wasn't touching any of
+ * it. This version works well with both compilers, i.e. we're telling
+ * the compiler that the inline asm absolutely may see the contents
+ * of @ptr. See also: https://llvm.org/bugs/show_bug.cgi?id=15495
+ */
+# define barrier_data(ptr) __asm__ __volatile__("": :"r"(ptr) :"memory")
 #endif
 
 /* workaround for GCC PR82365 if needed */</pre><hr><pre>commit c2fe61d8be491ff8188edaf22e838f819999146b
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Tue Nov 10 11:39:19 2020 -0500

    efi/x86: Free efi_pgd with free_pages()
    
    Commit
    
      d9e9a6418065 ("x86/mm/pti: Allocate a separate user PGD")
    
    changed the PGD allocation to allocate PGD_ALLOCATION_ORDER pages, so in
    the error path it should be freed using free_pages() rather than
    free_page().
    
    Commit
    
        06ace26f4e6f ("x86/efi: Free efi_pgd with free_pages()")
    
    fixed one instance of this, but missed another.
    
    Move the freeing out-of-line to avoid code duplication and fix this bug.
    
    Fixes: d9e9a6418065 ("x86/mm/pti: Allocate a separate user PGD")
    Link: https://lore.kernel.org/r/20201110163919.1134431-1-nivedita@alum.mit.edu
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Ard Biesheuvel &lt;ardb@kernel.org&gt;

diff --git a/arch/x86/platform/efi/efi_64.c b/arch/x86/platform/efi/efi_64.c
index 8f5759df7776..e1e8d4e3a213 100644
--- a/arch/x86/platform/efi/efi_64.c
+++ b/arch/x86/platform/efi/efi_64.c
@@ -78,28 +78,30 @@ int __init efi_alloc_page_tables(void)
 	gfp_mask = GFP_KERNEL | __GFP_ZERO;
 	efi_pgd = (pgd_t *)__get_free_pages(gfp_mask, PGD_ALLOCATION_ORDER);
 	if (!efi_pgd)
-		return -ENOMEM;
+		goto fail;
 
 	pgd = efi_pgd + pgd_index(EFI_VA_END);
 	p4d = p4d_alloc(&amp;init_mm, pgd, EFI_VA_END);
-	if (!p4d) {
-		free_page((unsigned long)efi_pgd);
-		return -ENOMEM;
-	}
+	if (!p4d)
+		goto free_pgd;
 
 	pud = pud_alloc(&amp;init_mm, p4d, EFI_VA_END);
-	if (!pud) {
-		if (pgtable_l5_enabled())
-			free_page((unsigned long) pgd_page_vaddr(*pgd));
-		free_pages((unsigned long)efi_pgd, PGD_ALLOCATION_ORDER);
-		return -ENOMEM;
-	}
+	if (!pud)
+		goto free_p4d;
 
 	efi_mm.pgd = efi_pgd;
 	mm_init_cpumask(&amp;efi_mm);
 	init_new_context(NULL, &amp;efi_mm);
 
 	return 0;
+
+free_p4d:
+	if (pgtable_l5_enabled())
+		free_page((unsigned long)pgd_page_vaddr(*pgd));
+free_pgd:
+	free_pages((unsigned long)efi_pgd, PGD_ALLOCATION_ORDER);
+fail:
+	return -ENOMEM;
 }
 
 /*</pre><hr><pre>commit 18d05ca4486fe38991c3166b1f4df26b8a029665
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Sun Oct 25 10:31:19 2020 -0400

    crypto: lib/sha256 - Unroll LOAD and BLEND loops
    
    Unrolling the LOAD and BLEND loops improves performance by ~8% on x86_64
    (tested on Broadwell Xeon) while not increasing code size too much.
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Reviewed-by: Eric Biggers &lt;ebiggers@google.com&gt;
    Acked-by: Ard Biesheuvel &lt;ardb@kernel.org&gt;
    Signed-off-by: Herbert Xu &lt;herbert@gondor.apana.org.au&gt;

diff --git a/lib/crypto/sha256.c b/lib/crypto/sha256.c
index e2e29d9b0ccd..cdef37c05972 100644
--- a/lib/crypto/sha256.c
+++ b/lib/crypto/sha256.c
@@ -76,12 +76,28 @@ static void sha256_transform(u32 *state, const u8 *input, u32 *W)
 	int i;
 
 	/* load the input */
-	for (i = 0; i &lt; 16; i++)
-		LOAD_OP(i, W, input);
+	for (i = 0; i &lt; 16; i += 8) {
+		LOAD_OP(i + 0, W, input);
+		LOAD_OP(i + 1, W, input);
+		LOAD_OP(i + 2, W, input);
+		LOAD_OP(i + 3, W, input);
+		LOAD_OP(i + 4, W, input);
+		LOAD_OP(i + 5, W, input);
+		LOAD_OP(i + 6, W, input);
+		LOAD_OP(i + 7, W, input);
+	}
 
 	/* now blend */
-	for (i = 16; i &lt; 64; i++)
-		BLEND_OP(i, W);
+	for (i = 16; i &lt; 64; i += 8) {
+		BLEND_OP(i + 0, W);
+		BLEND_OP(i + 1, W);
+		BLEND_OP(i + 2, W);
+		BLEND_OP(i + 3, W);
+		BLEND_OP(i + 4, W);
+		BLEND_OP(i + 5, W);
+		BLEND_OP(i + 6, W);
+		BLEND_OP(i + 7, W);
+	}
 
 	/* load the state into our registers */
 	a = state[0];  b = state[1];  c = state[2];  d = state[3];</pre>
    <div class="pagination">
        <a href='1_113.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><span>[114]</span><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_115.html'>Next&gt;&gt;</a>
    <div>
</body>

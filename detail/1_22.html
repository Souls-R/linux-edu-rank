<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_21.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><span>[22]</span><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_23.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 232530680290ba94ca37852ab10d9556ea28badf
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Nov 8 22:23:20 2017 -0500

    ext4: improve smp scalability for inode generation
    
    -&gt;s_next_generation is protected by s_next_gen_lock but its usage
    pattern is very primitive.  We don't actually need sequentially
    increasing new generation numbers, so let's use prandom_u32() instead.
    
    Reported-by: Dmitry Monakhov &lt;dmonakhov@openvz.org&gt;
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 53ce95b52fd8..5e6d7b6f50c7 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1355,8 +1355,6 @@ struct ext4_sb_info {
 	int s_first_ino;
 	unsigned int s_inode_readahead_blks;
 	unsigned int s_inode_goal;
-	spinlock_t s_next_gen_lock;
-	u32 s_next_generation;
 	u32 s_hash_seed[4];
 	int s_def_hash_version;
 	int s_hash_unsigned;	/* 3 if hash should be signed, 0 if not */
diff --git a/fs/ext4/ialloc.c b/fs/ext4/ialloc.c
index ee823022aa34..da79eb5dba40 100644
--- a/fs/ext4/ialloc.c
+++ b/fs/ext4/ialloc.c
@@ -1138,9 +1138,7 @@ struct inode *__ext4_new_inode(handle_t *handle, struct inode *dir,
 			   inode-&gt;i_ino);
 		goto out;
 	}
-	spin_lock(&amp;sbi-&gt;s_next_gen_lock);
-	inode-&gt;i_generation = sbi-&gt;s_next_generation++;
-	spin_unlock(&amp;sbi-&gt;s_next_gen_lock);
+	inode-&gt;i_generation = prandom_u32();
 
 	/* Precompute checksum seed for inode metadata */
 	if (ext4_has_metadata_csum(sb)) {
diff --git a/fs/ext4/ioctl.c b/fs/ext4/ioctl.c
index 144bbda2b808..23a4766f6678 100644
--- a/fs/ext4/ioctl.c
+++ b/fs/ext4/ioctl.c
@@ -14,6 +14,7 @@
 #include &lt;linux/mount.h&gt;
 #include &lt;linux/file.h&gt;
 #include &lt;linux/quotaops.h&gt;
+#include &lt;linux/random.h&gt;
 #include &lt;linux/uuid.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/delay.h&gt;
@@ -98,7 +99,6 @@ static long swap_inode_boot_loader(struct super_block *sb,
 	int err;
 	struct inode *inode_bl;
 	struct ext4_inode_info *ei_bl;
-	struct ext4_sb_info *sbi = EXT4_SB(sb);
 
 	if (inode-&gt;i_nlink != 1 || !S_ISREG(inode-&gt;i_mode))
 		return -EINVAL;
@@ -157,10 +157,8 @@ static long swap_inode_boot_loader(struct super_block *sb,
 
 	inode-&gt;i_ctime = inode_bl-&gt;i_ctime = current_time(inode);
 
-	spin_lock(&amp;sbi-&gt;s_next_gen_lock);
-	inode-&gt;i_generation = sbi-&gt;s_next_generation++;
-	inode_bl-&gt;i_generation = sbi-&gt;s_next_generation++;
-	spin_unlock(&amp;sbi-&gt;s_next_gen_lock);
+	inode-&gt;i_generation = prandom_u32();
+	inode_bl-&gt;i_generation = prandom_u32();
 
 	ext4_discard_preallocations(inode);
 
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 3a278faf5868..9f2e3eb5131f 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -3982,8 +3982,6 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	}
 
 	sbi-&gt;s_gdb_count = db_count;
-	get_random_bytes(&amp;sbi-&gt;s_next_generation, sizeof(u32));
-	spin_lock_init(&amp;sbi-&gt;s_next_gen_lock);
 
 	timer_setup(&amp;sbi-&gt;s_err_report, print_daily_error_info, 0);
 </pre><hr><pre>commit 51e3ae81ec58e95f10a98ef3dd6d7bce5d8e35a2
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Oct 6 23:09:55 2017 -0400

    ext4: fix interaction between i_size, fallocate, and delalloc after a crash
    
    If there are pending writes subject to delayed allocation, then i_size
    will show size after the writes have completed, while i_disksize
    contains the value of i_size on the disk (since the writes have not
    been persisted to disk).
    
    If fallocate(2) is called with the FALLOC_FL_KEEP_SIZE flag, either
    with or without the FALLOC_FL_ZERO_RANGE flag set, and the new size
    after the fallocate(2) is between i_size and i_disksize, then after a
    crash, if a journal commit has resulted in the changes made by the
    fallocate() call to be persisted after a crash, but the delayed
    allocation write has not resolved itself, i_size would not be updated,
    and this would cause the following e2fsck complaint:
    
    Inode 12, end of extent exceeds allowed value
            (logical block 33, physical block 33441, len 7)
    
    This can only take place on a sparse file, where the fallocate(2) call
    is allocating blocks in a range which is before a pending delayed
    allocation write which is extending i_size.  Since this situation is
    quite rare, and the window in which the crash must take place is
    typically &lt; 30 seconds, in practice this condition will rarely happen.
    
    Nevertheless, it can be triggered in testing, and in particular by
    xfstests generic/456.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Reported-by: Amir Goldstein &lt;amir73il@gmail.com&gt;
    Cc: stable@vger.kernel.org

diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index 97f0fd06728d..07bca11749d4 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -4794,7 +4794,8 @@ static long ext4_zero_range(struct file *file, loff_t offset,
 	}
 
 	if (!(mode &amp; FALLOC_FL_KEEP_SIZE) &amp;&amp;
-	     offset + len &gt; i_size_read(inode)) {
+	    (offset + len &gt; i_size_read(inode) ||
+	     offset + len &gt; EXT4_I(inode)-&gt;i_disksize)) {
 		new_size = offset + len;
 		ret = inode_newsize_ok(inode, new_size);
 		if (ret)
@@ -4965,7 +4966,8 @@ long ext4_fallocate(struct file *file, int mode, loff_t offset, loff_t len)
 	}
 
 	if (!(mode &amp; FALLOC_FL_KEEP_SIZE) &amp;&amp;
-	     offset + len &gt; i_size_read(inode)) {
+	    (offset + len &gt; i_size_read(inode) ||
+	     offset + len &gt; EXT4_I(inode)-&gt;i_disksize)) {
 		new_size = offset + len;
 		ret = inode_newsize_ok(inode, new_size);
 		if (ret)</pre><hr><pre>commit 68fd97504ad2f70850c47ce45caa110a0ca843d2
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun Oct 1 17:59:54 2017 -0400

    ext4: retry allocations conservatively
    
    Now that we no longer try to reserve metadata blocks for delayed
    allocations (which tended to overestimate the required number of
    blocks significantly), we really don't need retry allocations when the
    disk is very full as aggressively any more.
    
    The only time when it makes sense to retry an allocation is if we have
    freshly deleted blocks that will only become available after a
    transaction commit.  And if we lose that race, it's not worth it to
    try more than once.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/balloc.c b/fs/ext4/balloc.c
index e04ec868e37e..a3798b25a8dc 100644
--- a/fs/ext4/balloc.c
+++ b/fs/ext4/balloc.c
@@ -600,22 +600,21 @@ int ext4_claim_free_clusters(struct ext4_sb_info *sbi,
  * ext4_should_retry_alloc() is called when ENOSPC is returned, and if
  * it is profitable to retry the operation, this function will wait
  * for the current or committing transaction to complete, and then
- * return TRUE.
- *
- * if the total number of retries exceed three times, return FALSE.
+ * return TRUE.  We will only retry once.
  */
 int ext4_should_retry_alloc(struct super_block *sb, int *retries)
 {
 	if (!ext4_has_free_clusters(EXT4_SB(sb), 1, 0) ||
-	    (*retries)++ &gt; 3 ||
+	    (*retries)++ &gt; 1 ||
 	    !EXT4_SB(sb)-&gt;s_journal)
 		return 0;
 
-	jbd_debug(1, "%s: retrying operation after ENOSPC\n", sb-&gt;s_id);
-
 	smp_mb();
-	if (EXT4_SB(sb)-&gt;s_mb_free_pending)
-		jbd2_journal_force_commit_nested(EXT4_SB(sb)-&gt;s_journal);
+	if (EXT4_SB(sb)-&gt;s_mb_free_pending == 0)
+		return 0;
+
+	jbd_debug(1, "%s: retrying operation after ENOSPC\n", sb-&gt;s_id);
+	jbd2_journal_force_commit_nested(EXT4_SB(sb)-&gt;s_journal);
 	return 1;
 }
 </pre><hr><pre>commit b80b32b6d5e79798b85cd4644206aaa069059390
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Aug 14 08:29:18 2017 -0400

    ext4: fix clang build regression
    
    Arnd Bergmann &lt;arnd@arndb.de&gt;
    
    As Stefan pointed out, I misremembered what clang can do specifically,
    and it turns out that the variable-length array at the end of the
    structure did not work (a flexible array would have worked here
    but not solved the problem):
    
    fs/ext4/mballoc.c:2303:17: error: fields must have a constant size:
    'variable length array in structure' extension will never be supported
                    ext4_grpblk_t counters[blocksize_bits + 2];
    
    This reverts part of my previous patch, using a fixed-size array
    again, but keeping the check for the array overflow.
    
    Fixes: 2df2c3402fc8 ("ext4: fix warning about stack corruption")
    Reported-by: Stefan Agner &lt;stefan@agner.ch&gt;
    Tested-by: Chandan Rajendra &lt;chandan@linux.vnet.ibm.com&gt;
    Signed-off-by: Arnd Bergmann &lt;arnd@arndb.de&gt;
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 5a1052627a81..701085620cd8 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2300,7 +2300,7 @@ static int ext4_mb_seq_groups_show(struct seq_file *seq, void *v)
 					     EXT4_MAX_BLOCK_LOG_SIZE);
 	struct sg {
 		struct ext4_group_info info;
-		ext4_grpblk_t counters[blocksize_bits + 2];
+		ext4_grpblk_t counters[EXT4_MAX_BLOCK_LOG_SIZE + 2];
 	} sg;
 
 	group--;
@@ -2309,6 +2309,9 @@ static int ext4_mb_seq_groups_show(struct seq_file *seq, void *v)
 			      " 2^0   2^1   2^2   2^3   2^4   2^5   2^6  "
 			      " 2^7   2^8   2^9   2^10  2^11  2^12  2^13  ]\n");
 
+	i = (blocksize_bits + 2) * sizeof(sg.info.bb_counters[0]) +
+		sizeof(struct ext4_group_info);
+
 	grinfo = ext4_get_group_info(sb, group);
 	/* Load the group info in memory only if not already loaded. */
 	if (unlikely(EXT4_MB_GRP_NEED_INIT(grinfo))) {
@@ -2320,7 +2323,7 @@ static int ext4_mb_seq_groups_show(struct seq_file *seq, void *v)
 		buddy_loaded = 1;
 	}
 
-	memcpy(&amp;sg, ext4_get_group_info(sb, group), sizeof(sg));
+	memcpy(&amp;sg, ext4_get_group_info(sb, group), i);
 
 	if (buddy_loaded)
 		ext4_mb_unload_buddy(&amp;e4b);</pre><hr><pre>commit eecabf567422eda02bd179f2707d8fe24f52d888
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Jun 8 04:16:59 2017 -0400

    random: suppress spammy warnings about unseeded randomness
    
    Unfortunately, on some models of some architectures getting a fully
    seeded CRNG is extremely difficult, and so this can result in dmesg
    getting spammed for a surprisingly long time.  This is really bad from
    a security perspective, and so architecture maintainers really need to
    do what they can to get the CRNG seeded sooner after the system is
    booted.  However, users can't do anything actionble to address this,
    and spamming the kernel messages log will only just annoy people.
    
    For developers who want to work on improving this situation,
    CONFIG_WARN_UNSEEDED_RANDOM has been renamed to
    CONFIG_WARN_ALL_UNSEEDED_RANDOM.  By default the kernel will always
    print the first use of unseeded randomness.  This way, hopefully the
    security obsessed will be happy that there is _some_ indication when
    the kernel boots there may be a potential issue with that architecture
    or subarchitecture.  To see all uses of unseeded randomness,
    developers can enable CONFIG_WARN_ALL_UNSEEDED_RANDOM.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/drivers/char/random.c b/drivers/char/random.c
index fa5bbd5a7ca0..799d37981d99 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -436,6 +436,7 @@ static void _extract_crng(struct crng_state *crng,
 static void _crng_backtrack_protect(struct crng_state *crng,
 				    __u8 tmp[CHACHA20_BLOCK_SIZE], int used);
 static void process_random_ready_list(void);
+static void _get_random_bytes(void *buf, int nbytes);
 
 /**********************************************************************
  *
@@ -776,7 +777,7 @@ static void crng_initialize(struct crng_state *crng)
 		_extract_entropy(&amp;input_pool, &amp;crng-&gt;state[4],
 				 sizeof(__u32) * 12, 0);
 	else
-		get_random_bytes(&amp;crng-&gt;state[4], sizeof(__u32) * 12);
+		_get_random_bytes(&amp;crng-&gt;state[4], sizeof(__u32) * 12);
 	for (i = 4; i &lt; 16; i++) {
 		if (!arch_get_random_seed_long(&amp;rv) &amp;&amp;
 		    !arch_get_random_long(&amp;rv))
@@ -1466,6 +1467,30 @@ static ssize_t extract_entropy_user(struct entropy_store *r, void __user *buf,
 	return ret;
 }
 
+#define warn_unseeded_randomness(previous) \
+	_warn_unseeded_randomness(__func__, (void *) _RET_IP_, (previous))
+
+static void _warn_unseeded_randomness(const char *func_name, void *caller,
+				      void **previous)
+{
+#ifdef CONFIG_WARN_ALL_UNSEEDED_RANDOM
+	const bool print_once = false;
+#else
+	static bool print_once __read_mostly;
+#endif
+
+	if (print_once ||
+	    crng_ready() ||
+	    (previous &amp;&amp; (caller == READ_ONCE(*previous))))
+		return;
+	WRITE_ONCE(*previous, caller);
+#ifndef CONFIG_WARN_ALL_UNSEEDED_RANDOM
+	print_once = true;
+#endif
+	pr_notice("random: %s called from %pF with crng_init=%d\n",
+		  func_name, caller, crng_init);
+}
+
 /*
  * This function is the exported kernel interface.  It returns some
  * number of good random numbers, suitable for key generation, seeding
@@ -1476,15 +1501,10 @@ static ssize_t extract_entropy_user(struct entropy_store *r, void __user *buf,
  * wait_for_random_bytes() should be called and return 0 at least once
  * at any point prior.
  */
-void get_random_bytes(void *buf, int nbytes)
+static void _get_random_bytes(void *buf, int nbytes)
 {
 	__u8 tmp[CHACHA20_BLOCK_SIZE];
 
-#ifdef CONFIG_WARN_UNSEEDED_RANDOM
-	if (!crng_ready())
-		printk(KERN_NOTICE "random: %pF get_random_bytes called "
-		       "with crng_init = %d\n", (void *) _RET_IP_, crng_init);
-#endif
 	trace_get_random_bytes(nbytes, _RET_IP_);
 
 	while (nbytes &gt;= CHACHA20_BLOCK_SIZE) {
@@ -1501,6 +1521,14 @@ void get_random_bytes(void *buf, int nbytes)
 		crng_backtrack_protect(tmp, CHACHA20_BLOCK_SIZE);
 	memzero_explicit(tmp, sizeof(tmp));
 }
+
+void get_random_bytes(void *buf, int nbytes)
+{
+	static void *previous;
+
+	warn_unseeded_randomness(&amp;previous);
+	_get_random_bytes(buf, nbytes);
+}
 EXPORT_SYMBOL(get_random_bytes);
 
 /*
@@ -2064,6 +2092,7 @@ u64 get_random_u64(void)
 	bool use_lock = READ_ONCE(crng_init) &lt; 2;
 	unsigned long flags = 0;
 	struct batched_entropy *batch;
+	static void *previous;
 
 #if BITS_PER_LONG == 64
 	if (arch_get_random_long((unsigned long *)&amp;ret))
@@ -2074,11 +2103,7 @@ u64 get_random_u64(void)
 	    return ret;
 #endif
 
-#ifdef CONFIG_WARN_UNSEEDED_RANDOM
-	if (!crng_ready())
-		printk(KERN_NOTICE "random: %pF get_random_u64 called "
-		       "with crng_init = %d\n", (void *) _RET_IP_, crng_init);
-#endif
+	warn_unseeded_randomness(&amp;previous);
 
 	batch = &amp;get_cpu_var(batched_entropy_u64);
 	if (use_lock)
@@ -2102,15 +2127,12 @@ u32 get_random_u32(void)
 	bool use_lock = READ_ONCE(crng_init) &lt; 2;
 	unsigned long flags = 0;
 	struct batched_entropy *batch;
+	static void *previous;
 
 	if (arch_get_random_int(&amp;ret))
 		return ret;
 
-#ifdef CONFIG_WARN_UNSEEDED_RANDOM
-	if (!crng_ready())
-		printk(KERN_NOTICE "random: %pF get_random_u32 called "
-		       "with crng_init = %d\n", (void *) _RET_IP_, crng_init);
-#endif
+	warn_unseeded_randomness(&amp;previous);
 
 	batch = &amp;get_cpu_var(batched_entropy_u32);
 	if (use_lock)
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index c4159605bfbf..9d0a244074b9 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -1209,10 +1209,9 @@ config STACKTRACE
 	  It is also used by various kernel debugging features that require
 	  stack trace generation.
 
-config WARN_UNSEEDED_RANDOM
-	bool "Warn when kernel uses unseeded randomness"
-	default y
-	depends on DEBUG_KERNEL
+config WARN_ALL_UNSEEDED_RANDOM
+	bool "Warn for all uses of unseeded randomness"
+	default n
 	help
 	  Some parts of the kernel contain bugs relating to their use of
 	  cryptographically secure random numbers before it's actually possible
@@ -1222,8 +1221,21 @@ config WARN_UNSEEDED_RANDOM
 	  are going wrong, so that they might contact developers about fixing
 	  it.
 
-	  Say Y here, unless you simply do not care about using unseeded
-	  randomness and do not want a potential warning message in your logs.
+	  Unfortunately, on some models of some architectures getting
+	  a fully seeded CRNG is extremely difficult, and so this can
+	  result in dmesg getting spammed for a surprisingly long
+	  time.  This is really bad from a security perspective, and
+	  so architecture maintainers really need to do what they can
+	  to get the CRNG seeded sooner after the system is booted.
+	  However, since users can not do anything actionble to
+	  address this, by default the kernel will issue only a single
+	  warning for the first use of unseeded randomness.
+
+	  Say Y here if you want to receive warnings for all uses of
+	  unseeded randomness.  This will be of use primarily for
+	  those developers interersted in improving the security of
+	  Linux kernels running on their architecture (or
+	  subarchitecture).
 
 config DEBUG_KOBJECT
 	bool "kobject debugging"</pre><hr><pre>commit bdddf342796765a1a946e7c4aed2574f4488e4e5
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Jun 23 00:47:05 2017 -0400

    ext4: return EFSBADCRC if a bad checksum error is found in ext4_find_entry()
    
    Previously a bad directory block with a bad checksum is skipped; we
    should be returning EFSBADCRC (aka EBADMSG).
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/namei.c b/fs/ext4/namei.c
index 2a7f2dc7f4dd..13f0cadb1238 100644
--- a/fs/ext4/namei.c
+++ b/fs/ext4/namei.c
@@ -1456,7 +1456,8 @@ static struct buffer_head * ext4_find_entry (struct inode *dir,
 			EXT4_ERROR_INODE(dir, "checksumming directory "
 					 "block %lu", (unsigned long)block);
 			brelse(bh);
-			goto next;
+			ret = ERR_PTR(-EFSBADCRC);
+			goto cleanup_and_exit;
 		}
 		set_buffer_verified(bh);
 		i = search_dirblock(bh, dir, &amp;fname,</pre><hr><pre>commit 92e75428ffc90e2a0321062379f883f3671cfebe
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Jun 7 19:01:32 2017 -0400

    random: use lockless method of accessing and updating f-&gt;reg_idx
    
    Linus pointed out that there is a much more efficient way of avoiding
    the problem that we were trying to address in commit 9dfa7bba35ac0:
    "fix race in drivers/char/random.c:get_reg()".
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/drivers/char/random.c b/drivers/char/random.c
index a561f0c2f428..473ad34378f2 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -1097,15 +1097,15 @@ static void add_interrupt_bench(cycles_t start)
 static __u32 get_reg(struct fast_pool *f, struct pt_regs *regs)
 {
 	__u32 *ptr = (__u32 *) regs;
-	unsigned long flags;
+	unsigned int idx;
 
 	if (regs == NULL)
 		return 0;
-	local_irq_save(flags);
-	if (f-&gt;reg_idx &gt;= sizeof(struct pt_regs) / sizeof(__u32))
-		f-&gt;reg_idx = 0;
-	ptr += f-&gt;reg_idx++;
-	local_irq_restore(flags);
+	idx = READ_ONCE(f-&gt;reg_idx);
+	if (idx &gt;= sizeof(struct pt_regs) / sizeof(__u32))
+		idx = 0;
+	ptr += idx++;
+	WRITE_ONCE(f-&gt;reg_idx, idx);
 	return *ptr;
 }
 </pre><hr><pre>commit 72d622b42258a0ed0b6e8c0f40d7628de935d058
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun Apr 30 20:08:05 2017 -0400

    ext4: replace BUG_ON with WARN_ONCE in ext4_end_bio()
    
    Add fallback code and a WARN_ONCE() call instead of a BUG_ON() in
    the ext4_end_bio() function.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/page-io.c b/fs/ext4/page-io.c
index 208241b06662..1a82138ba739 100644
--- a/fs/ext4/page-io.c
+++ b/fs/ext4/page-io.c
@@ -297,8 +297,17 @@ static void ext4_end_bio(struct bio *bio)
 {
 	ext4_io_end_t *io_end = bio-&gt;bi_private;
 	sector_t bi_sector = bio-&gt;bi_iter.bi_sector;
+	char b[BDEVNAME_SIZE];
 
-	BUG_ON(!io_end);
+	if (WARN_ONCE(!io_end, "io_end is NULL: %s: sector %Lu len %u err %d\n",
+		      bdevname(bio-&gt;bi_bdev, b),
+		      (long long) bio-&gt;bi_iter.bi_sector,
+		      (unsigned) bio_sectors(bio),
+		      bio-&gt;bi_error)) {
+		ext4_finish_bio(bio);
+		bio_put(bio);
+		return;
+	}
 	bio-&gt;bi_end_io = NULL;
 
 	if (bio-&gt;bi_error) {</pre><hr><pre>commit 80a2ea9f85850f1cdae814be03b4a16c3d3abc00
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Apr 28 09:51:54 2017 -0400

    mm: retry writepages() on ENOMEM when doing an data integrity writeback
    
    Currently, file system's writepages() function must not fail with an
    ENOMEM, since if they do, it's possible for buffered data to be lost.
    This is because on a data integrity writeback writepages() gets called
    but once, and if it returns ENOMEM, if you're lucky the error will get
    reflected back to the userspace process calling fsync().  If you
    aren't lucky, the user is unmounting the file system, and the dirty
    pages will simply be lost.
    
    For this reason, file system code generally will use GFP_NOFS, and in
    some cases, will retry the allocation in a loop, on the theory that
    "kernel livelocks are temporary; data loss is forever".
    Unfortunately, this can indeed cause livelocks, since inside the
    writepages() call, the file system is holding various mutexes, and
    these mutexes may prevent the OOM killer from killing its targetted
    victim if it is also holding on to those mutexes.
    
    A better solution would be to allow writepages() to call the memory
    allocator with flags that give greater latitude to the allocator to
    fail, and then release its locks and return ENOMEM, and in the case of
    background writeback, the writes can be retried at a later time.  In
    the case of data-integrity writeback retry after waiting a brief
    amount of time.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index d8ac2a7fb9e7..03a70d8a6030 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -2353,10 +2353,16 @@ int do_writepages(struct address_space *mapping, struct writeback_control *wbc)
 
 	if (wbc-&gt;nr_to_write &lt;= 0)
 		return 0;
-	if (mapping-&gt;a_ops-&gt;writepages)
-		ret = mapping-&gt;a_ops-&gt;writepages(mapping, wbc);
-	else
-		ret = generic_writepages(mapping, wbc);
+	while (1) {
+		if (mapping-&gt;a_ops-&gt;writepages)
+			ret = mapping-&gt;a_ops-&gt;writepages(mapping, wbc);
+		else
+			ret = generic_writepages(mapping, wbc);
+		if ((ret != -ENOMEM) || (wbc-&gt;sync_mode != WB_SYNC_ALL))
+			break;
+		cond_resched();
+		congestion_wait(BLK_RW_ASYNC, HZ/50);
+	}
 	return ret;
 }
 </pre><hr><pre>commit d67d64f423147cf4fe8212658255e1160a4ef02c
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Mar 25 17:33:31 2017 -0400

    ext4: fix two spelling nits
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index f622d4a577e3..f303d3a7f44a 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -5400,7 +5400,7 @@ int ext4_getattr(struct vfsmount *mnt, struct dentry *dentry,
 	 * If there is inline data in the inode, the inode will normally not
 	 * have data blocks allocated (it may have an external xattr block).
 	 * Report at least one sector for such files, so tools like tar, rsync,
-	 * others doen't incorrectly think the file is completely sparse.
+	 * others don't incorrectly think the file is completely sparse.
 	 */
 	if (unlikely(ext4_has_inline_data(inode)))
 		stat-&gt;blocks += (stat-&gt;size + 511) &gt;&gt; 9;
diff --git a/fs/ext4/move_extent.c b/fs/ext4/move_extent.c
index 6fc14def0c70..615bc03d0fbd 100644
--- a/fs/ext4/move_extent.c
+++ b/fs/ext4/move_extent.c
@@ -511,7 +511,7 @@ mext_check_arguments(struct inode *orig_inode,
 	if ((orig_start &amp; ~(PAGE_MASK &gt;&gt; orig_inode-&gt;i_blkbits)) !=
 	    (donor_start &amp; ~(PAGE_MASK &gt;&gt; orig_inode-&gt;i_blkbits))) {
 		ext4_debug("ext4 move extent: orig and donor's start "
-			"offset are not alligned [ino:orig %lu, donor %lu]\n",
+			"offsets are not aligned [ino:orig %lu, donor %lu]\n",
 			orig_inode-&gt;i_ino, donor_inode-&gt;i_ino);
 		return -EINVAL;
 	}</pre>
    <div class="pagination">
        <a href='1_21.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><span>[22]</span><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_23.html'>Next&gt;&gt;</a>
    <div>
</body>

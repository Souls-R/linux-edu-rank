<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by University of Washington</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by University of Washington</h1>
    <div class="pagination">
        <a href='13_2.html'>&lt;&lt;Prev</a><a href='13.html'>1</a><a href='13_2.html'>2</a><span>[3]</span><a href='13_4.html'>4</a><a href='13_5.html'>5</a><a href='13_6.html'>6</a><a href='13_7.html'>7</a><a href='13_8.html'>8</a><a href='13_4.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit c648c9c7429e979ca081359f39b6902aed92d490
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Thu Apr 30 19:02:10 2020 -0700

    bpf, arm: Optimize ALU ARSH K using asr immediate instruction
    
    This patch adds an optimization that uses the asr immediate instruction
    for BPF_ALU BPF_ARSH BPF_K, rather than loading the immediate to
    a temporary register. This is similar to existing code for handling
    BPF_ALU BPF_{LSH,RSH} BPF_K. This optimization saves two instructions
    and is more consistent with LSH and RSH.
    
    Example of the code generated for BPF_ALU32_IMM(BPF_ARSH, BPF_REG_0, 5)
    before the optimization:
    
      2c:  mov    r8, #5
      30:  mov    r9, #0
      34:  asr    r0, r0, r8
    
    and after optimization:
    
      2c:  asr    r0, r0, #5
    
    Tested on QEMU using lib/test_bpf and test_verifier.
    
    Co-developed-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Daniel Borkmann &lt;daniel@iogearbox.net&gt;
    Link: https://lore.kernel.org/bpf/20200501020210.32294-3-luke.r.nels@gmail.com

diff --git a/arch/arm/net/bpf_jit_32.c b/arch/arm/net/bpf_jit_32.c
index 48b89211ee5c..0207b6ea6e8a 100644
--- a/arch/arm/net/bpf_jit_32.c
+++ b/arch/arm/net/bpf_jit_32.c
@@ -795,6 +795,9 @@ static inline void emit_a32_alu_i(const s8 dst, const u32 val,
 	case BPF_RSH:
 		emit(ARM_LSR_I(rd, rd, val), ctx);
 		break;
+	case BPF_ARSH:
+		emit(ARM_ASR_I(rd, rd, val), ctx);
+		break;
 	case BPF_NEG:
 		emit(ARM_RSB_I(rd, rd, val), ctx);
 		break;
@@ -1408,7 +1411,6 @@ static int build_insn(const struct bpf_insn *insn, struct jit_ctx *ctx)
 	case BPF_ALU | BPF_MUL | BPF_X:
 	case BPF_ALU | BPF_LSH | BPF_X:
 	case BPF_ALU | BPF_RSH | BPF_X:
-	case BPF_ALU | BPF_ARSH | BPF_K:
 	case BPF_ALU | BPF_ARSH | BPF_X:
 	case BPF_ALU64 | BPF_ADD | BPF_K:
 	case BPF_ALU64 | BPF_ADD | BPF_X:
@@ -1465,10 +1467,12 @@ static int build_insn(const struct bpf_insn *insn, struct jit_ctx *ctx)
 	case BPF_ALU64 | BPF_MOD | BPF_K:
 	case BPF_ALU64 | BPF_MOD | BPF_X:
 		goto notyet;
-	/* dst = dst &gt;&gt; imm */
 	/* dst = dst &lt;&lt; imm */
-	case BPF_ALU | BPF_RSH | BPF_K:
+	/* dst = dst &gt;&gt; imm */
+	/* dst = dst &gt;&gt; imm (signed) */
 	case BPF_ALU | BPF_LSH | BPF_K:
+	case BPF_ALU | BPF_RSH | BPF_K:
+	case BPF_ALU | BPF_ARSH | BPF_K:
 		if (unlikely(imm &gt; 31))
 			return -EINVAL;
 		if (imm)
diff --git a/arch/arm/net/bpf_jit_32.h b/arch/arm/net/bpf_jit_32.h
index fb67cbc589e0..e0b593a1498d 100644
--- a/arch/arm/net/bpf_jit_32.h
+++ b/arch/arm/net/bpf_jit_32.h
@@ -94,6 +94,9 @@
 #define ARM_INST_LSR_I		0x01a00020
 #define ARM_INST_LSR_R		0x01a00030
 
+#define ARM_INST_ASR_I		0x01a00040
+#define ARM_INST_ASR_R		0x01a00050
+
 #define ARM_INST_MOV_R		0x01a00000
 #define ARM_INST_MOVS_R		0x01b00000
 #define ARM_INST_MOV_I		0x03a00000</pre><hr><pre>commit cf48db69bdfad2930b95fd51d64444e5a7b469ae
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Thu Apr 30 19:02:09 2020 -0700

    bpf, arm: Optimize ALU64 ARSH X using orrpl conditional instruction
    
    This patch optimizes the code generated by emit_a32_arsh_r64, which
    handles the BPF_ALU64 BPF_ARSH BPF_X instruction.
    
    The original code uses a conditional B followed by an unconditional ORR.
    The optimization saves one instruction by removing the B instruction
    and using a conditional ORR (with an inverted condition).
    
    Example of the code generated for BPF_ALU64_REG(BPF_ARSH, BPF_REG_0,
    BPF_REG_1), before optimization:
    
      34:  rsb    ip, r2, #32
      38:  subs   r9, r2, #32
      3c:  lsr    lr, r0, r2
      40:  orr    lr, lr, r1, lsl ip
      44:  bmi    0x4c
      48:  orr    lr, lr, r1, asr r9
      4c:  asr    ip, r1, r2
      50:  mov    r0, lr
      54:  mov    r1, ip
    
    and after optimization:
    
      34:  rsb    ip, r2, #32
      38:  subs   r9, r2, #32
      3c:  lsr    lr, r0, r2
      40:  orr    lr, lr, r1, lsl ip
      44:  orrpl  lr, lr, r1, asr r9
      48:  asr    ip, r1, r2
      4c:  mov    r0, lr
      50:  mov    r1, ip
    
    Tested on QEMU using lib/test_bpf and test_verifier.
    
    Co-developed-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Daniel Borkmann &lt;daniel@iogearbox.net&gt;
    Link: https://lore.kernel.org/bpf/20200501020210.32294-2-luke.r.nels@gmail.com

diff --git a/arch/arm/net/bpf_jit_32.c b/arch/arm/net/bpf_jit_32.c
index bf85d6db4931..48b89211ee5c 100644
--- a/arch/arm/net/bpf_jit_32.c
+++ b/arch/arm/net/bpf_jit_32.c
@@ -860,8 +860,8 @@ static inline void emit_a32_arsh_r64(const s8 dst[], const s8 src[],
 	emit(ARM_SUBS_I(tmp2[0], rt, 32), ctx);
 	emit(ARM_MOV_SR(ARM_LR, rd[1], SRTYPE_LSR, rt), ctx);
 	emit(ARM_ORR_SR(ARM_LR, ARM_LR, rd[0], SRTYPE_ASL, ARM_IP), ctx);
-	_emit(ARM_COND_MI, ARM_B(0), ctx);
-	emit(ARM_ORR_SR(ARM_LR, ARM_LR, rd[0], SRTYPE_ASR, tmp2[0]), ctx);
+	_emit(ARM_COND_PL,
+	      ARM_ORR_SR(ARM_LR, ARM_LR, rd[0], SRTYPE_ASR, tmp2[0]), ctx);
 	emit(ARM_MOV_SR(ARM_IP, rd[0], SRTYPE_ASR, rt), ctx);
 
 	arm_bpf_put_reg32(dst_lo, ARM_LR, ctx);</pre><hr><pre>commit 91f658587a962378a410cc7dc90e122a4ccd7cf3
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Wed Apr 29 17:51:27 2020 -0700

    bpf, riscv: Fix stack layout of JITed code on RV32
    
    This patch fixes issues with stackframe unwinding and alignment in the
    current stack layout for BPF programs on RV32.
    
    In the current layout, RV32 fp points to the JIT scratch registers, rather
    than to the callee-saved registers. This breaks stackframe unwinding,
    which expects fp to point just above the saved ra and fp registers.
    
    This patch fixes the issue by moving the callee-saved registers to be
    stored on the top of the stack, pointed to by fp. This satisfies the
    assumptions of stackframe unwinding.
    
    This patch also fixes an issue with the old layout that the stack was
    not aligned to 16 bytes.
    
    Stacktrace from JITed code using the old stack layout:
    
      [   12.196249 ] [&lt;c0402200&gt;] walk_stackframe+0x0/0x96
    
    Stacktrace using the new stack layout:
    
      [   13.062888 ] [&lt;c0402200&gt;] walk_stackframe+0x0/0x96
      [   13.063028 ] [&lt;c04023c6&gt;] show_stack+0x28/0x32
      [   13.063253 ] [&lt;a403e778&gt;] bpf_prog_82b916b2dfa00464+0x80/0x908
      [   13.063417 ] [&lt;c09270b2&gt;] bpf_test_run+0x124/0x39a
      [   13.063553 ] [&lt;c09276c0&gt;] bpf_prog_test_run_skb+0x234/0x448
      [   13.063704 ] [&lt;c048510e&gt;] __do_sys_bpf+0x766/0x13b4
      [   13.063840 ] [&lt;c0485d82&gt;] sys_bpf+0xc/0x14
      [   13.063961 ] [&lt;c04010f0&gt;] ret_from_syscall+0x0/0x2
    
    The new code is also simpler to understand and includes an ASCII diagram
    of the stack layout.
    
    Tested on riscv32 QEMU virt machine.
    
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Daniel Borkmann &lt;daniel@iogearbox.net&gt;
    Acked-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Link: https://lore.kernel.org/bpf/20200430005127.2205-1-luke.r.nels@gmail.com

diff --git a/arch/riscv/net/bpf_jit_comp32.c b/arch/riscv/net/bpf_jit_comp32.c
index 11083d4d5f2d..b198eaa74456 100644
--- a/arch/riscv/net/bpf_jit_comp32.c
+++ b/arch/riscv/net/bpf_jit_comp32.c
@@ -13,8 +13,35 @@
 #include &lt;linux/filter.h&gt;
 #include "bpf_jit.h"
 
+/*
+ * Stack layout during BPF program execution:
+ *
+ *                     high
+ *     RV32 fp =&gt;  +----------+
+ *                 | saved ra |
+ *                 | saved fp | RV32 callee-saved registers
+ *                 |   ...    |
+ *                 +----------+ &lt;= (fp - 4 * NR_SAVED_REGISTERS)
+ *                 |  hi(R6)  |
+ *                 |  lo(R6)  |
+ *                 |  hi(R7)  | JIT scratch space for BPF registers
+ *                 |  lo(R7)  |
+ *                 |   ...    |
+ *  BPF_REG_FP =&gt;  +----------+ &lt;= (fp - 4 * NR_SAVED_REGISTERS
+ *                 |          |        - 4 * BPF_JIT_SCRATCH_REGS)
+ *                 |          |
+ *                 |   ...    | BPF program stack
+ *                 |          |
+ *     RV32 sp =&gt;  +----------+
+ *                 |          |
+ *                 |   ...    | Function call stack
+ *                 |          |
+ *                 +----------+
+ *                     low
+ */
+
 enum {
-	/* Stack layout - these are offsets from (top of stack - 4). */
+	/* Stack layout - these are offsets from top of JIT scratch space. */
 	BPF_R6_HI,
 	BPF_R6_LO,
 	BPF_R7_HI,
@@ -29,7 +56,11 @@ enum {
 	BPF_JIT_SCRATCH_REGS,
 };
 
-#define STACK_OFFSET(k) (-4 - ((k) * 4))
+/* Number of callee-saved registers stored to stack: ra, fp, s1--s7. */
+#define NR_SAVED_REGISTERS	9
+
+/* Offset from fp for BPF registers stored on stack. */
+#define STACK_OFFSET(k)	(-4 - (4 * NR_SAVED_REGISTERS) - (4 * (k)))
 
 #define TMP_REG_1	(MAX_BPF_JIT_REG + 0)
 #define TMP_REG_2	(MAX_BPF_JIT_REG + 1)
@@ -111,11 +142,9 @@ static void emit_imm64(const s8 *rd, s32 imm_hi, s32 imm_lo,
 
 static void __build_epilogue(bool is_tail_call, struct rv_jit_context *ctx)
 {
-	int stack_adjust = ctx-&gt;stack_size, store_offset = stack_adjust - 4;
+	int stack_adjust = ctx-&gt;stack_size;
 	const s8 *r0 = bpf2rv32[BPF_REG_0];
 
-	store_offset -= 4 * BPF_JIT_SCRATCH_REGS;
-
 	/* Set return value if not tail call. */
 	if (!is_tail_call) {
 		emit(rv_addi(RV_REG_A0, lo(r0), 0), ctx);
@@ -123,15 +152,15 @@ static void __build_epilogue(bool is_tail_call, struct rv_jit_context *ctx)
 	}
 
 	/* Restore callee-saved registers. */
-	emit(rv_lw(RV_REG_RA, store_offset - 0, RV_REG_SP), ctx);
-	emit(rv_lw(RV_REG_FP, store_offset - 4, RV_REG_SP), ctx);
-	emit(rv_lw(RV_REG_S1, store_offset - 8, RV_REG_SP), ctx);
-	emit(rv_lw(RV_REG_S2, store_offset - 12, RV_REG_SP), ctx);
-	emit(rv_lw(RV_REG_S3, store_offset - 16, RV_REG_SP), ctx);
-	emit(rv_lw(RV_REG_S4, store_offset - 20, RV_REG_SP), ctx);
-	emit(rv_lw(RV_REG_S5, store_offset - 24, RV_REG_SP), ctx);
-	emit(rv_lw(RV_REG_S6, store_offset - 28, RV_REG_SP), ctx);
-	emit(rv_lw(RV_REG_S7, store_offset - 32, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_RA, stack_adjust - 4, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_FP, stack_adjust - 8, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_S1, stack_adjust - 12, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_S2, stack_adjust - 16, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_S3, stack_adjust - 20, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_S4, stack_adjust - 24, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_S5, stack_adjust - 28, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_S6, stack_adjust - 32, RV_REG_SP), ctx);
+	emit(rv_lw(RV_REG_S7, stack_adjust - 36, RV_REG_SP), ctx);
 
 	emit(rv_addi(RV_REG_SP, RV_REG_SP, stack_adjust), ctx);
 
@@ -1260,17 +1289,20 @@ int bpf_jit_emit_insn(const struct bpf_insn *insn, struct rv_jit_context *ctx,
 
 void bpf_jit_build_prologue(struct rv_jit_context *ctx)
 {
-	/* Make space to save 9 registers: ra, fp, s1--s7. */
-	int stack_adjust = 9 * sizeof(u32), store_offset, bpf_stack_adjust;
 	const s8 *fp = bpf2rv32[BPF_REG_FP];
 	const s8 *r1 = bpf2rv32[BPF_REG_1];
-
-	bpf_stack_adjust = round_up(ctx-&gt;prog-&gt;aux-&gt;stack_depth, 16);
+	int stack_adjust = 0;
+	int bpf_stack_adjust =
+		round_up(ctx-&gt;prog-&gt;aux-&gt;stack_depth, STACK_ALIGN);
+
+	/* Make space for callee-saved registers. */
+	stack_adjust += NR_SAVED_REGISTERS * sizeof(u32);
+	/* Make space for BPF registers on stack. */
+	stack_adjust += BPF_JIT_SCRATCH_REGS * sizeof(u32);
+	/* Make space for BPF stack. */
 	stack_adjust += bpf_stack_adjust;
-
-	store_offset = stack_adjust - 4;
-
-	stack_adjust += 4 * BPF_JIT_SCRATCH_REGS;
+	/* Round up for stack alignment. */
+	stack_adjust = round_up(stack_adjust, STACK_ALIGN);
 
 	/*
 	 * The first instruction sets the tail-call-counter (TCC) register.
@@ -1281,24 +1313,24 @@ void bpf_jit_build_prologue(struct rv_jit_context *ctx)
 	emit(rv_addi(RV_REG_SP, RV_REG_SP, -stack_adjust), ctx);
 
 	/* Save callee-save registers. */
-	emit(rv_sw(RV_REG_SP, store_offset - 0, RV_REG_RA), ctx);
-	emit(rv_sw(RV_REG_SP, store_offset - 4, RV_REG_FP), ctx);
-	emit(rv_sw(RV_REG_SP, store_offset - 8, RV_REG_S1), ctx);
-	emit(rv_sw(RV_REG_SP, store_offset - 12, RV_REG_S2), ctx);
-	emit(rv_sw(RV_REG_SP, store_offset - 16, RV_REG_S3), ctx);
-	emit(rv_sw(RV_REG_SP, store_offset - 20, RV_REG_S4), ctx);
-	emit(rv_sw(RV_REG_SP, store_offset - 24, RV_REG_S5), ctx);
-	emit(rv_sw(RV_REG_SP, store_offset - 28, RV_REG_S6), ctx);
-	emit(rv_sw(RV_REG_SP, store_offset - 32, RV_REG_S7), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 4, RV_REG_RA), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 8, RV_REG_FP), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 12, RV_REG_S1), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 16, RV_REG_S2), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 20, RV_REG_S3), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 24, RV_REG_S4), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 28, RV_REG_S5), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 32, RV_REG_S6), ctx);
+	emit(rv_sw(RV_REG_SP, stack_adjust - 36, RV_REG_S7), ctx);
 
 	/* Set fp: used as the base address for stacked BPF registers. */
 	emit(rv_addi(RV_REG_FP, RV_REG_SP, stack_adjust), ctx);
 
-	/* Set up BPF stack pointer. */
+	/* Set up BPF frame pointer. */
 	emit(rv_addi(lo(fp), RV_REG_SP, bpf_stack_adjust), ctx);
 	emit(rv_addi(hi(fp), RV_REG_ZERO, 0), ctx);
 
-	/* Set up context pointer. */
+	/* Set up BPF context pointer. */
 	emit(rv_addi(lo(r1), RV_REG_A0, 0), ctx);
 	emit(rv_addi(hi(r1), RV_REG_ZERO, 0), ctx);
 </pre><hr><pre>commit 745abfaa9eafa597d31fdf24a3249e5206a98768
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Mon Apr 20 17:28:04 2020 -0700

    bpf, riscv: Fix tail call count off by one in RV32 BPF JIT
    
    This patch fixes an off by one error in the RV32 JIT handling for BPF
    tail call. Currently, the code decrements TCC before checking if it
    is less than zero. This limits the maximum number of tail calls to 32
    instead of 33 as in other JITs. The fix is to instead check the old
    value of TCC before decrementing.
    
    Fixes: 5f316b65e99f ("riscv, bpf: Add RV32G eBPF JIT")
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Alexei Starovoitov &lt;ast@kernel.org&gt;
    Acked-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Link: https://lore.kernel.org/bpf/20200421002804.5118-1-luke.r.nels@gmail.com

diff --git a/arch/riscv/net/bpf_jit_comp32.c b/arch/riscv/net/bpf_jit_comp32.c
index 302934177760..11083d4d5f2d 100644
--- a/arch/riscv/net/bpf_jit_comp32.c
+++ b/arch/riscv/net/bpf_jit_comp32.c
@@ -770,12 +770,13 @@ static int emit_bpf_tail_call(int insn, struct rv_jit_context *ctx)
 	emit_bcc(BPF_JGE, lo(idx_reg), RV_REG_T1, off, ctx);
 
 	/*
-	 * if ((temp_tcc = tcc - 1) &lt; 0)
+	 * temp_tcc = tcc - 1;
+	 * if (tcc &lt; 0)
 	 *   goto out;
 	 */
 	emit(rv_addi(RV_REG_T1, RV_REG_TCC, -1), ctx);
 	off = (tc_ninsn - (ctx-&gt;ninsns - start_insn)) &lt;&lt; 2;
-	emit_bcc(BPF_JSLT, RV_REG_T1, RV_REG_ZERO, off, ctx);
+	emit_bcc(BPF_JSLT, RV_REG_TCC, RV_REG_ZERO, off, ctx);
 
 	/*
 	 * prog = array-&gt;ptrs[index];</pre><hr><pre>commit 50fe7ebb6475711c15b3397467e6424e20026d94
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Wed Apr 22 10:36:30 2020 -0700

    bpf, x86_32: Fix clobbering of dst for BPF_JSET
    
    The current JIT clobbers the destination register for BPF_JSET BPF_X
    and BPF_K by using "and" and "or" instructions. This is fine when the
    destination register is a temporary loaded from a register stored on
    the stack but not otherwise.
    
    This patch fixes the problem (for both BPF_K and BPF_X) by always loading
    the destination register into temporaries since BPF_JSET should not
    modify the destination register.
    
    This bug may not be currently triggerable as BPF_REG_AX is the only
    register not stored on the stack and the verifier uses it in a limited
    way.
    
    Fixes: 03f5781be2c7b ("bpf, x86_32: add eBPF JIT compiler for ia32")
    Signed-off-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Alexei Starovoitov &lt;ast@kernel.org&gt;
    Acked-by: Wang YanQing &lt;udknight@gmail.com&gt;
    Link: https://lore.kernel.org/bpf/20200422173630.8351-2-luke.r.nels@gmail.com

diff --git a/arch/x86/net/bpf_jit_comp32.c b/arch/x86/net/bpf_jit_comp32.c
index cc9ad3892ea6..ba7d9ccfc662 100644
--- a/arch/x86/net/bpf_jit_comp32.c
+++ b/arch/x86/net/bpf_jit_comp32.c
@@ -2015,8 +2015,8 @@ static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image,
 		case BPF_JMP | BPF_JSET | BPF_X:
 		case BPF_JMP32 | BPF_JSET | BPF_X: {
 			bool is_jmp64 = BPF_CLASS(insn-&gt;code) == BPF_JMP;
-			u8 dreg_lo = dstk ? IA32_EAX : dst_lo;
-			u8 dreg_hi = dstk ? IA32_EDX : dst_hi;
+			u8 dreg_lo = IA32_EAX;
+			u8 dreg_hi = IA32_EDX;
 			u8 sreg_lo = sstk ? IA32_ECX : src_lo;
 			u8 sreg_hi = sstk ? IA32_EBX : src_hi;
 
@@ -2028,6 +2028,13 @@ static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image,
 					      add_2reg(0x40, IA32_EBP,
 						       IA32_EDX),
 					      STACK_VAR(dst_hi));
+			} else {
+				/* mov dreg_lo,dst_lo */
+				EMIT2(0x89, add_2reg(0xC0, dreg_lo, dst_lo));
+				if (is_jmp64)
+					/* mov dreg_hi,dst_hi */
+					EMIT2(0x89,
+					      add_2reg(0xC0, dreg_hi, dst_hi));
 			}
 
 			if (sstk) {
@@ -2052,8 +2059,8 @@ static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image,
 		case BPF_JMP | BPF_JSET | BPF_K:
 		case BPF_JMP32 | BPF_JSET | BPF_K: {
 			bool is_jmp64 = BPF_CLASS(insn-&gt;code) == BPF_JMP;
-			u8 dreg_lo = dstk ? IA32_EAX : dst_lo;
-			u8 dreg_hi = dstk ? IA32_EDX : dst_hi;
+			u8 dreg_lo = IA32_EAX;
+			u8 dreg_hi = IA32_EDX;
 			u8 sreg_lo = IA32_ECX;
 			u8 sreg_hi = IA32_EBX;
 			u32 hi;
@@ -2066,6 +2073,13 @@ static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image,
 					      add_2reg(0x40, IA32_EBP,
 						       IA32_EDX),
 					      STACK_VAR(dst_hi));
+			} else {
+				/* mov dreg_lo,dst_lo */
+				EMIT2(0x89, add_2reg(0xC0, dreg_lo, dst_lo));
+				if (is_jmp64)
+					/* mov dreg_hi,dst_hi */
+					EMIT2(0x89,
+					      add_2reg(0xC0, dreg_hi, dst_hi));
 			}
 
 			/* mov ecx,imm32 */</pre><hr><pre>commit 5fa9a98fb10380e48a398998cd36a85e4ef711d6
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Wed Apr 22 10:36:29 2020 -0700

    bpf, x86_32: Fix incorrect encoding in BPF_LDX zero-extension
    
    The current JIT uses the following sequence to zero-extend into the
    upper 32 bits of the destination register for BPF_LDX BPF_{B,H,W},
    when the destination register is not on the stack:
    
      EMIT3(0xC7, add_1reg(0xC0, dst_hi), 0);
    
    The problem is that C7 /0 encodes a MOV instruction that requires a 4-byte
    immediate; the current code emits only 1 byte of the immediate. This
    means that the first 3 bytes of the next instruction will be treated as
    the rest of the immediate, breaking the stream of instructions.
    
    This patch fixes the problem by instead emitting "xor dst_hi,dst_hi"
    to clear the upper 32 bits. This fixes the problem and is more efficient
    than using MOV to load a zero immediate.
    
    This bug may not be currently triggerable as BPF_REG_AX is the only
    register not stored on the stack and the verifier uses it in a limited
    way, and the verifier implements a zero-extension optimization. But the
    JIT should avoid emitting incorrect encodings regardless.
    
    Fixes: 03f5781be2c7b ("bpf, x86_32: add eBPF JIT compiler for ia32")
    Signed-off-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Alexei Starovoitov &lt;ast@kernel.org&gt;
    Reviewed-by: H. Peter Anvin (Intel) &lt;hpa@zytor.com&gt;
    Acked-by: Wang YanQing &lt;udknight@gmail.com&gt;
    Link: https://lore.kernel.org/bpf/20200422173630.8351-1-luke.r.nels@gmail.com

diff --git a/arch/x86/net/bpf_jit_comp32.c b/arch/x86/net/bpf_jit_comp32.c
index 4d2a7a764602..cc9ad3892ea6 100644
--- a/arch/x86/net/bpf_jit_comp32.c
+++ b/arch/x86/net/bpf_jit_comp32.c
@@ -1854,7 +1854,9 @@ static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image,
 					      STACK_VAR(dst_hi));
 					EMIT(0x0, 4);
 				} else {
-					EMIT3(0xC7, add_1reg(0xC0, dst_hi), 0);
+					/* xor dst_hi,dst_hi */
+					EMIT2(0x33,
+					      add_2reg(0xC0, dst_hi, dst_hi));
 				}
 				break;
 			case BPF_DW:</pre><hr><pre>commit d2b6c3ab70dbc0069a69c57edd8c96f365f06b7c
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Sat Apr 18 16:26:54 2020 -0700

    bpf, selftests: Add test for BPF_STX BPF_B storing R10
    
    This patch adds a test to test_verifier that writes the lower 8 bits of
    R10 (aka FP) using BPF_B to an array map and reads the result back. The
    expected behavior is that the result should be the same as first copying
    R10 to R9, and then storing / loading the lower 8 bits of R9.
    
    This test catches a bug that was present in the x86-64 JIT that caused
    an incorrect encoding for BPF_STX BPF_B when the source operand is R10.
    
    Signed-off-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Alexei Starovoitov &lt;ast@kernel.org&gt;
    Link: https://lore.kernel.org/bpf/20200418232655.23870-2-luke.r.nels@gmail.com

diff --git a/tools/testing/selftests/bpf/verifier/stack_ptr.c b/tools/testing/selftests/bpf/verifier/stack_ptr.c
index 7276620ef242..8bfeb77c60bd 100644
--- a/tools/testing/selftests/bpf/verifier/stack_ptr.c
+++ b/tools/testing/selftests/bpf/verifier/stack_ptr.c
@@ -315,3 +315,43 @@
 	},
 	.result = ACCEPT,
 },
+{
+	"store PTR_TO_STACK in R10 to array map using BPF_B",
+	.insns = {
+	/* Load pointer to map. */
+	BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),
+	BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -8),
+	BPF_ST_MEM(BPF_DW, BPF_REG_2, 0, 0),
+	BPF_LD_MAP_FD(BPF_REG_1, 0),
+	BPF_EMIT_CALL(BPF_FUNC_map_lookup_elem),
+	BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 2),
+	BPF_MOV64_IMM(BPF_REG_0, 2),
+	BPF_EXIT_INSN(),
+	BPF_MOV64_REG(BPF_REG_1, BPF_REG_0),
+	/* Copy R10 to R9. */
+	BPF_MOV64_REG(BPF_REG_9, BPF_REG_10),
+	/* Pollute other registers with unaligned values. */
+	BPF_MOV64_IMM(BPF_REG_2, -1),
+	BPF_MOV64_IMM(BPF_REG_3, -1),
+	BPF_MOV64_IMM(BPF_REG_4, -1),
+	BPF_MOV64_IMM(BPF_REG_5, -1),
+	BPF_MOV64_IMM(BPF_REG_6, -1),
+	BPF_MOV64_IMM(BPF_REG_7, -1),
+	BPF_MOV64_IMM(BPF_REG_8, -1),
+	/* Store both R9 and R10 with BPF_B and read back. */
+	BPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_10, 0),
+	BPF_LDX_MEM(BPF_B, BPF_REG_2, BPF_REG_1, 0),
+	BPF_STX_MEM(BPF_B, BPF_REG_1, BPF_REG_9, 0),
+	BPF_LDX_MEM(BPF_B, BPF_REG_3, BPF_REG_1, 0),
+	/* Should read back as same value. */
+	BPF_JMP_REG(BPF_JEQ, BPF_REG_2, BPF_REG_3, 2),
+	BPF_MOV64_IMM(BPF_REG_0, 1),
+	BPF_EXIT_INSN(),
+	BPF_MOV64_IMM(BPF_REG_0, 42),
+	BPF_EXIT_INSN(),
+	},
+	.fixup_map_array_48b = { 3 },
+	.result = ACCEPT,
+	.retval = 42,
+	.prog_type = BPF_PROG_TYPE_SCHED_CLS,
+},</pre><hr><pre>commit aee194b14dd2b2bde6252b3acf57d36dccfc743a
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Sat Apr 18 16:26:53 2020 -0700

    bpf, x86: Fix encoding for lower 8-bit registers in BPF_STX BPF_B
    
    This patch fixes an encoding bug in emit_stx for BPF_B when the source
    register is BPF_REG_FP.
    
    The current implementation for BPF_STX BPF_B in emit_stx saves one REX
    byte when the operands can be encoded using Mod-R/M alone. The lower 8
    bits of registers %rax, %rbx, %rcx, and %rdx can be accessed without using
    a REX prefix via %al, %bl, %cl, and %dl, respectively. Other registers,
    (e.g., %rsi, %rdi, %rbp, %rsp) require a REX prefix to use their 8-bit
    equivalents (%sil, %dil, %bpl, %spl).
    
    The current code checks if the source for BPF_STX BPF_B is BPF_REG_1
    or BPF_REG_2 (which map to %rdi and %rsi), in which case it emits the
    required REX prefix. However, it misses the case when the source is
    BPF_REG_FP (mapped to %rbp).
    
    The result is that BPF_STX BPF_B with BPF_REG_FP as the source operand
    will read from register %ch instead of the correct %bpl. This patch fixes
    the problem by fixing and refactoring the check on which registers need
    the extra REX byte. Since no BPF registers map to %rsp, there is no need
    to handle %spl.
    
    Fixes: 622582786c9e0 ("net: filter: x86: internal BPF JIT")
    Signed-off-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Alexei Starovoitov &lt;ast@kernel.org&gt;
    Link: https://lore.kernel.org/bpf/20200418232655.23870-1-luke.r.nels@gmail.com

diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index 5ea7c2cf7ab4..42b6709e6dc7 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -158,6 +158,19 @@ static bool is_ereg(u32 reg)
 			     BIT(BPF_REG_AX));
 }
 
+/*
+ * is_ereg_8l() == true if BPF register 'reg' is mapped to access x86-64
+ * lower 8-bit registers dil,sil,bpl,spl,r8b..r15b, which need extra byte
+ * of encoding. al,cl,dl,bl have simpler encoding.
+ */
+static bool is_ereg_8l(u32 reg)
+{
+	return is_ereg(reg) ||
+	    (1 &lt;&lt; reg) &amp; (BIT(BPF_REG_1) |
+			  BIT(BPF_REG_2) |
+			  BIT(BPF_REG_FP));
+}
+
 static bool is_axreg(u32 reg)
 {
 	return reg == BPF_REG_0;
@@ -598,9 +611,8 @@ static void emit_stx(u8 **pprog, u32 size, u32 dst_reg, u32 src_reg, int off)
 	switch (size) {
 	case BPF_B:
 		/* Emit 'mov byte ptr [rax + off], al' */
-		if (is_ereg(dst_reg) || is_ereg(src_reg) ||
-		    /* We have to add extra byte for x86 SIL, DIL regs */
-		    src_reg == BPF_REG_1 || src_reg == BPF_REG_2)
+		if (is_ereg(dst_reg) || is_ereg_8l(src_reg))
+			/* Add extra byte for eregs or SIL,DIL,BPL in src_reg */
 			EMIT2(add_2mod(0x40, dst_reg, src_reg), 0x88);
 		else
 			EMIT1(0x88);</pre><hr><pre>commit 4178417cc5359c329790a4a8f4a6604612338cca
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Thu Apr 9 15:17:52 2020 -0700

    arm, bpf: Fix offset overflow for BPF_MEM BPF_DW
    
    This patch fixes an incorrect check in how immediate memory offsets are
    computed for BPF_DW on arm.
    
    For BPF_LDX/ST/STX + BPF_DW, the 32-bit arm JIT breaks down an 8-byte
    access into two separate 4-byte accesses using off+0 and off+4. If off
    fits in imm12, the JIT emits a ldr/str instruction with the immediate
    and avoids the use of a temporary register. While the current check off
    &lt;= 0xfff ensures that the first immediate off+0 doesn't overflow imm12,
    it's not sufficient for the second immediate off+4, which may cause the
    second access of BPF_DW to read/write the wrong address.
    
    This patch fixes the problem by changing the check to
    off &lt;= 0xfff - 4 for BPF_DW, ensuring off+4 will never overflow.
    
    A side effect of simplifying the check is that it now allows using
    negative immediate offsets in ldr/str. This means that small negative
    offsets can also avoid the use of a temporary register.
    
    This patch introduces no new failures in test_verifier or test_bpf.c.
    
    Fixes: c5eae692571d6 ("ARM: net: bpf: improve 64-bit store implementation")
    Fixes: ec19e02b343db ("ARM: net: bpf: fix LDX instructions")
    Co-developed-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Daniel Borkmann &lt;daniel@iogearbox.net&gt;
    Link: https://lore.kernel.org/bpf/20200409221752.28448-1-luke.r.nels@gmail.com

diff --git a/arch/arm/net/bpf_jit_32.c b/arch/arm/net/bpf_jit_32.c
index d124f78e20ac..bf85d6db4931 100644
--- a/arch/arm/net/bpf_jit_32.c
+++ b/arch/arm/net/bpf_jit_32.c
@@ -1000,21 +1000,35 @@ static inline void emit_a32_mul_r64(const s8 dst[], const s8 src[],
 	arm_bpf_put_reg32(dst_hi, rd[0], ctx);
 }
 
+static bool is_ldst_imm(s16 off, const u8 size)
+{
+	s16 off_max = 0;
+
+	switch (size) {
+	case BPF_B:
+	case BPF_W:
+		off_max = 0xfff;
+		break;
+	case BPF_H:
+		off_max = 0xff;
+		break;
+	case BPF_DW:
+		/* Need to make sure off+4 does not overflow. */
+		off_max = 0xfff - 4;
+		break;
+	}
+	return -off_max &lt;= off &amp;&amp; off &lt;= off_max;
+}
+
 /* *(size *)(dst + off) = src */
 static inline void emit_str_r(const s8 dst, const s8 src[],
-			      s32 off, struct jit_ctx *ctx, const u8 sz){
+			      s16 off, struct jit_ctx *ctx, const u8 sz){
 	const s8 *tmp = bpf2a32[TMP_REG_1];
-	s32 off_max;
 	s8 rd;
 
 	rd = arm_bpf_get_reg32(dst, tmp[1], ctx);
 
-	if (sz == BPF_H)
-		off_max = 0xff;
-	else
-		off_max = 0xfff;
-
-	if (off &lt; 0 || off &gt; off_max) {
+	if (!is_ldst_imm(off, sz)) {
 		emit_a32_mov_i(tmp[0], off, ctx);
 		emit(ARM_ADD_R(tmp[0], tmp[0], rd), ctx);
 		rd = tmp[0];
@@ -1043,18 +1057,12 @@ static inline void emit_str_r(const s8 dst, const s8 src[],
 
 /* dst = *(size*)(src + off) */
 static inline void emit_ldx_r(const s8 dst[], const s8 src,
-			      s32 off, struct jit_ctx *ctx, const u8 sz){
+			      s16 off, struct jit_ctx *ctx, const u8 sz){
 	const s8 *tmp = bpf2a32[TMP_REG_1];
 	const s8 *rd = is_stacked(dst_lo) ? tmp : dst;
 	s8 rm = src;
-	s32 off_max;
-
-	if (sz == BPF_H)
-		off_max = 0xff;
-	else
-		off_max = 0xfff;
 
-	if (off &lt; 0 || off &gt; off_max) {
+	if (!is_ldst_imm(off, sz)) {
 		emit_a32_mov_i(tmp[0], off, ctx);
 		emit(ARM_ADD_R(tmp[0], tmp[0], src), ctx);
 		rm = tmp[0];</pre><hr><pre>commit bb9562cf5c67813034c96afb50bd21130a504441
Author: Luke Nelson &lt;lukenels@cs.washington.edu&gt;
Date:   Wed Apr 8 18:12:29 2020 +0000

    arm, bpf: Fix bugs with ALU64 {RSH, ARSH} BPF_K shift by 0
    
    The current arm BPF JIT does not correctly compile RSH or ARSH when the
    immediate shift amount is 0. This causes the "rsh64 by 0 imm" and "arsh64
    by 0 imm" BPF selftests to hang the kernel by reaching an instruction
    the verifier determines to be unreachable.
    
    The root cause is in how immediate right shifts are encoded on arm.
    For LSR and ASR (logical and arithmetic right shift), a bit-pattern
    of 00000 in the immediate encodes a shift amount of 32. When the BPF
    immediate is 0, the generated code shifts by 32 instead of the expected
    behavior (a no-op).
    
    This patch fixes the bugs by adding an additional check if the BPF
    immediate is 0. After the change, the above mentioned BPF selftests pass.
    
    Fixes: 39c13c204bb11 ("arm: eBPF JIT compiler")
    Co-developed-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Xi Wang &lt;xi.wang@gmail.com&gt;
    Signed-off-by: Luke Nelson &lt;luke.r.nels@gmail.com&gt;
    Signed-off-by: Daniel Borkmann &lt;daniel@iogearbox.net&gt;
    Link: https://lore.kernel.org/bpf/20200408181229.10909-1-luke.r.nels@gmail.com

diff --git a/arch/arm/net/bpf_jit_32.c b/arch/arm/net/bpf_jit_32.c
index cc29869d12a3..d124f78e20ac 100644
--- a/arch/arm/net/bpf_jit_32.c
+++ b/arch/arm/net/bpf_jit_32.c
@@ -929,7 +929,11 @@ static inline void emit_a32_rsh_i64(const s8 dst[],
 	rd = arm_bpf_get_reg64(dst, tmp, ctx);
 
 	/* Do LSR operation */
-	if (val &lt; 32) {
+	if (val == 0) {
+		/* An immediate value of 0 encodes a shift amount of 32
+		 * for LSR. To shift by 0, don't do anything.
+		 */
+	} else if (val &lt; 32) {
 		emit(ARM_MOV_SI(tmp2[1], rd[1], SRTYPE_LSR, val), ctx);
 		emit(ARM_ORR_SI(rd[1], tmp2[1], rd[0], SRTYPE_ASL, 32 - val), ctx);
 		emit(ARM_MOV_SI(rd[0], rd[0], SRTYPE_LSR, val), ctx);
@@ -955,7 +959,11 @@ static inline void emit_a32_arsh_i64(const s8 dst[],
 	rd = arm_bpf_get_reg64(dst, tmp, ctx);
 
 	/* Do ARSH operation */
-	if (val &lt; 32) {
+	if (val == 0) {
+		/* An immediate value of 0 encodes a shift amount of 32
+		 * for ASR. To shift by 0, don't do anything.
+		 */
+	} else if (val &lt; 32) {
 		emit(ARM_MOV_SI(tmp2[1], rd[1], SRTYPE_LSR, val), ctx);
 		emit(ARM_ORR_SI(rd[1], tmp2[1], rd[0], SRTYPE_ASL, 32 - val), ctx);
 		emit(ARM_MOV_SI(rd[0], rd[0], SRTYPE_ASR, val), ctx);</pre>
    <div class="pagination">
        <a href='13_2.html'>&lt;&lt;Prev</a><a href='13.html'>1</a><a href='13_2.html'>2</a><span>[3]</span><a href='13_4.html'>4</a><a href='13_5.html'>5</a><a href='13_6.html'>6</a><a href='13_7.html'>7</a><a href='13_8.html'>8</a><a href='13_4.html'>Next&gt;&gt;</a>
    <div>
</body>

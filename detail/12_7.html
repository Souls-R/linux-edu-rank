<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by University of Science and Technology of China</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by University of Science and Technology of China</h1>
    <div class="pagination">
        <a href='12_6.html'>&lt;&lt;Prev</a><a href='12.html'>1</a><a href='12_2.html'>2</a><a href='12_3.html'>3</a><a href='12_4.html'>4</a><a href='12_5.html'>5</a><a href='12_6.html'>6</a><span>[7]</span><a href='12_8.html'>8</a><a href='12_9.html'>9</a><a href='12_8.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 6df8ba4f8a4c4abca9ccad10441d0dddbdff301c
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Tue Oct 16 01:24:33 2007 -0700

    radixtree: introduce radix_tree_next_hole()
    
    Introduce radix_tree_next_hole(root, index, max_scan) to scan radix tree for
    the first hole.  It will be used in interleaved readahead.
    
    The implementation is dumb and obviously correct.  It can help debug(and
    document) the possible smart one in future.
    
    Cc: Nick Piggin &lt;nickpiggin@yahoo.com.au&gt;
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/include/linux/radix-tree.h b/include/linux/radix-tree.h
index f9e77d2ee320..430e4a8c1382 100644
--- a/include/linux/radix-tree.h
+++ b/include/linux/radix-tree.h
@@ -155,6 +155,8 @@ void *radix_tree_delete(struct radix_tree_root *, unsigned long);
 unsigned int
 radix_tree_gang_lookup(struct radix_tree_root *root, void **results,
 			unsigned long first_index, unsigned int max_items);
+unsigned long radix_tree_next_hole(struct radix_tree_root *root,
+				unsigned long index, unsigned long max_scan);
 int radix_tree_preload(gfp_t gfp_mask);
 void radix_tree_init(void);
 void *radix_tree_tag_set(struct radix_tree_root *root,
diff --git a/lib/radix-tree.c b/lib/radix-tree.c
index 514efb200be6..7af368a4401b 100644
--- a/lib/radix-tree.c
+++ b/lib/radix-tree.c
@@ -599,6 +599,42 @@ int radix_tree_tag_get(struct radix_tree_root *root,
 EXPORT_SYMBOL(radix_tree_tag_get);
 #endif
 
+/**
+ *	radix_tree_next_hole    -    find the next hole (not-present entry)
+ *	@root:		tree root
+ *	@index:		index key
+ *	@max_scan:	maximum range to search
+ *
+ *	Search the set [index, min(index+max_scan-1, MAX_INDEX)] for the lowest
+ *	indexed hole.
+ *
+ *	Returns: the index of the hole if found, otherwise returns an index
+ *	outside of the set specified (in which case 'return - index &gt;= max_scan'
+ *	will be true).
+ *
+ *	radix_tree_next_hole may be called under rcu_read_lock. However, like
+ *	radix_tree_gang_lookup, this will not atomically search a snapshot of the
+ *	tree at a single point in time. For example, if a hole is created at index
+ *	5, then subsequently a hole is created at index 10, radix_tree_next_hole
+ *	covering both indexes may return 10 if called under rcu_read_lock.
+ */
+unsigned long radix_tree_next_hole(struct radix_tree_root *root,
+				unsigned long index, unsigned long max_scan)
+{
+	unsigned long i;
+
+	for (i = 0; i &lt; max_scan; i++) {
+		if (!radix_tree_lookup(root, index))
+			break;
+		index++;
+		if (index == 0)
+			break;
+	}
+
+	return index;
+}
+EXPORT_SYMBOL(radix_tree_next_hole);
+
 static unsigned int
 __lookup(struct radix_tree_node *slot, void **results, unsigned long index,
 	unsigned int max_items, unsigned long *next_index)</pre><hr><pre>commit f4e6b498d6e06742d72706ef50593a9c4dd72214
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Tue Oct 16 01:24:33 2007 -0700

    readahead: combine file_ra_state.prev_index/prev_offset into prev_pos
    
    Combine the file_ra_state members
                                    unsigned long prev_index
                                    unsigned int prev_offset
    into
                                    loff_t prev_pos
    
    It is more consistent and better supports huge files.
    
    Thanks to Peter for the nice proposal!
    
    [akpm@linux-foundation.org: fix shift overflow]
    Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/fs/ext3/dir.c b/fs/ext3/dir.c
index c00723a99f44..c2c3491b18cf 100644
--- a/fs/ext3/dir.c
+++ b/fs/ext3/dir.c
@@ -143,7 +143,7 @@ static int ext3_readdir(struct file * filp,
 					sb-&gt;s_bdev-&gt;bd_inode-&gt;i_mapping,
 					&amp;filp-&gt;f_ra, filp,
 					index, 1);
-			filp-&gt;f_ra.prev_index = index;
+			filp-&gt;f_ra.prev_pos = (loff_t)index &lt;&lt; PAGE_CACHE_SHIFT;
 			bh = ext3_bread(NULL, inode, blk, 0, &amp;err);
 		}
 
diff --git a/fs/ext4/dir.c b/fs/ext4/dir.c
index 3ab01c04e00c..e11890acfa21 100644
--- a/fs/ext4/dir.c
+++ b/fs/ext4/dir.c
@@ -142,7 +142,7 @@ static int ext4_readdir(struct file * filp,
 					sb-&gt;s_bdev-&gt;bd_inode-&gt;i_mapping,
 					&amp;filp-&gt;f_ra, filp,
 					index, 1);
-			filp-&gt;f_ra.prev_index = index;
+			filp-&gt;f_ra.prev_pos = (loff_t)index &lt;&lt; PAGE_CACHE_SHIFT;
 			bh = ext4_bread(NULL, inode, blk, 0, &amp;err);
 		}
 
diff --git a/fs/splice.c b/fs/splice.c
index e95a36228863..2df6be43c667 100644
--- a/fs/splice.c
+++ b/fs/splice.c
@@ -447,7 +447,7 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 	 */
 	while (page_nr &lt; nr_pages)
 		page_cache_release(pages[page_nr++]);
-	in-&gt;f_ra.prev_index = index;
+	in-&gt;f_ra.prev_pos = (loff_t)index &lt;&lt; PAGE_CACHE_SHIFT;
 
 	if (spd.nr_pages)
 		return splice_to_pipe(pipe, &amp;spd);
diff --git a/include/linux/fs.h b/include/linux/fs.h
index 8250811081ff..500ffc0e4ac7 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -704,8 +704,7 @@ struct file_ra_state {
 
 	unsigned int ra_pages;		/* Maximum readahead window */
 	int mmap_miss;			/* Cache miss stat for mmap accesses */
-	unsigned long prev_index;	/* Cache last read() position */
-	unsigned int prev_offset;	/* Offset where last read() ended in a page */
+	loff_t prev_pos;		/* Cache last read() position */
 };
 
 /*
diff --git a/mm/filemap.c b/mm/filemap.c
index 5dc18d76e703..bbcca456d8a6 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -879,8 +879,8 @@ void do_generic_mapping_read(struct address_space *mapping,
 	cached_page = NULL;
 	index = *ppos &gt;&gt; PAGE_CACHE_SHIFT;
 	next_index = index;
-	prev_index = ra.prev_index;
-	prev_offset = ra.prev_offset;
+	prev_index = ra.prev_pos &gt;&gt; PAGE_CACHE_SHIFT;
+	prev_offset = ra.prev_pos &amp; (PAGE_CACHE_SIZE-1);
 	last_index = (*ppos + desc-&gt;count + PAGE_CACHE_SIZE-1) &gt;&gt; PAGE_CACHE_SHIFT;
 	offset = *ppos &amp; ~PAGE_CACHE_MASK;
 
@@ -966,7 +966,6 @@ void do_generic_mapping_read(struct address_space *mapping,
 		index += offset &gt;&gt; PAGE_CACHE_SHIFT;
 		offset &amp;= ~PAGE_CACHE_MASK;
 		prev_offset = offset;
-		ra.prev_offset = offset;
 
 		page_cache_release(page);
 		if (ret == nr &amp;&amp; desc-&gt;count)
@@ -1056,9 +1055,11 @@ void do_generic_mapping_read(struct address_space *mapping,
 
 out:
 	*_ra = ra;
-	_ra-&gt;prev_index = prev_index;
+	_ra-&gt;prev_pos = prev_index;
+	_ra-&gt;prev_pos &lt;&lt;= PAGE_CACHE_SHIFT;
+	_ra-&gt;prev_pos |= prev_offset;
 
-	*ppos = ((loff_t) index &lt;&lt; PAGE_CACHE_SHIFT) + offset;
+	*ppos = ((loff_t)index &lt;&lt; PAGE_CACHE_SHIFT) + offset;
 	if (cached_page)
 		page_cache_release(cached_page);
 	if (filp)
@@ -1396,7 +1397,7 @@ int filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	 * Found the page and have a reference on it.
 	 */
 	mark_page_accessed(page);
-	ra-&gt;prev_index = page-&gt;index;
+	ra-&gt;prev_pos = (loff_t)page-&gt;index &lt;&lt; PAGE_CACHE_SHIFT;
 	vmf-&gt;page = page;
 	return ret | VM_FAULT_LOCKED;
 
diff --git a/mm/readahead.c b/mm/readahead.c
index d2504877b269..4a58befbde4a 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -46,7 +46,7 @@ void
 file_ra_state_init(struct file_ra_state *ra, struct address_space *mapping)
 {
 	ra-&gt;ra_pages = mapping-&gt;backing_dev_info-&gt;ra_pages;
-	ra-&gt;prev_index = -1;
+	ra-&gt;prev_pos = -1;
 }
 EXPORT_SYMBOL_GPL(file_ra_state_init);
 
@@ -327,7 +327,7 @@ static unsigned long get_next_ra_size(struct file_ra_state *ra,
  * indicator. The flag won't be set on already cached pages, to avoid the
  * readahead-for-nothing fuss, saving pointless page cache lookups.
  *
- * prev_index tracks the last visited page in the _previous_ read request.
+ * prev_pos tracks the last visited byte in the _previous_ read request.
  * It should be maintained by the caller, and will be used for detecting
  * small random reads. Note that the readahead algorithm checks loosely
  * for sequential patterns. Hence interleaved reads might be served as
@@ -351,11 +351,9 @@ ondemand_readahead(struct address_space *mapping,
 		   bool hit_readahead_marker, pgoff_t offset,
 		   unsigned long req_size)
 {
-	int max;	/* max readahead pages */
-	int sequential;
-
-	max = ra-&gt;ra_pages;
-	sequential = (offset - ra-&gt;prev_index &lt;= 1UL) || (req_size &gt; max);
+	int	max = ra-&gt;ra_pages;	/* max readahead pages */
+	pgoff_t prev_offset;
+	int	sequential;
 
 	/*
 	 * It's the expected callback offset, assume sequential access.
@@ -369,6 +367,9 @@ ondemand_readahead(struct address_space *mapping,
 		goto readit;
 	}
 
+	prev_offset = ra-&gt;prev_pos &gt;&gt; PAGE_CACHE_SHIFT;
+	sequential = offset - prev_offset &lt;= 1UL || req_size &gt; max;
+
 	/*
 	 * Standalone, small read.
 	 * Read as is, and do not pollute the readahead state.</pre><hr><pre>commit 0bb7ba6b9c358c12084a3cbc6ac08c8d1e973937
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Tue Oct 16 01:24:32 2007 -0700

    readahead: mmap read-around simplification
    
    Fold file_ra_state.mmap_hit into file_ra_state.mmap_miss and make it an int.
    
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/include/linux/fs.h b/include/linux/fs.h
index 7a998c49a086..8250811081ff 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -703,8 +703,7 @@ struct file_ra_state {
 					   there are only # of pages ahead */
 
 	unsigned int ra_pages;		/* Maximum readahead window */
-	unsigned long mmap_hit;		/* Cache hit stat for mmap accesses */
-	unsigned long mmap_miss;	/* Cache miss stat for mmap accesses */
+	int mmap_miss;			/* Cache miss stat for mmap accesses */
 	unsigned long prev_index;	/* Cache last read() position */
 	unsigned int prev_offset;	/* Offset where last read() ended in a page */
 };
diff --git a/mm/filemap.c b/mm/filemap.c
index 15c8413ee929..5dc18d76e703 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -1349,7 +1349,7 @@ int filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 		 * Do we miss much more than hit in this file? If so,
 		 * stop bothering with read-ahead. It will only hurt.
 		 */
-		if (ra-&gt;mmap_miss &gt; ra-&gt;mmap_hit + MMAP_LOTSAMISS)
+		if (ra-&gt;mmap_miss &gt; MMAP_LOTSAMISS)
 			goto no_cached_page;
 
 		/*
@@ -1375,7 +1375,7 @@ int filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	}
 
 	if (!did_readaround)
-		ra-&gt;mmap_hit++;
+		ra-&gt;mmap_miss--;
 
 	/*
 	 * We have a locked page in the page cache, now we need to check</pre><hr><pre>commit 937085aa35cc873d427d250a1e304d641af24628
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Tue Oct 16 01:24:31 2007 -0700

    readahead: compacting file_ra_state
    
    Use 'unsigned int' instead of 'unsigned long' for readahead sizes.
    
    This helps reduce memory consumption on 64bit CPU when a lot of files are
    opened.
    
    CC: Andi Kleen &lt;andi@firstfloor.org&gt;
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/include/linux/fs.h b/include/linux/fs.h
index 4a6a21077bae..7a998c49a086 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -697,12 +697,12 @@ struct fown_struct {
  * Track a single file's readahead state
  */
 struct file_ra_state {
-	pgoff_t start;                  /* where readahead started */
-	unsigned long size;             /* # of readahead pages */
-	unsigned long async_size;       /* do asynchronous readahead when
+	pgoff_t start;			/* where readahead started */
+	unsigned int size;		/* # of readahead pages */
+	unsigned int async_size;	/* do asynchronous readahead when
 					   there are only # of pages ahead */
 
-	unsigned long ra_pages;		/* Maximum readahead window */
+	unsigned int ra_pages;		/* Maximum readahead window */
 	unsigned long mmap_hit;		/* Cache hit stat for mmap accesses */
 	unsigned long mmap_miss;	/* Cache miss stat for mmap accesses */
 	unsigned long prev_index;	/* Cache last read() position */
diff --git a/mm/readahead.c b/mm/readahead.c
index be20c9d699d3..d2504877b269 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -351,7 +351,7 @@ ondemand_readahead(struct address_space *mapping,
 		   bool hit_readahead_marker, pgoff_t offset,
 		   unsigned long req_size)
 {
-	unsigned long max;	/* max readahead pages */
+	int max;	/* max readahead pages */
 	int sequential;
 
 	max = ra-&gt;ra_pages;</pre><hr><pre>commit f9acc8c7b35a100f3a9e0e6977f7807b0169f9a5
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Thu Jul 19 01:48:08 2007 -0700

    readahead: sanify file_ra_state names
    
    Rename some file_ra_state variables and remove some accessors.
    
    It results in much simpler code.
    Kudos to Rusty!
    
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/include/linux/fs.h b/include/linux/fs.h
index 29cb32d3a849..d33beadd9a43 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -695,16 +695,12 @@ struct fown_struct {
 
 /*
  * Track a single file's readahead state
- *
- *  ================#============|==================#==================|
- *                  ^            ^                  ^                  ^
- *  file_ra_state.la_index    .ra_index   .lookahead_index   .readahead_index
  */
 struct file_ra_state {
-	pgoff_t la_index;               /* enqueue time */
-	pgoff_t ra_index;               /* begin offset */
-	pgoff_t lookahead_index;        /* time to do next readahead */
-	pgoff_t readahead_index;        /* end offset */
+	pgoff_t start;                  /* where readahead started */
+	unsigned long size;             /* # of readahead pages */
+	unsigned long async_size;       /* do asynchronous readahead when
+					   there are only # of pages ahead */
 
 	unsigned long ra_pages;		/* Maximum readahead window */
 	unsigned long mmap_hit;		/* Cache hit stat for mmap accesses */
@@ -713,60 +709,15 @@ struct file_ra_state {
 	unsigned int prev_offset;	/* Offset where last read() ended in a page */
 };
 
-/*
- * Measuring read-ahead sizes.
- *
- *                  |----------- readahead size ------------&gt;|
- *  ===#============|==================#=====================|
- *     |------- invoke interval ------&gt;|-- lookahead size --&gt;|
- */
-static inline unsigned long ra_readahead_size(struct file_ra_state *ra)
-{
-	return ra-&gt;readahead_index - ra-&gt;ra_index;
-}
-
-static inline unsigned long ra_lookahead_size(struct file_ra_state *ra)
-{
-	return ra-&gt;readahead_index - ra-&gt;lookahead_index;
-}
-
-static inline unsigned long ra_invoke_interval(struct file_ra_state *ra)
-{
-	return ra-&gt;lookahead_index - ra-&gt;la_index;
-}
-
 /*
  * Check if @index falls in the readahead windows.
  */
 static inline int ra_has_index(struct file_ra_state *ra, pgoff_t index)
 {
-	return (index &gt;= ra-&gt;la_index &amp;&amp;
-		index &lt;  ra-&gt;readahead_index);
-}
-
-/*
- * Where is the old read-ahead and look-ahead?
- */
-static inline void ra_set_index(struct file_ra_state *ra,
-				pgoff_t la_index, pgoff_t ra_index)
-{
-	ra-&gt;la_index = la_index;
-	ra-&gt;ra_index = ra_index;
+	return (index &gt;= ra-&gt;start &amp;&amp;
+		index &lt;  ra-&gt;start + ra-&gt;size);
 }
 
-/*
- * Where is the new read-ahead and look-ahead?
- */
-static inline void ra_set_size(struct file_ra_state *ra,
-				unsigned long ra_size, unsigned long la_size)
-{
-	ra-&gt;readahead_index = ra-&gt;ra_index + ra_size;
-	ra-&gt;lookahead_index = ra-&gt;ra_index + ra_size - la_size;
-}
-
-unsigned long ra_submit(struct file_ra_state *ra,
-		       struct address_space *mapping, struct file *filp);
-
 struct file {
 	/*
 	 * fu_list becomes invalid after file_free is called and queued via
diff --git a/mm/readahead.c b/mm/readahead.c
index 3d262bb738a9..39bf45d43320 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -253,21 +253,16 @@ unsigned long max_sane_readahead(unsigned long nr)
 /*
  * Submit IO for the read-ahead request in file_ra_state.
  */
-unsigned long ra_submit(struct file_ra_state *ra,
+static unsigned long ra_submit(struct file_ra_state *ra,
 		       struct address_space *mapping, struct file *filp)
 {
-	unsigned long ra_size;
-	unsigned long la_size;
 	int actual;
 
-	ra_size = ra_readahead_size(ra);
-	la_size = ra_lookahead_size(ra);
 	actual = __do_page_cache_readahead(mapping, filp,
-					ra-&gt;ra_index, ra_size, la_size);
+					ra-&gt;start, ra-&gt;size, ra-&gt;async_size);
 
 	return actual;
 }
-EXPORT_SYMBOL_GPL(ra_submit);
 
 /*
  * Set the initial window size, round to next power of 2 and square
@@ -296,7 +291,7 @@ static unsigned long get_init_ra_size(unsigned long size, unsigned long max)
 static unsigned long get_next_ra_size(struct file_ra_state *ra,
 						unsigned long max)
 {
-	unsigned long cur = ra-&gt;readahead_index - ra-&gt;ra_index;
+	unsigned long cur = ra-&gt;size;
 	unsigned long newsize;
 
 	if (cur &lt; max / 16)
@@ -313,28 +308,21 @@ static unsigned long get_next_ra_size(struct file_ra_state *ra,
  * The fields in struct file_ra_state represent the most-recently-executed
  * readahead attempt:
  *
- *                    |-------- last readahead window --------&gt;|
- *       |-- application walking here --&gt;|
- * ======#============|==================#=====================|
- *       ^la_index    ^ra_index          ^lookahead_index      ^readahead_index
- *
- * [ra_index, readahead_index) represents the last readahead window.
- *
- * [la_index, lookahead_index] is where the application would be walking(in
- * the common case of cache-cold sequential reads): the last window was
- * established when the application was at la_index, and the next window will
- * be bring in when the application reaches lookahead_index.
+ *                        |&lt;----- async_size ---------|
+ *     |------------------- size --------------------&gt;|
+ *     |==================#===========================|
+ *     ^start             ^page marked with PG_readahead
  *
  * To overlap application thinking time and disk I/O time, we do
  * `readahead pipelining': Do not wait until the application consumed all
  * readahead pages and stalled on the missing page at readahead_index;
- * Instead, submit an asynchronous readahead I/O as early as the application
- * reads on the page at lookahead_index. Normally lookahead_index will be
- * equal to ra_index, for maximum pipelining.
+ * Instead, submit an asynchronous readahead I/O as soon as there are
+ * only async_size pages left in the readahead window. Normally async_size
+ * will be equal to size, for maximum pipelining.
  *
  * In interleaved sequential reads, concurrent streams on the same fd can
  * be invalidating each other's readahead state. So we flag the new readahead
- * page at lookahead_index with PG_readahead, and use it as readahead
+ * page at (start+size-async_size) with PG_readahead, and use it as readahead
  * indicator. The flag won't be set on already cached pages, to avoid the
  * readahead-for-nothing fuss, saving pointless page cache lookups.
  *
@@ -363,24 +351,21 @@ ondemand_readahead(struct address_space *mapping,
 		   unsigned long req_size)
 {
 	unsigned long max;	/* max readahead pages */
-	pgoff_t ra_index;	/* readahead index */
-	unsigned long ra_size;	/* readahead size */
-	unsigned long la_size;	/* lookahead size */
 	int sequential;
 
 	max = ra-&gt;ra_pages;
 	sequential = (offset - ra-&gt;prev_index &lt;= 1UL) || (req_size &gt; max);
 
 	/*
-	 * Lookahead/readahead hit, assume sequential access.
+	 * It's the expected callback offset, assume sequential access.
 	 * Ramp up sizes, and push forward the readahead window.
 	 */
-	if (offset &amp;&amp; (offset == ra-&gt;lookahead_index ||
-			offset == ra-&gt;readahead_index)) {
-		ra_index = ra-&gt;readahead_index;
-		ra_size = get_next_ra_size(ra, max);
-		la_size = ra_size;
-		goto fill_ra;
+	if (offset &amp;&amp; (offset == (ra-&gt;start + ra-&gt;size - ra-&gt;async_size) ||
+			offset == (ra-&gt;start + ra-&gt;size))) {
+		ra-&gt;start += ra-&gt;size;
+		ra-&gt;size = get_next_ra_size(ra, max);
+		ra-&gt;async_size = ra-&gt;size;
+		goto readit;
 	}
 
 	/*
@@ -399,24 +384,21 @@ ondemand_readahead(struct address_space *mapping,
 	 * 	- oversize random read
 	 * Start readahead for it.
 	 */
-	ra_index = offset;
-	ra_size = get_init_ra_size(req_size, max);
-	la_size = ra_size &gt; req_size ? ra_size - req_size : ra_size;
+	ra-&gt;start = offset;
+	ra-&gt;size = get_init_ra_size(req_size, max);
+	ra-&gt;async_size = ra-&gt;size &gt; req_size ? ra-&gt;size - req_size : ra-&gt;size;
 
 	/*
-	 * Hit on a lookahead page without valid readahead state.
+	 * Hit on a marked page without valid readahead state.
 	 * E.g. interleaved reads.
 	 * Not knowing its readahead pos/size, bet on the minimal possible one.
 	 */
 	if (hit_readahead_marker) {
-		ra_index++;
-		ra_size = min(4 * ra_size, max);
+		ra-&gt;start++;
+		ra-&gt;size = get_next_ra_size(ra, max);
 	}
 
-fill_ra:
-	ra_set_index(ra, offset, ra_index);
-	ra_set_size(ra, ra_size, la_size);
-
+readit:
 	return ra_submit(ra, mapping, filp);
 }
 </pre><hr><pre>commit fe3cba17c49471e99d3421e675fc8b3deaaf0b70
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Thu Jul 19 01:48:07 2007 -0700

    mm: share PG_readahead and PG_reclaim
    
    Share the same page flag bit for PG_readahead and PG_reclaim.
    
    One is used only on file reads, another is only for emergency writes.  One
    is used mostly for fresh/young pages, another is for old pages.
    
    Combinations of possible interactions are:
    
    a) clear PG_reclaim =&gt; implicit clear of PG_readahead
            it will delay an asynchronous readahead into a synchronous one
            it actually does _good_ for readahead:
                    the pages will be reclaimed soon, it's readahead thrashing!
                    in this case, synchronous readahead makes more sense.
    
    b) clear PG_readahead =&gt; implicit clear of PG_reclaim
            one(and only one) page will not be reclaimed in time
            it can be avoided by checking PageWriteback(page) in readahead first
    
    c) set PG_reclaim =&gt; implicit set of PG_readahead
            will confuse readahead and make it restart the size rampup process
            it's a trivial problem, and can mostly be avoided by checking
            PageWriteback(page) first in readahead
    
    d) set PG_readahead =&gt; implicit set of PG_reclaim
            PG_readahead will never be set on already cached pages.
            PG_reclaim will always be cleared on dirtying a page.
            so not a problem.
    
    In summary,
            a)   we get better behavior
            b,d) possible interactions can be avoided
            c)   racy condition exists that might affect readahead, but the chance
                 is _really_ low, and the hurt on readahead is trivial.
    
    Compound pages also use PG_reclaim, but for now they do not interact with
    reclaim/readahead code.
    
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h
index 709d92fd2877..a454176c3e30 100644
--- a/include/linux/page-flags.h
+++ b/include/linux/page-flags.h
@@ -83,7 +83,6 @@
 #define PG_private		11	/* If pagecache, has fs-private data */
 
 #define PG_writeback		12	/* Page is under writeback */
-#define PG_readahead		13	/* Reminder to do async read-ahead */
 #define PG_compound		14	/* Part of a compound page */
 #define PG_swapcache		15	/* Swap page: swp_entry_t in private */
 
@@ -91,6 +90,9 @@
 #define PG_reclaim		17	/* To be reclaimed asap */
 #define PG_buddy		19	/* Page is free, on buddy lists */
 
+/* PG_readahead is only used for file reads; PG_reclaim is only for writes */
+#define PG_readahead		PG_reclaim /* Reminder to do async read-ahead */
+
 /* PG_owner_priv_1 users should have descriptive aliases */
 #define PG_checked		PG_owner_priv_1 /* Used by some filesystems */
 #define PG_pinned		PG_owner_priv_1	/* Xen pinned pagetable */
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index e62482718012..51b3eb6ab445 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -920,6 +920,7 @@ int clear_page_dirty_for_io(struct page *page)
 
 	BUG_ON(!PageLocked(page));
 
+	ClearPageReclaim(page);
 	if (mapping &amp;&amp; mapping_cap_account_dirty(mapping)) {
 		/*
 		 * Yes, Virginia, this is indeed insane.
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 2165be9462c0..43cb3b3e1679 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -453,12 +453,6 @@ static inline int free_pages_check(struct page *page)
 			1 &lt;&lt; PG_reserved |
 			1 &lt;&lt; PG_buddy ))))
 		bad_page(page);
-	/*
-	 * PageReclaim == PageTail. It is only an error
-	 * for PageReclaim to be set if PageCompound is clear.
-	 */
-	if (unlikely(!PageCompound(page) &amp;&amp; PageReclaim(page)))
-		bad_page(page);
 	if (PageDirty(page))
 		__ClearPageDirty(page);
 	/*
@@ -602,7 +596,6 @@ static int prep_new_page(struct page *page, int order, gfp_t gfp_flags)
 			1 &lt;&lt; PG_locked	|
 			1 &lt;&lt; PG_active	|
 			1 &lt;&lt; PG_dirty	|
-			1 &lt;&lt; PG_reclaim	|
 			1 &lt;&lt; PG_slab    |
 			1 &lt;&lt; PG_swapcache |
 			1 &lt;&lt; PG_writeback |
diff --git a/mm/readahead.c b/mm/readahead.c
index 5b3c9b7d70fa..205a4a431516 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -448,6 +448,12 @@ page_cache_readahead_ondemand(struct address_space *mapping,
 		return 0;
 
 	if (page) {
+		/*
+		 * It can be PG_reclaim.
+		 */
+		if (PageWriteback(page))
+			return 0;
+
 		ClearPageReadahead(page);
 
 		/*</pre><hr><pre>commit d8983910a4045fa21022cfccf76ed13eb40fd7f5
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Thu Jul 19 01:48:06 2007 -0700

    readahead: pass real splice size
    
    Pass real splice size to page_cache_readahead_ondemand().
    
    The splice code works in chunks of 16 pages internally.  The readahead code
    should be told of the overall splice size, instead of the internal chunk size.
     Otherwize bad things may happen.  Imagine some 17-page random splice reads.
    The code before this patch will result in two readahead calls: readahead(16);
    readahead(1); That leads to one 16-page I/O and one 32-page I/O: one extra I/O
    and 31 readahead miss pages.
    
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Jens Axboe &lt;jens.axboe@oracle.com&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/fs/splice.c b/fs/splice.c
index 421b3b821152..6ddd0329f866 100644
--- a/fs/splice.c
+++ b/fs/splice.c
@@ -265,7 +265,7 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 			   unsigned int flags)
 {
 	struct address_space *mapping = in-&gt;f_mapping;
-	unsigned int loff, nr_pages;
+	unsigned int loff, nr_pages, req_pages;
 	struct page *pages[PIPE_BUFFERS];
 	struct partial_page partial[PIPE_BUFFERS];
 	struct page *page;
@@ -281,10 +281,8 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 
 	index = *ppos &gt;&gt; PAGE_CACHE_SHIFT;
 	loff = *ppos &amp; ~PAGE_CACHE_MASK;
-	nr_pages = (len + loff + PAGE_CACHE_SIZE - 1) &gt;&gt; PAGE_CACHE_SHIFT;
-
-	if (nr_pages &gt; PIPE_BUFFERS)
-		nr_pages = PIPE_BUFFERS;
+	req_pages = (len + loff + PAGE_CACHE_SIZE - 1) &gt;&gt; PAGE_CACHE_SHIFT;
+	nr_pages = min(req_pages, (unsigned)PIPE_BUFFERS);
 
 	/*
 	 * Lookup the (hopefully) full range of pages we need.
@@ -298,7 +296,7 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 	 */
 	if (spd.nr_pages &lt; nr_pages)
 		page_cache_readahead_ondemand(mapping, &amp;in-&gt;f_ra, in,
-				NULL, index, nr_pages - spd.nr_pages);
+				NULL, index, req_pages - spd.nr_pages);
 
 	error = 0;
 	while (spd.nr_pages &lt; nr_pages) {
@@ -355,7 +353,7 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 
 		if (PageReadahead(page))
 			page_cache_readahead_ondemand(mapping, &amp;in-&gt;f_ra, in,
-					page, index, nr_pages - page_nr);
+					page, index, req_pages - page_nr);
 
 		/*
 		 * If the page isn't uptodate, we may need to start io on it</pre><hr><pre>commit 431a4820bfcdf7ff530e745230bafb06c9bf2d6d
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Thu Jul 19 01:48:05 2007 -0700

    readahead: move synchronous readahead call out of splice loop
    
    Move synchronous page_cache_readahead_ondemand() call out of splice loop.
    
    This avoids one pointless page allocation/insertion in case of non-zero
    ra_pages, or many pointless readahead calls in case of zero ra_pages.
    
    Note that if a user sets ra_pages to less than PIPE_BUFFERS=16 pages, he will
    not get expected readahead behavior anyway.  The splice code works in batches
    of 16 pages, which can be taken as another form of synchronous readahead.
    
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Jens Axboe &lt;jens.axboe@oracle.com&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/fs/splice.c b/fs/splice.c
index 722449f7d5d6..421b3b821152 100644
--- a/fs/splice.c
+++ b/fs/splice.c
@@ -290,13 +290,17 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 	 * Lookup the (hopefully) full range of pages we need.
 	 */
 	spd.nr_pages = find_get_pages_contig(mapping, index, nr_pages, pages);
+	index += spd.nr_pages;
 
 	/*
 	 * If find_get_pages_contig() returned fewer pages than we needed,
-	 * allocate the rest and fill in the holes.
+	 * readahead/allocate the rest and fill in the holes.
 	 */
+	if (spd.nr_pages &lt; nr_pages)
+		page_cache_readahead_ondemand(mapping, &amp;in-&gt;f_ra, in,
+				NULL, index, nr_pages - spd.nr_pages);
+
 	error = 0;
-	index += spd.nr_pages;
 	while (spd.nr_pages &lt; nr_pages) {
 		/*
 		 * Page could be there, find_get_pages_contig() breaks on
@@ -304,9 +308,6 @@ __generic_file_splice_read(struct file *in, loff_t *ppos,
 		 */
 		page = find_get_page(mapping, index);
 		if (!page) {
-			page_cache_readahead_ondemand(mapping, &amp;in-&gt;f_ra, in,
-					NULL, index, nr_pages - spd.nr_pages);
-
 			/*
 			 * page didn't exist, allocate one.
 			 */</pre><hr><pre>commit c743d96b6d2ff55a94df7b5ac7c74987bb9c343b
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Thu Jul 19 01:48:04 2007 -0700

    readahead: remove the old algorithm
    
    Remove the old readahead algorithm.
    
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Steven Pratt &lt;slpratt@austin.ibm.com&gt;
    Cc: Ram Pai &lt;linuxram@us.ibm.com&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/include/linux/fs.h b/include/linux/fs.h
index 9a5f562abc77..29cb32d3a849 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -701,14 +701,6 @@ struct fown_struct {
  *  file_ra_state.la_index    .ra_index   .lookahead_index   .readahead_index
  */
 struct file_ra_state {
-	unsigned long start;		/* Current window */
-	unsigned long size;
-	unsigned long flags;		/* ra flags RA_FLAG_xxx*/
-	unsigned long cache_hit;	/* cache hit count*/
-	unsigned long prev_index;	/* Cache last read() position */
-	unsigned long ahead_start;	/* Ahead window */
-	unsigned long ahead_size;
-
 	pgoff_t la_index;               /* enqueue time */
 	pgoff_t ra_index;               /* begin offset */
 	pgoff_t lookahead_index;        /* time to do next readahead */
@@ -717,10 +709,9 @@ struct file_ra_state {
 	unsigned long ra_pages;		/* Maximum readahead window */
 	unsigned long mmap_hit;		/* Cache hit stat for mmap accesses */
 	unsigned long mmap_miss;	/* Cache miss stat for mmap accesses */
+	unsigned long prev_index;	/* Cache last read() position */
 	unsigned int prev_offset;	/* Offset where last read() ended in a page */
 };
-#define RA_FLAG_MISS 0x01	/* a cache miss occured against this file */
-#define RA_FLAG_INCACHE 0x02	/* file is already in cache */
 
 /*
  * Measuring read-ahead sizes.
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 619c0e80cf0c..3d0d7d285237 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1144,13 +1144,6 @@ unsigned long page_cache_readahead_ondemand(struct address_space *mapping,
 			  struct page *page,
 			  pgoff_t offset,
 			  unsigned long size);
-unsigned long page_cache_readahead(struct address_space *mapping,
-			  struct file_ra_state *ra,
-			  struct file *filp,
-			  pgoff_t offset,
-			  unsigned long size);
-void handle_ra_miss(struct address_space *mapping, 
-		    struct file_ra_state *ra, pgoff_t offset);
 unsigned long max_sane_readahead(unsigned long nr);
 
 /* Do stack extension */
diff --git a/mm/readahead.c b/mm/readahead.c
index c094e4f5a250..5b3c9b7d70fa 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -49,82 +49,6 @@ file_ra_state_init(struct file_ra_state *ra, struct address_space *mapping)
 }
 EXPORT_SYMBOL_GPL(file_ra_state_init);
 
-/*
- * Return max readahead size for this inode in number-of-pages.
- */
-static inline unsigned long get_max_readahead(struct file_ra_state *ra)
-{
-	return ra-&gt;ra_pages;
-}
-
-static inline unsigned long get_min_readahead(struct file_ra_state *ra)
-{
-	return MIN_RA_PAGES;
-}
-
-static inline void reset_ahead_window(struct file_ra_state *ra)
-{
-	/*
-	 * ... but preserve ahead_start + ahead_size value,
-	 * see 'recheck:' label in page_cache_readahead().
-	 * Note: We never use -&gt;ahead_size as rvalue without
-	 * checking -&gt;ahead_start != 0 first.
-	 */
-	ra-&gt;ahead_size += ra-&gt;ahead_start;
-	ra-&gt;ahead_start = 0;
-}
-
-static inline void ra_off(struct file_ra_state *ra)
-{
-	ra-&gt;start = 0;
-	ra-&gt;flags = 0;
-	ra-&gt;size = 0;
-	reset_ahead_window(ra);
-	return;
-}
-
-/*
- * Set the initial window size, round to next power of 2 and square
- * for small size, x 4 for medium, and x 2 for large
- * for 128k (32 page) max ra
- * 1-8 page = 32k initial, &gt; 8 page = 128k initial
- */
-static unsigned long get_init_ra_size(unsigned long size, unsigned long max)
-{
-	unsigned long newsize = roundup_pow_of_two(size);
-
-	if (newsize &lt;= max / 32)
-		newsize = newsize * 4;
-	else if (newsize &lt;= max / 4)
-		newsize = newsize * 2;
-	else
-		newsize = max;
-	return newsize;
-}
-
-/*
- * Set the new window size, this is called only when I/O is to be submitted,
- * not for each call to readahead.  If a cache miss occured, reduce next I/O
- * size, else increase depending on how close to max we are.
- */
-static inline unsigned long get_next_ra_size(struct file_ra_state *ra)
-{
-	unsigned long max = get_max_readahead(ra);
-	unsigned long min = get_min_readahead(ra);
-	unsigned long cur = ra-&gt;size;
-	unsigned long newsize;
-
-	if (ra-&gt;flags &amp; RA_FLAG_MISS) {
-		ra-&gt;flags &amp;= ~RA_FLAG_MISS;
-		newsize = max((cur - 2), min);
-	} else if (cur &lt; max / 16) {
-		newsize = 4 * cur;
-	} else {
-		newsize = 2 * cur;
-	}
-	return min(newsize, max);
-}
-
 #define list_to_page(head) (list_entry((head)-&gt;prev, struct page, lru))
 
 /**
@@ -200,66 +124,6 @@ static int read_pages(struct address_space *mapping, struct file *filp,
 	return ret;
 }
 
-/*
- * Readahead design.
- *
- * The fields in struct file_ra_state represent the most-recently-executed
- * readahead attempt:
- *
- * start:	Page index at which we started the readahead
- * size:	Number of pages in that read
- *              Together, these form the "current window".
- *              Together, start and size represent the `readahead window'.
- * prev_index:  The page which the readahead algorithm most-recently inspected.
- *              It is mainly used to detect sequential file reading.
- *              If page_cache_readahead sees that it is again being called for
- *              a page which it just looked at, it can return immediately without
- *              making any state changes.
- * offset:      Offset in the prev_index where the last read ended - used for
- *              detection of sequential file reading.
- * ahead_start,
- * ahead_size:  Together, these form the "ahead window".
- * ra_pages:	The externally controlled max readahead for this fd.
- *
- * When readahead is in the off state (size == 0), readahead is disabled.
- * In this state, prev_index is used to detect the resumption of sequential I/O.
- *
- * The readahead code manages two windows - the "current" and the "ahead"
- * windows.  The intent is that while the application is walking the pages
- * in the current window, I/O is underway on the ahead window.  When the
- * current window is fully traversed, it is replaced by the ahead window
- * and the ahead window is invalidated.  When this copying happens, the
- * new current window's pages are probably still locked.  So
- * we submit a new batch of I/O immediately, creating a new ahead window.
- *
- * So:
- *
- *   ----|----------------|----------------|-----
- *       ^start           ^start+size
- *                        ^ahead_start     ^ahead_start+ahead_size
- *
- *         ^ When this page is read, we submit I/O for the
- *           ahead window.
- *
- * A `readahead hit' occurs when a read request is made against a page which is
- * the next sequential page. Ahead window calculations are done only when it
- * is time to submit a new IO.  The code ramps up the size agressively at first,
- * but slow down as it approaches max_readhead.
- *
- * Any seek/ramdom IO will result in readahead being turned off.  It will resume
- * at the first sequential access.
- *
- * There is a special-case: if the first page which the application tries to
- * read happens to be the first page of the file, it is assumed that a linear
- * read is about to happen and the window is immediately set to the initial size
- * based on I/O request size and the max_readahead.
- *
- * This function is to be called for every read request, rather than when
- * it is time to perform readahead.  It is called only once for the entire I/O
- * regardless of size unless readahead is unable to start enough I/O to satisfy
- * the request (I/O request &gt; max_readahead).
- */
-
 /*
  * do_page_cache_readahead actually reads a chunk of disk.  It allocates all
  * the pages first, then submits them all for I/O. This avoids the very bad
@@ -295,7 +159,7 @@ __do_page_cache_readahead(struct address_space *mapping, struct file *filp,
 	read_lock_irq(&amp;mapping-&gt;tree_lock);
 	for (page_idx = 0; page_idx &lt; nr_to_read; page_idx++) {
 		pgoff_t page_offset = offset + page_idx;
-		
+
 		if (page_offset &gt; end_index)
 			break;
 
@@ -360,28 +224,6 @@ int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
 	return ret;
 }
 
-/*
- * Check how effective readahead is being.  If the amount of started IO is
- * less than expected then the file is partly or fully in pagecache and
- * readahead isn't helping.
- *
- */
-static inline int check_ra_success(struct file_ra_state *ra,
-			unsigned long nr_to_read, unsigned long actual)
-{
-	if (actual == 0) {
-		ra-&gt;cache_hit += nr_to_read;
-		if (ra-&gt;cache_hit &gt;= VM_MAX_CACHE_HIT) {
-			ra_off(ra);
-			ra-&gt;flags |= RA_FLAG_INCACHE;
-			return 0;
-		}
-	} else {
-		ra-&gt;cache_hit=0;
-	}
-	return 1;
-}
-
 /*
  * This version skips the IO if the queue is read-congested, and will tell the
  * block layer to abandon the readahead if request allocation would block.
@@ -398,191 +240,6 @@ int do_page_cache_readahead(struct address_space *mapping, struct file *filp,
 	return __do_page_cache_readahead(mapping, filp, offset, nr_to_read, 0);
 }
 
-/*
- * Read 'nr_to_read' pages starting at page 'offset'. If the flag 'block'
- * is set wait till the read completes.  Otherwise attempt to read without
- * blocking.
- * Returns 1 meaning 'success' if read is successful without switching off
- * readahead mode. Otherwise return failure.
- */
-static int
-blockable_page_cache_readahead(struct address_space *mapping, struct file *filp,
-			pgoff_t offset, unsigned long nr_to_read,
-			struct file_ra_state *ra, int block)
-{
-	int actual;
-
-	if (!block &amp;&amp; bdi_read_congested(mapping-&gt;backing_dev_info))
-		return 0;
-
-	actual = __do_page_cache_readahead(mapping, filp, offset, nr_to_read, 0);
-
-	return check_ra_success(ra, nr_to_read, actual);
-}
-
-static int make_ahead_window(struct address_space *mapping, struct file *filp,
-				struct file_ra_state *ra, int force)
-{
-	int block, ret;
-
-	ra-&gt;ahead_size = get_next_ra_size(ra);
-	ra-&gt;ahead_start = ra-&gt;start + ra-&gt;size;
-
-	block = force || (ra-&gt;prev_index &gt;= ra-&gt;ahead_start);
-	ret = blockable_page_cache_readahead(mapping, filp,
-			ra-&gt;ahead_start, ra-&gt;ahead_size, ra, block);
-
-	if (!ret &amp;&amp; !force) {
-		/* A read failure in blocking mode, implies pages are
-		 * all cached. So we can safely assume we have taken
-		 * care of all the pages requested in this call.
-		 * A read failure in non-blocking mode, implies we are
-		 * reading more pages than requested in this call.  So
-		 * we safely assume we have taken care of all the pages
-		 * requested in this call.
-		 *
-		 * Just reset the ahead window in case we failed due to
-		 * congestion.  The ahead window will any way be closed
-		 * in case we failed due to excessive page cache hits.
-		 */
-		reset_ahead_window(ra);
-	}
-
-	return ret;
-}
-
-/**
- * page_cache_readahead - generic adaptive readahead
- * @mapping: address_space which holds the pagecache and I/O vectors
- * @ra: file_ra_state which holds the readahead state
- * @filp: passed on to -&gt;readpage() and -&gt;readpages()
- * @offset: start offset into @mapping, in PAGE_CACHE_SIZE units
- * @req_size: hint: total size of the read which the caller is performing in
- *            PAGE_CACHE_SIZE units
- *
- * page_cache_readahead() is the main function.  It performs the adaptive
- * readahead window size management and submits the readahead I/O.
- *
- * Note that @filp is purely used for passing on to the -&gt;readpage[s]()
- * handler: it may refer to a different file from @mapping (so we may not use
- * @filp-&gt;f_mapping or @filp-&gt;f_path.dentry-&gt;d_inode here).
- * Also, @ra may not be equal to &amp;@filp-&gt;f_ra.
- *
- */
-unsigned long
-page_cache_readahead(struct address_space *mapping, struct file_ra_state *ra,
-		     struct file *filp, pgoff_t offset, unsigned long req_size)
-{
-	unsigned long max, newsize;
-	int sequential;
-
-	/*
-	 * We avoid doing extra work and bogusly perturbing the readahead
-	 * window expansion logic.
-	 */
-	if (offset == ra-&gt;prev_index &amp;&amp; --req_size)
-		++offset;
-
-	/* Note that prev_index == -1 if it is a first read */
-	sequential = (offset == ra-&gt;prev_index + 1);
-	ra-&gt;prev_index = offset;
-	ra-&gt;prev_offset = 0;
-
-	max = get_max_readahead(ra);
-	newsize = min(req_size, max);
-
-	/* No readahead or sub-page sized read or file already in cache */
-	if (newsize == 0 || (ra-&gt;flags &amp; RA_FLAG_INCACHE))
-		goto out;
-
-	ra-&gt;prev_index += newsize - 1;
-
-	/*
-	 * Special case - first read at start of file. We'll assume it's
-	 * a whole-file read and grow the window fast.  Or detect first
-	 * sequential access
-	 */
-	if (sequential &amp;&amp; ra-&gt;size == 0) {
-		ra-&gt;size = get_init_ra_size(newsize, max);
-		ra-&gt;start = offset;
-		if (!blockable_page_cache_readahead(mapping, filp, offset,
-							 ra-&gt;size, ra, 1))
-			goto out;
-
-		/*
-		 * If the request size is larger than our max readahead, we
-		 * at least want to be sure that we get 2 IOs in flight and
-		 * we know that we will definitly need the new I/O.
-		 * once we do this, subsequent calls should be able to overlap
-		 * IOs,* thus preventing stalls. so issue the ahead window
-		 * immediately.
-		 */
-		if (req_size &gt;= max)
-			make_ahead_window(mapping, filp, ra, 1);
-
-		goto out;
-	}
-
-	/*
-	 * Now handle the random case:
-	 * partial page reads and first access were handled above,
-	 * so this must be the next page otherwise it is random
-	 */
-	if (!sequential) {
-		ra_off(ra);
-		blockable_page_cache_readahead(mapping, filp, offset,
-				 newsize, ra, 1);
-		goto out;
-	}
-
-	/*
-	 * If we get here we are doing sequential IO and this was not the first
-	 * occurence (ie we have an existing window)
-	 */
-	if (ra-&gt;ahead_start == 0) {	 /* no ahead window yet */
-		if (!make_ahead_window(mapping, filp, ra, 0))
-			goto recheck;
-	}
-
-	/*
-	 * Already have an ahead window, check if we crossed into it.
-	 * If so, shift windows and issue a new ahead window.
-	 * Only return the #pages that are in the current window, so that
-	 * we get called back on the first page of the ahead window which
-	 * will allow us to submit more IO.
-	 */
-	if (ra-&gt;prev_index &gt;= ra-&gt;ahead_start) {
-		ra-&gt;start = ra-&gt;ahead_start;
-		ra-&gt;size = ra-&gt;ahead_size;
-		make_ahead_window(mapping, filp, ra, 0);
-recheck:
-		/* prev_index shouldn't overrun the ahead window */
-		ra-&gt;prev_index = min(ra-&gt;prev_index,
-			ra-&gt;ahead_start + ra-&gt;ahead_size - 1);
-	}
-
-out:
-	return ra-&gt;prev_index + 1;
-}
-EXPORT_SYMBOL_GPL(page_cache_readahead);
-
-/*
- * handle_ra_miss() is called when it is known that a page which should have
- * been present in the pagecache (we just did some readahead there) was in fact
- * not found.  This will happen if it was evicted by the VM (readahead
- * thrashing)
- *
- * Turn on the cache miss flag in the RA struct, this will cause the RA code
- * to reduce the RA size on the next read.
- */
-void handle_ra_miss(struct address_space *mapping,
-		struct file_ra_state *ra, pgoff_t offset)
-{
-	ra-&gt;flags |= RA_FLAG_MISS;
-	ra-&gt;flags &amp;= ~RA_FLAG_INCACHE;
-	ra-&gt;cache_hit = 0;
-}
-
 /*
  * Given a desired number of PAGE_CACHE_SIZE readahead pages, return a
  * sensible upper limit.
@@ -612,20 +269,40 @@ unsigned long ra_submit(struct file_ra_state *ra,
 }
 EXPORT_SYMBOL_GPL(ra_submit);
 
+/*
+ * Set the initial window size, round to next power of 2 and square
+ * for small size, x 4 for medium, and x 2 for large
+ * for 128k (32 page) max ra
+ * 1-8 page = 32k initial, &gt; 8 page = 128k initial
+ */
+static unsigned long get_init_ra_size(unsigned long size, unsigned long max)
+{
+	unsigned long newsize = roundup_pow_of_two(size);
+
+	if (newsize &lt;= max / 32)
+		newsize = newsize * 4;
+	else if (newsize &lt;= max / 4)
+		newsize = newsize * 2;
+	else
+		newsize = max;
+
+	return newsize;
+}
+
 /*
  *  Get the previous window size, ramp it up, and
  *  return it as the new window size.
  */
-static unsigned long get_next_ra_size2(struct file_ra_state *ra,
+static unsigned long get_next_ra_size(struct file_ra_state *ra,
 						unsigned long max)
 {
 	unsigned long cur = ra-&gt;readahead_index - ra-&gt;ra_index;
 	unsigned long newsize;
 
 	if (cur &lt; max / 16)
-		newsize = cur * 4;
+		newsize = 4 * cur;
 	else
-		newsize = cur * 2;
+		newsize = 2 * cur;
 
 	return min(newsize, max);
 }
@@ -701,7 +378,7 @@ ondemand_readahead(struct address_space *mapping,
 	if (offset &amp;&amp; (offset == ra-&gt;lookahead_index ||
 			offset == ra-&gt;readahead_index)) {
 		ra_index = ra-&gt;readahead_index;
-		ra_size = get_next_ra_size2(ra, max);
+		ra_size = get_next_ra_size(ra, max);
 		la_size = ra_size;
 		goto fill_ra;
 	}</pre><hr><pre>commit dc7868fcb9a73990e6f30371c1be465c436a7a7f
Author: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
Date:   Thu Jul 19 01:48:04 2007 -0700

    readahead: convert ext3/ext4 invocations
    
    Convert ext3/ext4 dir reads to use on-demand readahead.
    
    Readahead for dirs operates _not_ on file level, but on blockdev level.  This
    makes a difference when the data blocks are not continuous.  And the read
    routine is somehow opaque: there's no handy info about the status of current
    page.  So a simplified call scheme is employed: to call into readahead
    whenever the current page falls out of readahead windows.
    
    Signed-off-by: Fengguang Wu &lt;wfg@mail.ustc.edu.cn&gt;
    Cc: Steven Pratt &lt;slpratt@austin.ibm.com&gt;
    Cc: Ram Pai &lt;linuxram@us.ibm.com&gt;
    Cc: Rusty Russell &lt;rusty@rustcorp.com.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/fs/ext3/dir.c b/fs/ext3/dir.c
index 852869840f24..3c6d384a2c66 100644
--- a/fs/ext3/dir.c
+++ b/fs/ext3/dir.c
@@ -136,12 +136,14 @@ static int ext3_readdir(struct file * filp,
 		err = ext3_get_blocks_handle(NULL, inode, blk, 1,
 						&amp;map_bh, 0, 0);
 		if (err &gt; 0) {
-			page_cache_readahead(sb-&gt;s_bdev-&gt;bd_inode-&gt;i_mapping,
-				&amp;filp-&gt;f_ra,
-				filp,
-				map_bh.b_blocknr &gt;&gt;
-					(PAGE_CACHE_SHIFT - inode-&gt;i_blkbits),
-				1);
+			pgoff_t index = map_bh.b_blocknr &gt;&gt;
+					(PAGE_CACHE_SHIFT - inode-&gt;i_blkbits);
+			if (!ra_has_index(&amp;filp-&gt;f_ra, index))
+				page_cache_readahead_ondemand(
+					sb-&gt;s_bdev-&gt;bd_inode-&gt;i_mapping,
+					&amp;filp-&gt;f_ra, filp,
+					NULL, index, 1);
+			filp-&gt;f_ra.prev_index = index;
 			bh = ext3_bread(NULL, inode, blk, 0, &amp;err);
 		}
 
diff --git a/fs/ext4/dir.c b/fs/ext4/dir.c
index e8ad06e28318..0a872a09fed8 100644
--- a/fs/ext4/dir.c
+++ b/fs/ext4/dir.c
@@ -135,12 +135,14 @@ static int ext4_readdir(struct file * filp,
 		map_bh.b_state = 0;
 		err = ext4_get_blocks_wrap(NULL, inode, blk, 1, &amp;map_bh, 0, 0);
 		if (err &gt; 0) {
-			page_cache_readahead(sb-&gt;s_bdev-&gt;bd_inode-&gt;i_mapping,
-				&amp;filp-&gt;f_ra,
-				filp,
-				map_bh.b_blocknr &gt;&gt;
-					(PAGE_CACHE_SHIFT - inode-&gt;i_blkbits),
-				1);
+			pgoff_t index = map_bh.b_blocknr &gt;&gt;
+					(PAGE_CACHE_SHIFT - inode-&gt;i_blkbits);
+			if (!ra_has_index(&amp;filp-&gt;f_ra, index))
+				page_cache_readahead_ondemand(
+					sb-&gt;s_bdev-&gt;bd_inode-&gt;i_mapping,
+					&amp;filp-&gt;f_ra, filp,
+					NULL, index, 1);
+			filp-&gt;f_ra.prev_index = index;
 			bh = ext4_bread(NULL, inode, blk, 0, &amp;err);
 		}
 </pre>
    <div class="pagination">
        <a href='12_6.html'>&lt;&lt;Prev</a><a href='12.html'>1</a><a href='12_2.html'>2</a><a href='12_3.html'>3</a><a href='12_4.html'>4</a><a href='12_5.html'>5</a><a href='12_6.html'>6</a><span>[7]</span><a href='12_8.html'>8</a><a href='12_9.html'>9</a><a href='12_8.html'>Next&gt;&gt;</a>
    <div>
</body>

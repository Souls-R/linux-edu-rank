<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_51.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><span>[52]</span><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_53.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 26a4c0c6ccecf6814cf44f951c97222bd795bc1a
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Apr 3 12:45:17 2013 -0400

    ext4: refactor punch hole code
    
    Move common code in ext4_ind_punch_hole() and ext4_ext_punch_hole()
    into ext4_punch_hole().  This saves over 150 lines of code.
    
    This also fixes a potential bug when the punch_hole() code is racing
    against indirect-to-extents or extents-to-indirect migation.  We are
    currently using i_mutex to protect against changes to the inode flag;
    specifically, the append-only, immutable, and extents inode flags.  So
    we need to take i_mutex before deciding whether to use the
    extents-specific or indirect-specific punch_hole code.
    
    Also, there was a missing call to ext4_inode_block_unlocked_dio() in
    the indirect punch codepath.  This was added in commit 02d262dffcf4c
    to block DIO readers racing against the punch operation in the
    codepath for extent-mapped inodes, but it was missing for
    indirect-block mapped inodes.  One of the advantages of refactoring
    the code is that it makes such oversights much less likely.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index f91e11bd9753..0649253804c4 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -2110,7 +2110,8 @@ extern ssize_t ext4_ind_direct_IO(int rw, struct kiocb *iocb,
 extern int ext4_ind_calc_metadata_amount(struct inode *inode, sector_t lblock);
 extern int ext4_ind_trans_blocks(struct inode *inode, int nrblocks, int chunk);
 extern void ext4_ind_truncate(struct inode *inode);
-extern int ext4_ind_punch_hole(struct file *file, loff_t offset, loff_t length);
+extern int ext4_free_hole_blocks(handle_t *handle, struct inode *inode,
+				 ext4_lblk_t first, ext4_lblk_t stop);
 
 /* ioctl.c */
 extern long ext4_ioctl(struct file *, unsigned int, unsigned long);
@@ -2575,8 +2576,8 @@ extern int ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks,
 extern int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 			       struct ext4_map_blocks *map, int flags);
 extern void ext4_ext_truncate(struct inode *);
-extern int ext4_ext_punch_hole(struct file *file, loff_t offset,
-				loff_t length);
+extern int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,
+				 ext4_lblk_t end);
 extern void ext4_ext_init(struct super_block *);
 extern void ext4_ext_release(struct super_block *);
 extern long ext4_fallocate(struct file *file, int mode, loff_t offset,
diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index 9c6d06dcef8b..d58365e40df7 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -2599,8 +2599,8 @@ ext4_ext_more_to_rm(struct ext4_ext_path *path)
 	return 1;
 }
 
-static int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,
-				 ext4_lblk_t end)
+int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start,
+			  ext4_lblk_t end)
 {
 	struct super_block *sb = inode-&gt;i_sb;
 	int depth = ext_depth(inode);
@@ -4623,187 +4623,6 @@ static int ext4_xattr_fiemap(struct inode *inode,
 	return (error &lt; 0 ? error : 0);
 }
 
-/*
- * ext4_ext_punch_hole
- *
- * Punches a hole of "length" bytes in a file starting
- * at byte "offset"
- *
- * @inode:  The inode of the file to punch a hole in
- * @offset: The starting byte offset of the hole
- * @length: The length of the hole
- *
- * Returns the number of blocks removed or negative on err
- */
-int ext4_ext_punch_hole(struct file *file, loff_t offset, loff_t length)
-{
-	struct inode *inode = file_inode(file);
-	struct super_block *sb = inode-&gt;i_sb;
-	ext4_lblk_t first_block, stop_block;
-	struct address_space *mapping = inode-&gt;i_mapping;
-	handle_t *handle;
-	loff_t first_page, last_page, page_len;
-	loff_t first_page_offset, last_page_offset;
-	int credits, err = 0;
-
-	/*
-	 * Write out all dirty pages to avoid race conditions
-	 * Then release them.
-	 */
-	if (mapping-&gt;nrpages &amp;&amp; mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
-		err = filemap_write_and_wait_range(mapping,
-			offset, offset + length - 1);
-
-		if (err)
-			return err;
-	}
-
-	mutex_lock(&amp;inode-&gt;i_mutex);
-	/* It's not possible punch hole on append only file */
-	if (IS_APPEND(inode) || IS_IMMUTABLE(inode)) {
-		err = -EPERM;
-		goto out_mutex;
-	}
-	if (IS_SWAPFILE(inode)) {
-		err = -ETXTBSY;
-		goto out_mutex;
-	}
-
-	/* No need to punch hole beyond i_size */
-	if (offset &gt;= inode-&gt;i_size)
-		goto out_mutex;
-
-	/*
-	 * If the hole extends beyond i_size, set the hole
-	 * to end after the page that contains i_size
-	 */
-	if (offset + length &gt; inode-&gt;i_size) {
-		length = inode-&gt;i_size +
-		   PAGE_CACHE_SIZE - (inode-&gt;i_size &amp; (PAGE_CACHE_SIZE - 1)) -
-		   offset;
-	}
-
-	first_page = (offset + PAGE_CACHE_SIZE - 1) &gt;&gt; PAGE_CACHE_SHIFT;
-	last_page = (offset + length) &gt;&gt; PAGE_CACHE_SHIFT;
-
-	first_page_offset = first_page &lt;&lt; PAGE_CACHE_SHIFT;
-	last_page_offset = last_page &lt;&lt; PAGE_CACHE_SHIFT;
-
-	/* Now release the pages */
-	if (last_page_offset &gt; first_page_offset) {
-		truncate_pagecache_range(inode, first_page_offset,
-					 last_page_offset - 1);
-	}
-
-	/* Wait all existing dio workers, newcomers will block on i_mutex */
-	ext4_inode_block_unlocked_dio(inode);
-	err = ext4_flush_unwritten_io(inode);
-	if (err)
-		goto out_dio;
-	inode_dio_wait(inode);
-
-	credits = ext4_writepage_trans_blocks(inode);
-	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
-	if (IS_ERR(handle)) {
-		err = PTR_ERR(handle);
-		goto out_dio;
-	}
-
-
-	/*
-	 * Now we need to zero out the non-page-aligned data in the
-	 * pages at the start and tail of the hole, and unmap the buffer
-	 * heads for the block aligned regions of the page that were
-	 * completely zeroed.
-	 */
-	if (first_page &gt; last_page) {
-		/*
-		 * If the file space being truncated is contained within a page
-		 * just zero out and unmap the middle of that page
-		 */
-		err = ext4_discard_partial_page_buffers(handle,
-			mapping, offset, length, 0);
-
-		if (err)
-			goto out;
-	} else {
-		/*
-		 * zero out and unmap the partial page that contains
-		 * the start of the hole
-		 */
-		page_len  = first_page_offset - offset;
-		if (page_len &gt; 0) {
-			err = ext4_discard_partial_page_buffers(handle, mapping,
-						   offset, page_len, 0);
-			if (err)
-				goto out;
-		}
-
-		/*
-		 * zero out and unmap the partial page that contains
-		 * the end of the hole
-		 */
-		page_len = offset + length - last_page_offset;
-		if (page_len &gt; 0) {
-			err = ext4_discard_partial_page_buffers(handle, mapping,
-					last_page_offset, page_len, 0);
-			if (err)
-				goto out;
-		}
-	}
-
-	/*
-	 * If i_size is contained in the last page, we need to
-	 * unmap and zero the partial page after i_size
-	 */
-	if (inode-&gt;i_size &gt;&gt; PAGE_CACHE_SHIFT == last_page &amp;&amp;
-	   inode-&gt;i_size % PAGE_CACHE_SIZE != 0) {
-
-		page_len = PAGE_CACHE_SIZE -
-			(inode-&gt;i_size &amp; (PAGE_CACHE_SIZE - 1));
-
-		if (page_len &gt; 0) {
-			err = ext4_discard_partial_page_buffers(handle,
-			  mapping, inode-&gt;i_size, page_len, 0);
-
-			if (err)
-				goto out;
-		}
-	}
-
-	first_block = (offset + sb-&gt;s_blocksize - 1) &gt;&gt;
-		EXT4_BLOCK_SIZE_BITS(sb);
-	stop_block = (offset + length) &gt;&gt; EXT4_BLOCK_SIZE_BITS(sb);
-
-	/* If there are no blocks to remove, return now */
-	if (first_block &gt;= stop_block)
-		goto out;
-
-	down_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
-	ext4_discard_preallocations(inode);
-
-	err = ext4_es_remove_extent(inode, first_block,
-				    stop_block - first_block);
-	err = ext4_ext_remove_space(inode, first_block, stop_block - 1);
-
-	ext4_discard_preallocations(inode);
-
-	if (IS_SYNC(inode))
-		ext4_handle_sync(handle);
-
-	up_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
-
-out:
-	inode-&gt;i_mtime = inode-&gt;i_ctime = ext4_current_time(inode);
-	ext4_mark_inode_dirty(handle, inode);
-	ext4_journal_stop(handle);
-out_dio:
-	ext4_inode_resume_unlocked_dio(inode);
-out_mutex:
-	mutex_unlock(&amp;inode-&gt;i_mutex);
-	return err;
-}
-
 int ext4_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,
 		__u64 start, __u64 len)
 {
diff --git a/fs/ext4/indirect.c b/fs/ext4/indirect.c
index c0f9e4699f0b..d8846779f4ea 100644
--- a/fs/ext4/indirect.c
+++ b/fs/ext4/indirect.c
@@ -1434,8 +1434,8 @@ static int free_hole_blocks(handle_t *handle, struct inode *inode,
 	return ret;
 }
 
-static int ext4_free_hole_blocks(handle_t *handle, struct inode *inode,
-				 ext4_lblk_t first, ext4_lblk_t stop)
+int ext4_free_hole_blocks(handle_t *handle, struct inode *inode,
+			  ext4_lblk_t first, ext4_lblk_t stop)
 {
 	int addr_per_block = EXT4_ADDR_PER_BLOCK(inode-&gt;i_sb);
 	int level, ret = 0;
@@ -1469,157 +1469,3 @@ static int ext4_free_hole_blocks(handle_t *handle, struct inode *inode,
 	return ret;
 }
 
-int ext4_ind_punch_hole(struct file *file, loff_t offset, loff_t length)
-{
-	struct inode *inode = file_inode(file);
-	struct super_block *sb = inode-&gt;i_sb;
-	ext4_lblk_t first_block, stop_block;
-	struct address_space *mapping = inode-&gt;i_mapping;
-	handle_t *handle = NULL;
-	loff_t first_page, last_page, page_len;
-	loff_t first_page_offset, last_page_offset;
-	int err = 0;
-
-	/*
-	 * Write out all dirty pages to avoid race conditions
-	 * Then release them.
-	 */
-	if (mapping-&gt;nrpages &amp;&amp; mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
-		err = filemap_write_and_wait_range(mapping,
-			offset, offset + length - 1);
-		if (err)
-			return err;
-	}
-
-	mutex_lock(&amp;inode-&gt;i_mutex);
-	/* It's not possible punch hole on append only file */
-	if (IS_APPEND(inode) || IS_IMMUTABLE(inode)) {
-		err = -EPERM;
-		goto out_mutex;
-	}
-	if (IS_SWAPFILE(inode)) {
-		err = -ETXTBSY;
-		goto out_mutex;
-	}
-
-	/* No need to punch hole beyond i_size */
-	if (offset &gt;= inode-&gt;i_size)
-		goto out_mutex;
-
-	/*
-	 * If the hole extents beyond i_size, set the hole
-	 * to end after the page that contains i_size
-	 */
-	if (offset + length &gt; inode-&gt;i_size) {
-		length = inode-&gt;i_size +
-		    PAGE_CACHE_SIZE - (inode-&gt;i_size &amp; (PAGE_CACHE_SIZE - 1)) -
-		    offset;
-	}
-
-	first_page = (offset + PAGE_CACHE_SIZE - 1) &gt;&gt; PAGE_CACHE_SHIFT;
-	last_page = (offset + length) &gt;&gt; PAGE_CACHE_SHIFT;
-
-	first_page_offset = first_page &lt;&lt; PAGE_CACHE_SHIFT;
-	last_page_offset = last_page &lt;&lt; PAGE_CACHE_SHIFT;
-
-	/* Now release the pages */
-	if (last_page_offset &gt; first_page_offset) {
-		truncate_pagecache_range(inode, first_page_offset,
-					 last_page_offset - 1);
-	}
-
-	/* Wait all existing dio works, newcomers will block on i_mutex */
-	inode_dio_wait(inode);
-
-	handle = start_transaction(inode);
-	if (IS_ERR(handle))
-		goto out_mutex;
-
-	/*
-	 * Now we need to zero out the non-page-aligned data in the
-	 * pages at the start and tail of the hole, and unmap the buffer
-	 * heads for the block aligned regions of the page that were
-	 * completely zerod.
-	 */
-	if (first_page &gt; last_page) {
-		/*
-		 * If the file space being truncated is contained within a page
-		 * just zero out and unmap the middle of that page
-		 */
-		err = ext4_discard_partial_page_buffers(handle,
-			mapping, offset, length, 0);
-		if (err)
-			goto out;
-	} else {
-		/*
-		 * Zero out and unmap the paritial page that contains
-		 * the start of the hole
-		 */
-		page_len = first_page_offset - offset;
-		if (page_len &gt; 0) {
-			err = ext4_discard_partial_page_buffers(handle, mapping,
-							offset, page_len, 0);
-			if (err)
-				goto out;
-		}
-
-		/*
-		 * Zero out and unmap the partial page that contains
-		 * the end of the hole
-		 */
-		page_len = offset + length - last_page_offset;
-		if (page_len &gt; 0) {
-			err = ext4_discard_partial_page_buffers(handle, mapping,
-						last_page_offset, page_len, 0);
-			if (err)
-				goto out;
-		}
-	}
-
-	/*
-	 * If i_size contained in the last page, we need to
-	 * unmap and zero the paritial page after i_size
-	 */
-	if (inode-&gt;i_size &gt;&gt; PAGE_CACHE_SHIFT == last_page &amp;&amp;
-	    inode-&gt;i_size % PAGE_CACHE_SIZE != 0) {
-		page_len = PAGE_CACHE_SIZE -
-			(inode-&gt;i_size &amp; (PAGE_CACHE_SIZE - 1));
-		if (page_len &gt; 0) {
-			err = ext4_discard_partial_page_buffers(handle,
-				mapping, inode-&gt;i_size, page_len, 0);
-			if (err)
-				goto out;
-		}
-	}
-
-	first_block = (offset + sb-&gt;s_blocksize - 1) &gt;&gt;
-		EXT4_BLOCK_SIZE_BITS(sb);
-	stop_block = (offset + length) &gt;&gt; EXT4_BLOCK_SIZE_BITS(sb);
-
-	if (first_block &gt;= stop_block)
-		goto out;
-
-	down_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
-	ext4_discard_preallocations(inode);
-
-	err = ext4_es_remove_extent(inode, first_block,
-				    stop_block - first_block);
-	err = ext4_free_hole_blocks(handle, inode, first_block, stop_block);
-
-	ext4_discard_preallocations(inode);
-
-	if (IS_SYNC(inode))
-		ext4_handle_sync(handle);
-
-	up_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
-
-out:
-	inode-&gt;i_mtime = inode-&gt;i_ctime = ext4_current_time(inode);
-	ext4_mark_inode_dirty(handle, inode);
-	ext4_journal_stop(handle);
-
-out_mutex:
-	mutex_unlock(&amp;inode-&gt;i_mutex);
-
-	return err;
-}
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index a4ffb470fbf3..9bda50aa34e2 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -3566,20 +3566,190 @@ int ext4_can_truncate(struct inode *inode)
 int ext4_punch_hole(struct file *file, loff_t offset, loff_t length)
 {
 	struct inode *inode = file_inode(file);
+	struct super_block *sb = inode-&gt;i_sb;
+	ext4_lblk_t first_block, stop_block;
+	struct address_space *mapping = inode-&gt;i_mapping;
+	loff_t first_page, last_page, page_len;
+	loff_t first_page_offset, last_page_offset;
+	handle_t *handle;
+	unsigned int credits;
+	int ret = 0;
+
 	if (!S_ISREG(inode-&gt;i_mode))
 		return -EOPNOTSUPP;
 
-	if (!ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
-		return ext4_ind_punch_hole(file, offset, length);
-
-	if (EXT4_SB(inode-&gt;i_sb)-&gt;s_cluster_ratio &gt; 1) {
+	if (EXT4_SB(sb)-&gt;s_cluster_ratio &gt; 1) {
 		/* TODO: Add support for bigalloc file systems */
 		return -EOPNOTSUPP;
 	}
 
 	trace_ext4_punch_hole(inode, offset, length);
 
-	return ext4_ext_punch_hole(file, offset, length);
+	/*
+	 * Write out all dirty pages to avoid race conditions
+	 * Then release them.
+	 */
+	if (mapping-&gt;nrpages &amp;&amp; mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
+		ret = filemap_write_and_wait_range(mapping, offset,
+						   offset + length - 1);
+		if (ret)
+			return ret;
+	}
+
+	mutex_lock(&amp;inode-&gt;i_mutex);
+	/* It's not possible punch hole on append only file */
+	if (IS_APPEND(inode) || IS_IMMUTABLE(inode)) {
+		ret = -EPERM;
+		goto out_mutex;
+	}
+	if (IS_SWAPFILE(inode)) {
+		ret = -ETXTBSY;
+		goto out_mutex;
+	}
+
+	/* No need to punch hole beyond i_size */
+	if (offset &gt;= inode-&gt;i_size)
+		goto out_mutex;
+
+	/*
+	 * If the hole extends beyond i_size, set the hole
+	 * to end after the page that contains i_size
+	 */
+	if (offset + length &gt; inode-&gt;i_size) {
+		length = inode-&gt;i_size +
+		   PAGE_CACHE_SIZE - (inode-&gt;i_size &amp; (PAGE_CACHE_SIZE - 1)) -
+		   offset;
+	}
+
+	first_page = (offset + PAGE_CACHE_SIZE - 1) &gt;&gt; PAGE_CACHE_SHIFT;
+	last_page = (offset + length) &gt;&gt; PAGE_CACHE_SHIFT;
+
+	first_page_offset = first_page &lt;&lt; PAGE_CACHE_SHIFT;
+	last_page_offset = last_page &lt;&lt; PAGE_CACHE_SHIFT;
+
+	/* Now release the pages */
+	if (last_page_offset &gt; first_page_offset) {
+		truncate_pagecache_range(inode, first_page_offset,
+					 last_page_offset - 1);
+	}
+
+	/* Wait all existing dio workers, newcomers will block on i_mutex */
+	ext4_inode_block_unlocked_dio(inode);
+	ret = ext4_flush_unwritten_io(inode);
+	if (ret)
+		goto out_dio;
+	inode_dio_wait(inode);
+
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		credits = ext4_writepage_trans_blocks(inode);
+	else
+		credits = ext4_blocks_for_truncate(inode);
+	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
+	if (IS_ERR(handle)) {
+		ret = PTR_ERR(handle);
+		ext4_std_error(sb, ret);
+		goto out_dio;
+	}
+
+	/*
+	 * Now we need to zero out the non-page-aligned data in the
+	 * pages at the start and tail of the hole, and unmap the
+	 * buffer heads for the block aligned regions of the page that
+	 * were completely zeroed.
+	 */
+	if (first_page &gt; last_page) {
+		/*
+		 * If the file space being truncated is contained
+		 * within a page just zero out and unmap the middle of
+		 * that page
+		 */
+		ret = ext4_discard_partial_page_buffers(handle,
+			mapping, offset, length, 0);
+
+		if (ret)
+			goto out_stop;
+	} else {
+		/*
+		 * zero out and unmap the partial page that contains
+		 * the start of the hole
+		 */
+		page_len = first_page_offset - offset;
+		if (page_len &gt; 0) {
+			ret = ext4_discard_partial_page_buffers(handle, mapping,
+						offset, page_len, 0);
+			if (ret)
+				goto out_stop;
+		}
+
+		/*
+		 * zero out and unmap the partial page that contains
+		 * the end of the hole
+		 */
+		page_len = offset + length - last_page_offset;
+		if (page_len &gt; 0) {
+			ret = ext4_discard_partial_page_buffers(handle, mapping,
+					last_page_offset, page_len, 0);
+			if (ret)
+				goto out_stop;
+		}
+	}
+
+	/*
+	 * If i_size is contained in the last page, we need to
+	 * unmap and zero the partial page after i_size
+	 */
+	if (inode-&gt;i_size &gt;&gt; PAGE_CACHE_SHIFT == last_page &amp;&amp;
+	   inode-&gt;i_size % PAGE_CACHE_SIZE != 0) {
+		page_len = PAGE_CACHE_SIZE -
+			(inode-&gt;i_size &amp; (PAGE_CACHE_SIZE - 1));
+
+		if (page_len &gt; 0) {
+			ret = ext4_discard_partial_page_buffers(handle,
+					mapping, inode-&gt;i_size, page_len, 0);
+
+			if (ret)
+				goto out_stop;
+		}
+	}
+
+	first_block = (offset + sb-&gt;s_blocksize - 1) &gt;&gt;
+		EXT4_BLOCK_SIZE_BITS(sb);
+	stop_block = (offset + length) &gt;&gt; EXT4_BLOCK_SIZE_BITS(sb);
+
+	/* If there are no blocks to remove, return now */
+	if (first_block &gt;= stop_block)
+		goto out_stop;
+
+	down_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
+	ext4_discard_preallocations(inode);
+
+	ret = ext4_es_remove_extent(inode, first_block,
+				    stop_block - first_block);
+	if (ret) {
+		up_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
+		goto out_stop;
+	}
+
+	if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
+		ret = ext4_ext_remove_space(inode, first_block,
+					    stop_block - 1);
+	else
+		ret = ext4_free_hole_blocks(handle, inode, first_block,
+					    stop_block);
+
+	ext4_discard_preallocations(inode);
+	if (IS_SYNC(inode))
+		ext4_handle_sync(handle);
+	up_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
+	inode-&gt;i_mtime = inode-&gt;i_ctime = ext4_current_time(inode);
+	ext4_mark_inode_dirty(handle, inode);
+out_stop:
+	ext4_journal_stop(handle);
+out_dio:
+	ext4_inode_resume_unlocked_dio(inode);
+out_mutex:
+	mutex_unlock(&amp;inode-&gt;i_mutex);
+	return ret;
 }
 
 /*</pre><hr><pre>commit 781f143ea0fd7981ebe2e8cd96114997c8cf6c07
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Apr 3 12:43:17 2013 -0400

    ext4: fold ext4_alloc_blocks() in ext4_alloc_branch()
    
    The older code was far more complicated than it needed to be because
    of how we spliced in the ext4's new multiblock allocator into ext3's
    indirect block code.  By folding ext4_alloc_blocks() into
    ext4_alloc_branch(), we make the code far more understable, shave off
    over 130 lines of code and half a kilobyte of compiled object code.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/indirect.c b/fs/ext4/indirect.c
index a04183127ef0..c0f9e4699f0b 100644
--- a/fs/ext4/indirect.c
+++ b/fs/ext4/indirect.c
@@ -291,131 +291,6 @@ static int ext4_blks_to_allocate(Indirect *branch, int k, unsigned int blks,
 	return count;
 }
 
-/**
- *	ext4_alloc_blocks: multiple allocate blocks needed for a branch
- *	@handle: handle for this transaction
- *	@inode: inode which needs allocated blocks
- *	@iblock: the logical block to start allocated at
- *	@goal: preferred physical block of allocation
- *	@indirect_blks: the number of blocks need to allocate for indirect
- *			blocks
- *	@blks: number of desired blocks
- *	@new_blocks: on return it will store the new block numbers for
- *	the indirect blocks(if needed) and the first direct block,
- *	@err: on return it will store the error code
- *
- *	This function will return the number of blocks allocated as
- *	requested by the passed-in parameters.
- */
-static int ext4_alloc_blocks(handle_t *handle, struct inode *inode,
-			     ext4_lblk_t iblock, ext4_fsblk_t goal,
-			     int indirect_blks, int blks,
-			     ext4_fsblk_t new_blocks[4], int *err)
-{
-	struct ext4_allocation_request ar;
-	int target, i;
-	unsigned long count = 0, blk_allocated = 0;
-	int index = 0;
-	ext4_fsblk_t current_block = 0;
-	int ret = 0;
-
-	/*
-	 * Here we try to allocate the requested multiple blocks at once,
-	 * on a best-effort basis.
-	 * To build a branch, we should allocate blocks for
-	 * the indirect blocks(if not allocated yet), and at least
-	 * the first direct block of this branch.  That's the
-	 * minimum number of blocks need to allocate(required)
-	 */
-	/* first we try to allocate the indirect blocks */
-	target = indirect_blks;
-	while (target &gt; 0) {
-		count = target;
-		/* allocating blocks for indirect blocks and direct blocks */
-		current_block = ext4_new_meta_blocks(handle, inode, goal,
-						     0, &amp;count, err);
-		if (*err)
-			goto failed_out;
-
-		if (unlikely(current_block + count &gt; EXT4_MAX_BLOCK_FILE_PHYS)) {
-			EXT4_ERROR_INODE(inode,
-					 "current_block %llu + count %lu &gt; %d!",
-					 current_block, count,
-					 EXT4_MAX_BLOCK_FILE_PHYS);
-			*err = -EIO;
-			goto failed_out;
-		}
-
-		target -= count;
-		/* allocate blocks for indirect blocks */
-		while (index &lt; indirect_blks &amp;&amp; count) {
-			new_blocks[index++] = current_block++;
-			count--;
-		}
-		if (count &gt; 0) {
-			/*
-			 * save the new block number
-			 * for the first direct block
-			 */
-			new_blocks[index] = current_block;
-			WARN(1, KERN_INFO "%s returned more blocks than "
-						"requested\n", __func__);
-			break;
-		}
-	}
-
-	target = blks - count ;
-	blk_allocated = count;
-	if (!target)
-		goto allocated;
-	/* Now allocate data blocks */
-	memset(&amp;ar, 0, sizeof(ar));
-	ar.inode = inode;
-	ar.goal = goal;
-	ar.len = target;
-	ar.logical = iblock;
-	if (S_ISREG(inode-&gt;i_mode))
-		/* enable in-core preallocation only for regular files */
-		ar.flags = EXT4_MB_HINT_DATA;
-
-	current_block = ext4_mb_new_blocks(handle, &amp;ar, err);
-	if (unlikely(current_block + ar.len &gt; EXT4_MAX_BLOCK_FILE_PHYS)) {
-		EXT4_ERROR_INODE(inode,
-				 "current_block %llu + ar.len %d &gt; %d!",
-				 current_block, ar.len,
-				 EXT4_MAX_BLOCK_FILE_PHYS);
-		*err = -EIO;
-		goto failed_out;
-	}
-
-	if (*err &amp;&amp; (target == blks)) {
-		/*
-		 * if the allocation failed and we didn't allocate
-		 * any blocks before
-		 */
-		goto failed_out;
-	}
-	if (!*err) {
-		if (target == blks) {
-			/*
-			 * save the new block number
-			 * for the first direct block
-			 */
-			new_blocks[index] = current_block;
-		}
-		blk_allocated += ar.len;
-	}
-allocated:
-	/* total number of blocks allocated for direct blocks */
-	ret = blk_allocated;
-	*err = 0;
-	return ret;
-failed_out:
-	for (i = 0; i &lt; index; i++)
-		ext4_free_blocks(handle, inode, NULL, new_blocks[i], 1, 0);
-	return ret;
-}
-
 /**
  *	ext4_alloc_branch - allocate and set up a chain of blocks.
  *	@handle: handle for this transaction
@@ -448,60 +323,59 @@ static int ext4_alloc_branch(handle_t *handle, struct inode *inode,
 			     int *blks, ext4_fsblk_t goal,
 			     ext4_lblk_t *offsets, Indirect *branch)
 {
-	int blocksize = inode-&gt;i_sb-&gt;s_blocksize;
-	int i, n = 0;
-	int err = 0;
-	struct buffer_head *bh;
-	int num;
-	ext4_fsblk_t new_blocks[4];
-	ext4_fsblk_t current_block;
+	struct ext4_allocation_request	ar;
+	struct buffer_head *		bh;
+	ext4_fsblk_t			b, new_blocks[4];
+	__le32				*p;
+	int				i, j, err, len = 1;
 
-	num = ext4_alloc_blocks(handle, inode, iblock, goal, indirect_blks,
-				*blks, new_blocks, &amp;err);
-	if (err)
-		return err;
-
-	branch[0].key = cpu_to_le32(new_blocks[0]);
 	/*
-	 * metadata blocks and data blocks are allocated.
+	 * Set up for the direct block allocation
 	 */
-	for (n = 1; n &lt;= indirect_blks;  n++) {
-		/*
-		 * Get buffer_head for parent block, zero it out
-		 * and set the pointer to new one, then send
-		 * parent to disk.
-		 */
-		bh = sb_getblk(inode-&gt;i_sb, new_blocks[n-1]);
+	memset(&amp;ar, 0, sizeof(ar));
+	ar.inode = inode;
+	ar.len = *blks;
+	ar.logical = iblock;
+	if (S_ISREG(inode-&gt;i_mode))
+		ar.flags = EXT4_MB_HINT_DATA;
+
+	for (i = 0; i &lt;= indirect_blks; i++) {
+		if (i == indirect_blks) {
+			ar.goal = goal;
+			new_blocks[i] = ext4_mb_new_blocks(handle, &amp;ar, &amp;err);
+		} else
+			goal = new_blocks[i] = ext4_new_meta_blocks(handle, inode,
+							goal, 0, NULL, &amp;err);
+		if (err) {
+			i--;
+			goto failed;
+		}
+		branch[i].key = cpu_to_le32(new_blocks[i]);
+		if (i == 0)
+			continue;
+
+		bh = branch[i].bh = sb_getblk(inode-&gt;i_sb, new_blocks[i-1]);
 		if (unlikely(!bh)) {
 			err = -ENOMEM;
 			goto failed;
 		}
-
-		branch[n].bh = bh;
 		lock_buffer(bh);
 		BUFFER_TRACE(bh, "call get_create_access");
 		err = ext4_journal_get_create_access(handle, bh);
 		if (err) {
-			/* Don't brelse(bh) here; it's done in
-			 * ext4_journal_forget() below */
 			unlock_buffer(bh);
 			goto failed;
 		}
 
-		memset(bh-&gt;b_data, 0, blocksize);
-		branch[n].p = (__le32 *) bh-&gt;b_data + offsets[n];
-		branch[n].key = cpu_to_le32(new_blocks[n]);
-		*branch[n].p = branch[n].key;
-		if (n == indirect_blks) {
-			current_block = new_blocks[n];
-			/*
-			 * End of chain, update the last new metablock of
-			 * the chain to point to the new allocated
-			 * data blocks numbers
-			 */
-			for (i = 1; i &lt; num; i++)
-				*(branch[n].p + i) = cpu_to_le32(++current_block);
-		}
+		memset(bh-&gt;b_data, 0, bh-&gt;b_size);
+		p = branch[i].p = (__le32 *) bh-&gt;b_data + offsets[i];
+		b = new_blocks[i];
+
+		if (i == indirect_blks)
+			len = ar.len;
+		for (j = 0; j &lt; len; j++)
+			*p++ = cpu_to_le32(b++);
+
 		BUFFER_TRACE(bh, "marking uptodate");
 		set_buffer_uptodate(bh);
 		unlock_buffer(bh);
@@ -511,25 +385,16 @@ static int ext4_alloc_branch(handle_t *handle, struct inode *inode,
 		if (err)
 			goto failed;
 	}
-	*blks = num;
-	return err;
+	*blks = ar.len;
+	return 0;
 failed:
-	/* Allocation failed, free what we already allocated */
-	ext4_free_blocks(handle, inode, NULL, new_blocks[0], 1, 0);
-	for (i = 1; i &lt;= n ; i++) {
-		/*
-		 * branch[i].bh is newly allocated, so there is no
-		 * need to revoke the block, which is why we don't
-		 * need to set EXT4_FREE_BLOCKS_METADATA.
-		 */
-		ext4_free_blocks(handle, inode, NULL, new_blocks[i], 1,
-				 EXT4_FREE_BLOCKS_FORGET);
+	for (; i &gt;= 0; i--) {
+		if (i != indirect_blks &amp;&amp; branch[i].bh)
+			ext4_forget(handle, 1, inode, branch[i].bh,
+				    branch[i].bh-&gt;b_blocknr);
+		ext4_free_blocks(handle, inode, NULL, new_blocks[i],
+				 (i == indirect_blks) ? ar.len : 1, 0);
 	}
-	for (i = n+1; i &lt; indirect_blks; i++)
-		ext4_free_blocks(handle, inode, NULL, new_blocks[i], 1, 0);
-
-	ext4_free_blocks(handle, inode, NULL, new_blocks[i], num, 0);
-
 	return err;
 }
 </pre><hr><pre>commit 74d553aad7926ed05e05d9d5cff516a7b31375fc
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Apr 3 12:39:17 2013 -0400

    ext4: collapse handling of data=ordered and data=writeback codepaths
    
    The only difference between how we handle data=ordered and
    data=writeback is a single call to ext4_jbd2_file_inode().  Eliminate
    code duplication by factoring out redundant the code paths.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;
    Reviewed-by: Lukas Czerner &lt;lczerner@redhat.com&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 3b83cd604796..f91e11bd9753 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1374,6 +1374,7 @@ enum {
 	EXT4_STATE_DIOREAD_LOCK,	/* Disable support for dio read
 					   nolocking */
 	EXT4_STATE_MAY_INLINE_DATA,	/* may have in-inode data */
+	EXT4_STATE_ORDERED_MODE,	/* data=ordered mode */
 };
 
 #define EXT4_INODE_BIT_FNS(name, field, offset)				\
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index b3a5213bc73e..4ee69270a48a 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1145,77 +1145,36 @@ static int ext4_generic_write_end(struct file *file,
  * ext4 never places buffers on inode-&gt;i_mapping-&gt;private_list.  metadata
  * buffers are managed internally.
  */
-static int ext4_ordered_write_end(struct file *file,
-				  struct address_space *mapping,
-				  loff_t pos, unsigned len, unsigned copied,
-				  struct page *page, void *fsdata)
+static int ext4_write_end(struct file *file,
+			  struct address_space *mapping,
+			  loff_t pos, unsigned len, unsigned copied,
+			  struct page *page, void *fsdata)
 {
 	handle_t *handle = ext4_journal_current_handle();
 	struct inode *inode = mapping-&gt;host;
 	int ret = 0, ret2;
 
-	trace_ext4_ordered_write_end(inode, pos, len, copied);
-	ret = ext4_jbd2_file_inode(handle, inode);
-
-	if (ret == 0) {
-		ret2 = ext4_generic_write_end(file, mapping, pos, len, copied,
-							page, fsdata);
-		copied = ret2;
-		if (pos + len &gt; inode-&gt;i_size &amp;&amp; ext4_can_truncate(inode))
-			/* if we have allocated more blocks and copied
-			 * less. We will have blocks allocated outside
-			 * inode-&gt;i_size. So truncate them
-			 */
-			ext4_orphan_add(handle, inode);
-		if (ret2 &lt; 0)
-			ret = ret2;
-	} else {
-		unlock_page(page);
-		page_cache_release(page);
-	}
-
-	ret2 = ext4_journal_stop(handle);
-	if (!ret)
-		ret = ret2;
-
-	if (pos + len &gt; inode-&gt;i_size) {
-		ext4_truncate_failed_write(inode);
-		/*
-		 * If truncate failed early the inode might still be
-		 * on the orphan list; we need to make sure the inode
-		 * is removed from the orphan list in that case.
-		 */
-		if (inode-&gt;i_nlink)
-			ext4_orphan_del(NULL, inode);
+	trace_ext4_write_end(inode, pos, len, copied);
+	if (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) {
+		ret = ext4_jbd2_file_inode(handle, inode);
+		if (ret) {
+			unlock_page(page);
+			page_cache_release(page);
+			goto errout;
+		}
 	}
 
-
-	return ret ? ret : copied;
-}
-
-static int ext4_writeback_write_end(struct file *file,
-				    struct address_space *mapping,
-				    loff_t pos, unsigned len, unsigned copied,
-				    struct page *page, void *fsdata)
-{
-	handle_t *handle = ext4_journal_current_handle();
-	struct inode *inode = mapping-&gt;host;
-	int ret = 0, ret2;
-
-	trace_ext4_writeback_write_end(inode, pos, len, copied);
-	ret2 = ext4_generic_write_end(file, mapping, pos, len, copied,
-							page, fsdata);
-	copied = ret2;
+	copied = ext4_generic_write_end(file, mapping, pos, len, copied,
+					page, fsdata);
+	if (copied &lt; 0)
+		ret = copied;
 	if (pos + len &gt; inode-&gt;i_size &amp;&amp; ext4_can_truncate(inode))
 		/* if we have allocated more blocks and copied
 		 * less. We will have blocks allocated outside
 		 * inode-&gt;i_size. So truncate them
 		 */
 		ext4_orphan_add(handle, inode);
-
-	if (ret2 &lt; 0)
-		ret = ret2;
-
+errout:
 	ret2 = ext4_journal_stop(handle);
 	if (!ret)
 		ret = ret2;
@@ -2818,18 +2777,9 @@ static int ext4_da_write_end(struct file *file,
 	unsigned long start, end;
 	int write_mode = (int)(unsigned long)fsdata;
 
-	if (write_mode == FALL_BACK_TO_NONDELALLOC) {
-		switch (ext4_inode_journal_mode(inode)) {
-		case EXT4_INODE_ORDERED_DATA_MODE:
-			return ext4_ordered_write_end(file, mapping, pos,
-					len, copied, page, fsdata);
-		case EXT4_INODE_WRITEBACK_DATA_MODE:
-			return ext4_writeback_write_end(file, mapping, pos,
-					len, copied, page, fsdata);
-		default:
-			BUG();
-		}
-	}
+	if (write_mode == FALL_BACK_TO_NONDELALLOC)
+		return ext4_write_end(file, mapping, pos,
+				      len, copied, page, fsdata);
 
 	trace_ext4_da_write_end(inode, pos, len, copied);
 	start = pos &amp; (PAGE_CACHE_SIZE - 1);
@@ -3334,27 +3284,12 @@ static int ext4_journalled_set_page_dirty(struct page *page)
 	return __set_page_dirty_nobuffers(page);
 }
 
-static const struct address_space_operations ext4_ordered_aops = {
+static const struct address_space_operations ext4_aops = {
 	.readpage		= ext4_readpage,
 	.readpages		= ext4_readpages,
 	.writepage		= ext4_writepage,
 	.write_begin		= ext4_write_begin,
-	.write_end		= ext4_ordered_write_end,
-	.bmap			= ext4_bmap,
-	.invalidatepage		= ext4_invalidatepage,
-	.releasepage		= ext4_releasepage,
-	.direct_IO		= ext4_direct_IO,
-	.migratepage		= buffer_migrate_page,
-	.is_partially_uptodate  = block_is_partially_uptodate,
-	.error_remove_page	= generic_error_remove_page,
-};
-
-static const struct address_space_operations ext4_writeback_aops = {
-	.readpage		= ext4_readpage,
-	.readpages		= ext4_readpages,
-	.writepage		= ext4_writepage,
-	.write_begin		= ext4_write_begin,
-	.write_end		= ext4_writeback_write_end,
+	.write_end		= ext4_write_end,
 	.bmap			= ext4_bmap,
 	.invalidatepage		= ext4_invalidatepage,
 	.releasepage		= ext4_releasepage,
@@ -3399,23 +3334,21 @@ void ext4_set_aops(struct inode *inode)
 {
 	switch (ext4_inode_journal_mode(inode)) {
 	case EXT4_INODE_ORDERED_DATA_MODE:
-		if (test_opt(inode-&gt;i_sb, DELALLOC))
-			inode-&gt;i_mapping-&gt;a_ops = &amp;ext4_da_aops;
-		else
-			inode-&gt;i_mapping-&gt;a_ops = &amp;ext4_ordered_aops;
+		ext4_set_inode_state(inode, EXT4_STATE_ORDERED_MODE);
 		break;
 	case EXT4_INODE_WRITEBACK_DATA_MODE:
-		if (test_opt(inode-&gt;i_sb, DELALLOC))
-			inode-&gt;i_mapping-&gt;a_ops = &amp;ext4_da_aops;
-		else
-			inode-&gt;i_mapping-&gt;a_ops = &amp;ext4_writeback_aops;
+		ext4_clear_inode_state(inode, EXT4_STATE_ORDERED_MODE);
 		break;
 	case EXT4_INODE_JOURNAL_DATA_MODE:
 		inode-&gt;i_mapping-&gt;a_ops = &amp;ext4_journalled_aops;
-		break;
+		return;
 	default:
 		BUG();
 	}
+	if (test_opt(inode-&gt;i_sb, DELALLOC))
+		inode-&gt;i_mapping-&gt;a_ops = &amp;ext4_da_aops;
+	else
+		inode-&gt;i_mapping-&gt;a_ops = &amp;ext4_aops;
 }
 
 
diff --git a/include/trace/events/ext4.h b/include/trace/events/ext4.h
index 4ee471003859..58459b785565 100644
--- a/include/trace/events/ext4.h
+++ b/include/trace/events/ext4.h
@@ -257,15 +257,7 @@ DECLARE_EVENT_CLASS(ext4__write_end,
 		  __entry-&gt;pos, __entry-&gt;len, __entry-&gt;copied)
 );
 
-DEFINE_EVENT(ext4__write_end, ext4_ordered_write_end,
-
-	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len,
-		 unsigned int copied),
-
-	TP_ARGS(inode, pos, len, copied)
-);
-
-DEFINE_EVENT(ext4__write_end, ext4_writeback_write_end,
+DEFINE_EVENT(ext4__write_end, ext4_write_end,
 
 	TP_PROTO(struct inode *inode, loff_t pos, unsigned int len,
 		 unsigned int copied),</pre><hr><pre>commit 2b405bfa84063bfa35621d2d6879f52693c614b0
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Mar 20 09:42:11 2013 -0400

    ext4: fix data=journal fast mount/umount hang
    
    In data=journal mode, if we unmount the file system before a
    transaction has a chance to complete, when the journal inode is being
    evicted, we can end up calling into jbd2_log_wait_commit() for the
    last transaction, after the journalling machinery has been shut down.
    
    Arguably we should adjust ext4_should_journal_data() to return FALSE
    for the journal inode, but the only place it matters is
    ext4_evict_inode(), and so to save a bit of CPU time, and to make the
    patch much more obviously correct by inspection(tm), we'll fix it by
    explicitly not trying to waiting for a journal commit when we are
    evicting the journal inode, since it's guaranteed to never succeed in
    this case.
    
    This can be easily replicated via:
    
         mount -t ext4 -o data=journal /dev/vdb /vdb ; umount /vdb
    
    ------------[ cut here ]------------
    WARNING: at /usr/projects/linux/ext4/fs/jbd2/journal.c:542 __jbd2_log_start_commit+0xba/0xcd()
    Hardware name: Bochs
    JBD2: bad log_start_commit: 3005630206 3005630206 0 0
    Modules linked in:
    Pid: 2909, comm: umount Not tainted 3.8.0-rc3 #1020
    Call Trace:
     [&lt;c015c0ef&gt;] warn_slowpath_common+0x68/0x7d
     [&lt;c02b7e7d&gt;] ? __jbd2_log_start_commit+0xba/0xcd
     [&lt;c015c177&gt;] warn_slowpath_fmt+0x2b/0x2f
     [&lt;c02b7e7d&gt;] __jbd2_log_start_commit+0xba/0xcd
     [&lt;c02b8075&gt;] jbd2_log_start_commit+0x24/0x34
     [&lt;c0279ed5&gt;] ext4_evict_inode+0x71/0x2e3
     [&lt;c021f0ec&gt;] evict+0x94/0x135
     [&lt;c021f9aa&gt;] iput+0x10a/0x110
     [&lt;c02b7836&gt;] jbd2_journal_destroy+0x190/0x1ce
     [&lt;c0175284&gt;] ? bit_waitqueue+0x50/0x50
     [&lt;c028d23f&gt;] ext4_put_super+0x52/0x294
     [&lt;c020efe3&gt;] generic_shutdown_super+0x48/0xb4
     [&lt;c020f071&gt;] kill_block_super+0x22/0x60
     [&lt;c020f3e0&gt;] deactivate_locked_super+0x22/0x49
     [&lt;c020f5d6&gt;] deactivate_super+0x30/0x33
     [&lt;c0222795&gt;] mntput_no_expire+0x107/0x10c
     [&lt;c02233a7&gt;] sys_umount+0x2cf/0x2e0
     [&lt;c02233ca&gt;] sys_oldumount+0x12/0x14
     [&lt;c08096b8&gt;] syscall_call+0x7/0xb
    ---[ end trace 6a954cc790501c1f ]---
    jbd2_log_wait_commit: error: j_commit_request=-1289337090, tid=0
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;
    Reviewed-by: Jan Kara &lt;jack@suse.cz&gt;
    Cc: stable@vger.kernel.org

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index ea5f24ffa60c..85e41a2a39ad 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -205,7 +205,8 @@ void ext4_evict_inode(struct inode *inode)
 		 * don't use page cache.
 		 */
 		if (ext4_should_journal_data(inode) &amp;&amp;
-		    (S_ISLNK(inode-&gt;i_mode) || S_ISREG(inode-&gt;i_mode))) {
+		    (S_ISLNK(inode-&gt;i_mode) || S_ISREG(inode-&gt;i_mode)) &amp;&amp;
+		    inode-&gt;i_ino != EXT4_JOURNAL_INO) {
 			journal_t *journal = EXT4_SB(inode-&gt;i_sb)-&gt;s_journal;
 			tid_t commit_tid = EXT4_I(inode)-&gt;i_datasync_tid;
 </pre><hr><pre>commit 1ada47d9468fe3907f7f9e00179168f5e2f90803
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Mar 20 09:39:42 2013 -0400

    ext4: fix ext4_evict_inode() racing against workqueue processing code
    
    Commit 84c17543ab56 (ext4: move work from io_end to inode) triggered a
    regression when running xfstest #270 when the file system is mounted
    with dioread_nolock.
    
    The problem is that after ext4_evict_inode() calls ext4_ioend_wait(),
    this guarantees that last io_end structure has been freed, but it does
    not guarantee that the workqueue structure, which was moved into the
    inode by commit 84c17543ab56, is actually finished.  Once
    ext4_flush_completed_IO() calls ext4_free_io_end() on CPU #1, this
    will allow ext4_ioend_wait() to return on CPU #2, at which point the
    evict_inode() codepath can race against the workqueue code on CPU #1
    accessing EXT4_I(inode)-&gt;i_unwritten_work to find the next item of
    work to do.
    
    Fix this by calling cancel_work_sync() in ext4_ioend_wait(), which
    will be renamed ext4_ioend_shutdown(), since it is only used by
    ext4_evict_inode().  Also, move the call to ext4_ioend_shutdown()
    until after truncate_inode_pages() and filemap_write_and_wait() are
    called, to make sure all dirty pages have been written back and
    flushed from the page cache first.
    
    BUG: unable to handle kernel NULL pointer dereference at   (null)
    IP: [&lt;c01dda6a&gt;] cwq_activate_delayed_work+0x3b/0x7e
    *pdpt = 0000000030bc3001 *pde = 0000000000000000
    Oops: 0000 [#1] SMP DEBUG_PAGEALLOC
    Modules linked in:
    Pid: 6, comm: kworker/u:0 Not tainted 3.8.0-rc3-00013-g84c1754-dirty #91 Bochs Bochs
    EIP: 0060:[&lt;c01dda6a&gt;] EFLAGS: 00010046 CPU: 0
    EIP is at cwq_activate_delayed_work+0x3b/0x7e
    EAX: 00000000 EBX: 00000000 ECX: f505fe54 EDX: 00000000
    ESI: ed5b697c EDI: 00000006 EBP: f64b7e8c ESP: f64b7e84
     DS: 007b ES: 007b FS: 00d8 GS: 0000 SS: 0068
    CR0: 8005003b CR2: 00000000 CR3: 30bc2000 CR4: 000006f0
    DR0: 00000000 DR1: 00000000 DR2: 00000000 DR3: 00000000
    DR6: ffff0ff0 DR7: 00000400
    Process kworker/u:0 (pid: 6, ti=f64b6000 task=f64b4160 task.ti=f64b6000)
    Stack:
     f505fe00 00000006 f64b7e9c c01de3d7 f6435540 00000003 f64b7efc c01def1d
     f6435540 00000002 00000000 0000008a c16d0808 c040a10b c16d07d8 c16d08b0
     f505fe00 c16d0780 00000000 00000000 ee153df4 c1ce4a30 c17d0e30 00000000
    Call Trace:
     [&lt;c01de3d7&gt;] cwq_dec_nr_in_flight+0x71/0xfb
     [&lt;c01def1d&gt;] process_one_work+0x5d8/0x637
     [&lt;c040a10b&gt;] ? ext4_end_bio+0x300/0x300
     [&lt;c01e3105&gt;] worker_thread+0x249/0x3ef
     [&lt;c01ea317&gt;] kthread+0xd8/0xeb
     [&lt;c01e2ebc&gt;] ? manage_workers+0x4bb/0x4bb
     [&lt;c023a370&gt;] ? trace_hardirqs_on+0x27/0x37
     [&lt;c0f1b4b7&gt;] ret_from_kernel_thread+0x1b/0x28
     [&lt;c01ea23f&gt;] ? __init_kthread_worker+0x71/0x71
    Code: 01 83 15 ac ff 6c c1 00 31 db 89 c6 8b 00 a8 04 74 12 89 c3 30 db 83 05 b0 ff 6c c1 01 83 15 b4 ff 6c c1 00 89 f0 e8 42 ff ff ff &lt;8b&gt; 13 89 f0 83 05 b8 ff 6c c1
     6c c1 00 31 c9 83
    EIP: [&lt;c01dda6a&gt;] cwq_activate_delayed_work+0x3b/0x7e SS:ESP 0068:f64b7e84
    CR2: 0000000000000000
    ---[ end trace a1923229da53d8a4 ]---
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;
    Cc: Jan Kara &lt;jack@suse.cz&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 167ff564bbfa..3b83cd604796 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -2617,7 +2617,7 @@ extern int ext4_move_extents(struct file *o_filp, struct file *d_filp,
 extern int __init ext4_init_pageio(void);
 extern void ext4_add_complete_io(ext4_io_end_t *io_end);
 extern void ext4_exit_pageio(void);
-extern void ext4_ioend_wait(struct inode *);
+extern void ext4_ioend_shutdown(struct inode *);
 extern void ext4_free_io_end(ext4_io_end_t *io);
 extern ext4_io_end_t *ext4_init_io_end(struct inode *inode, gfp_t flags);
 extern void ext4_end_io_work(struct work_struct *work);
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 65bbc9339aca..ea5f24ffa60c 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -185,8 +185,6 @@ void ext4_evict_inode(struct inode *inode)
 
 	trace_ext4_evict_inode(inode);
 
-	ext4_ioend_wait(inode);
-
 	if (inode-&gt;i_nlink) {
 		/*
 		 * When journalling data dirty buffers are tracked only in the
@@ -216,6 +214,7 @@ void ext4_evict_inode(struct inode *inode)
 			filemap_write_and_wait(&amp;inode-&gt;i_data);
 		}
 		truncate_inode_pages(&amp;inode-&gt;i_data, 0);
+		ext4_ioend_shutdown(inode);
 		goto no_delete;
 	}
 
@@ -225,6 +224,7 @@ void ext4_evict_inode(struct inode *inode)
 	if (ext4_should_order_data(inode))
 		ext4_begin_ordered_truncate(inode, 0);
 	truncate_inode_pages(&amp;inode-&gt;i_data, 0);
+	ext4_ioend_shutdown(inode);
 
 	if (is_bad_inode(inode))
 		goto no_delete;
diff --git a/fs/ext4/page-io.c b/fs/ext4/page-io.c
index 809b31003ecc..047a6de04a0a 100644
--- a/fs/ext4/page-io.c
+++ b/fs/ext4/page-io.c
@@ -50,11 +50,21 @@ void ext4_exit_pageio(void)
 	kmem_cache_destroy(io_page_cachep);
 }
 
-void ext4_ioend_wait(struct inode *inode)
+/*
+ * This function is called by ext4_evict_inode() to make sure there is
+ * no more pending I/O completion work left to do.
+ */
+void ext4_ioend_shutdown(struct inode *inode)
 {
 	wait_queue_head_t *wq = ext4_ioend_wq(inode);
 
 	wait_event(*wq, (atomic_read(&amp;EXT4_I(inode)-&gt;i_ioend_count) == 0));
+	/*
+	 * We need to make sure the work structure is finished being
+	 * used before we let the inode get destroyed.
+	 */
+	if (work_pending(&amp;EXT4_I(inode)-&gt;i_unwritten_work))
+		cancel_work_sync(&amp;EXT4_I(inode)-&gt;i_unwritten_work);
 }
 
 static void put_io_page(struct ext4_io_page *io_page)</pre><hr><pre>commit 90ba983f6889e65a3b506b30dc606aa9d1d46cd2
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Mar 11 23:39:59 2013 -0400

    ext4: use atomic64_t for the per-flexbg free_clusters count
    
    A user who was using a 8TB+ file system and with a very large flexbg
    size (&gt; 65536) could cause the atomic_t used in the struct flex_groups
    to overflow.  This was detected by PaX security patchset:
    
    http://forums.grsecurity.net/viewtopic.php?f=3&amp;t=3289&amp;p=12551#p12551
    
    This bug was introduced in commit 9f24e4208f7e, so it's been around
    since 2.6.30.  :-(
    
    Fix this by using an atomic64_t for struct orlav_stats's
    free_clusters.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;
    Reviewed-by: Lukas Czerner &lt;lczerner@redhat.com&gt;
    Cc: stable@vger.kernel.org

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 4a01ba315262..167ff564bbfa 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -335,9 +335,9 @@ struct ext4_group_desc
  */
 
 struct flex_groups {
-	atomic_t free_inodes;
-	atomic_t free_clusters;
-	atomic_t used_dirs;
+	atomic64_t	free_clusters;
+	atomic_t	free_inodes;
+	atomic_t	used_dirs;
 };
 
 #define EXT4_BG_INODE_UNINIT	0x0001 /* Inode table/bitmap not in use */
diff --git a/fs/ext4/ialloc.c b/fs/ext4/ialloc.c
index 32fd2b9075dd..6c5bb8d993fe 100644
--- a/fs/ext4/ialloc.c
+++ b/fs/ext4/ialloc.c
@@ -324,8 +324,8 @@ void ext4_free_inode(handle_t *handle, struct inode *inode)
 }
 
 struct orlov_stats {
+	__u64 free_clusters;
 	__u32 free_inodes;
-	__u32 free_clusters;
 	__u32 used_dirs;
 };
 
@@ -342,7 +342,7 @@ static void get_orlov_stats(struct super_block *sb, ext4_group_t g,
 
 	if (flex_size &gt; 1) {
 		stats-&gt;free_inodes = atomic_read(&amp;flex_group[g].free_inodes);
-		stats-&gt;free_clusters = atomic_read(&amp;flex_group[g].free_clusters);
+		stats-&gt;free_clusters = atomic64_read(&amp;flex_group[g].free_clusters);
 		stats-&gt;used_dirs = atomic_read(&amp;flex_group[g].used_dirs);
 		return;
 	}
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 8b2ea9f75004..ee6614bdb639 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2804,8 +2804,8 @@ ext4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi,
 							  ac-&gt;ac_b_ex.fe_group);
-		atomic_sub(ac-&gt;ac_b_ex.fe_len,
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
+		atomic64_sub(ac-&gt;ac_b_ex.fe_len,
+			     &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 	}
 
 	err = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);
@@ -4661,8 +4661,8 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
-		atomic_add(count_clusters,
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
+		atomic64_add(count_clusters,
+			     &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 	}
 
 	ext4_mb_unload_buddy(&amp;e4b);
@@ -4804,8 +4804,8 @@ int ext4_group_add_blocks(handle_t *handle, struct super_block *sb,
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
-		atomic_add(EXT4_NUM_B2C(sbi, blocks_freed),
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
+		atomic64_add(EXT4_NUM_B2C(sbi, blocks_freed),
+			     &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 	}
 
 	ext4_mb_unload_buddy(&amp;e4b);
diff --git a/fs/ext4/resize.c b/fs/ext4/resize.c
index b2c8ee56eb98..c169477a62c9 100644
--- a/fs/ext4/resize.c
+++ b/fs/ext4/resize.c
@@ -1360,8 +1360,8 @@ static void ext4_update_super(struct super_block *sb,
 	    sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group;
 		flex_group = ext4_flex_group(sbi, group_data[0].group);
-		atomic_add(EXT4_NUM_B2C(sbi, free_blocks),
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
+		atomic64_add(EXT4_NUM_B2C(sbi, free_blocks),
+			     &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 		atomic_add(EXT4_INODES_PER_GROUP(sb) * flex_gd-&gt;count,
 			   &amp;sbi-&gt;s_flex_groups[flex_group].free_inodes);
 	}
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 9379b7fbfd92..d1ee6a84338a 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -1923,8 +1923,8 @@ static int ext4_fill_flex_info(struct super_block *sb)
 		flex_group = ext4_flex_group(sbi, i);
 		atomic_add(ext4_free_inodes_count(sb, gdp),
 			   &amp;sbi-&gt;s_flex_groups[flex_group].free_inodes);
-		atomic_add(ext4_free_group_clusters(sb, gdp),
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
+		atomic64_add(ext4_free_group_clusters(sb, gdp),
+			     &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 		atomic_add(ext4_used_dirs_count(sb, gdp),
 			   &amp;sbi-&gt;s_flex_groups[flex_group].used_dirs);
 	}</pre><hr><pre>commit b980955236922ae6106774511c5c05003d3ad225
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Mar 4 11:59:12 2013 -0500

    random: fix locking dependency with the tasklist_lock
    
    Commit 6133705494bb introduced a circular lock dependency because
    posix_cpu_timers_exit() is called by release_task(), which is holding
    a writer lock on tasklist_lock, and this can cause a deadlock since
    kill_fasync() gets called with nonblocking_pool.lock taken.
    
    There's no reason why kill_fasync() needs to be taken while the random
    pool is locked, so move it out to fix this locking dependency.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;
    Reported-by: Russ Dill &lt;Russ.Dill@gmail.com&gt;
    Cc: stable@kernel.org

diff --git a/drivers/char/random.c b/drivers/char/random.c
index 85e81ec1451e..57d4b152267c 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -852,6 +852,7 @@ static size_t account(struct entropy_store *r, size_t nbytes, int min,
 		      int reserved)
 {
 	unsigned long flags;
+	int wakeup_write = 0;
 
 	/* Hold lock while accounting */
 	spin_lock_irqsave(&amp;r-&gt;lock, flags);
@@ -873,10 +874,8 @@ static size_t account(struct entropy_store *r, size_t nbytes, int min,
 		else
 			r-&gt;entropy_count = reserved;
 
-		if (r-&gt;entropy_count &lt; random_write_wakeup_thresh) {
-			wake_up_interruptible(&amp;random_write_wait);
-			kill_fasync(&amp;fasync, SIGIO, POLL_OUT);
-		}
+		if (r-&gt;entropy_count &lt; random_write_wakeup_thresh)
+			wakeup_write = 1;
 	}
 
 	DEBUG_ENT("debiting %zu entropy credits from %s%s\n",
@@ -884,6 +883,11 @@ static size_t account(struct entropy_store *r, size_t nbytes, int min,
 
 	spin_unlock_irqrestore(&amp;r-&gt;lock, flags);
 
+	if (wakeup_write) {
+		wake_up_interruptible(&amp;random_write_wait);
+		kill_fasync(&amp;fasync, SIGIO, POLL_OUT);
+	}
+
 	return nbytes;
 }
 </pre><hr><pre>commit 1ac6466f253ef7bd063b7877fb056afe1820841c
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Mar 2 10:27:46 2013 -0500

    ext4: use percpu counter for extent cache count
    
    Use a percpu counter rather than atomic types for shrinker accounting.
    There's no need for ultimate accuracy in the shrinker, so this
    should come a little more cheaply.  The percpu struct is somewhat
    large, but there was a big gap before the cache-aligned
    s_es_lru_lock anyway, and it fits nicely in there.
    
    Signed-off-by: Eric Sandeen &lt;sandeen@redhat.com&gt;
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 96c10934bb96..4a01ba315262 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1268,7 +1268,6 @@ struct ext4_sb_info {
 	atomic_t s_mb_preallocated;
 	atomic_t s_mb_discarded;
 	atomic_t s_lock_busy;
-	atomic_t s_extent_cache_cnt;
 
 	/* locality groups */
 	struct ext4_locality_group __percpu *s_locality_groups;
@@ -1310,6 +1309,7 @@ struct ext4_sb_info {
 	/* Reclaim extents from extent status tree */
 	struct shrinker s_es_shrinker;
 	struct list_head s_es_lru;
+	struct percpu_counter s_extent_cache_cnt;
 	spinlock_t s_es_lru_lock ____cacheline_aligned_in_smp;
 };
 
diff --git a/fs/ext4/extents_status.c b/fs/ext4/extents_status.c
index 27fcdd2b2607..95796a1b7522 100644
--- a/fs/ext4/extents_status.c
+++ b/fs/ext4/extents_status.c
@@ -305,7 +305,7 @@ ext4_es_alloc_extent(struct inode *inode, ext4_lblk_t lblk, ext4_lblk_t len,
 	 */
 	if (!ext4_es_is_delayed(es)) {
 		EXT4_I(inode)-&gt;i_es_lru_nr++;
-		atomic_inc(&amp;EXT4_SB(inode-&gt;i_sb)-&gt;s_extent_cache_cnt);
+		percpu_counter_inc(&amp;EXT4_SB(inode-&gt;i_sb)-&gt;s_extent_cache_cnt);
 	}
 
 	return es;
@@ -317,7 +317,7 @@ static void ext4_es_free_extent(struct inode *inode, struct extent_status *es)
 	if (!ext4_es_is_delayed(es)) {
 		BUG_ON(EXT4_I(inode)-&gt;i_es_lru_nr == 0);
 		EXT4_I(inode)-&gt;i_es_lru_nr--;
-		atomic_dec(&amp;EXT4_SB(inode-&gt;i_sb)-&gt;s_extent_cache_cnt);
+		percpu_counter_dec(&amp;EXT4_SB(inode-&gt;i_sb)-&gt;s_extent_cache_cnt);
 	}
 
 	kmem_cache_free(ext4_es_cachep, es);
@@ -678,7 +678,7 @@ static int ext4_es_shrink(struct shrinker *shrink, struct shrink_control *sc)
 	int nr_to_scan = sc-&gt;nr_to_scan;
 	int ret, nr_shrunk = 0;
 
-	ret = atomic_read(&amp;sbi-&gt;s_extent_cache_cnt);
+	ret = percpu_counter_read_positive(&amp;sbi-&gt;s_extent_cache_cnt);
 	trace_ext4_es_shrink_enter(sbi-&gt;s_sb, nr_to_scan, ret);
 
 	if (!nr_to_scan)
@@ -711,7 +711,7 @@ static int ext4_es_shrink(struct shrinker *shrink, struct shrink_control *sc)
 	list_splice_tail(&amp;scanned, &amp;sbi-&gt;s_es_lru);
 	spin_unlock(&amp;sbi-&gt;s_es_lru_lock);
 
-	ret = atomic_read(&amp;sbi-&gt;s_extent_cache_cnt);
+	ret = percpu_counter_read_positive(&amp;sbi-&gt;s_extent_cache_cnt);
 	trace_ext4_es_shrink_exit(sbi-&gt;s_sb, nr_shrunk, ret);
 	return ret;
 }
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 373d46cd5d3f..1ae5860b30a3 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -783,6 +783,7 @@ static void ext4_put_super(struct super_block *sb)
 	percpu_counter_destroy(&amp;sbi-&gt;s_freeinodes_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_dirs_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_dirtyclusters_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_extent_cache_cnt);
 	brelse(sbi-&gt;s_sbh);
 #ifdef CONFIG_QUOTA
 	for (i = 0; i &lt; MAXQUOTAS; i++)
@@ -3688,6 +3689,9 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	if (!err) {
 		err = percpu_counter_init(&amp;sbi-&gt;s_dirtyclusters_counter, 0);
 	}
+	if (!err) {
+		err = percpu_counter_init(&amp;sbi-&gt;s_extent_cache_cnt, 0);
+	}
 	if (err) {
 		ext4_msg(sb, KERN_ERR, "insufficient memory");
 		goto failed_mount3;
@@ -3993,6 +3997,7 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	percpu_counter_destroy(&amp;sbi-&gt;s_freeinodes_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_dirs_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_dirtyclusters_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_extent_cache_cnt);
 	if (sbi-&gt;s_mmp_tsk)
 		kthread_stop(sbi-&gt;s_mmp_tsk);
 failed_mount2:</pre><hr><pre>commit 246307745c406379996e6ed6411f0e20f1ce1449
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Feb 28 23:58:56 2013 -0500

    ext4: optimize ext4_es_shrink()
    
    When the system is under memory pressure, ext4_es_srhink() will get
    called very often.  So optimize returning the number of items in the
    file system's extent status cache by keeping a per-filesystem count,
    instead of calculating it each time by scanning all of the inodes in
    the extent status cache.
    
    Also rename the slab used for the extent status cache to be
    "ext4_extent_status" so it's obviousl the slab in question is created
    by ext4.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;
    Cc: Zheng Liu &lt;gnehzuil.liu@gmail.com&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 6e16c1867959..96c10934bb96 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1268,6 +1268,7 @@ struct ext4_sb_info {
 	atomic_t s_mb_preallocated;
 	atomic_t s_mb_discarded;
 	atomic_t s_lock_busy;
+	atomic_t s_extent_cache_cnt;
 
 	/* locality groups */
 	struct ext4_locality_group __percpu *s_locality_groups;
diff --git a/fs/ext4/extents_status.c b/fs/ext4/extents_status.c
index f768f4a98a2b..27fcdd2b2607 100644
--- a/fs/ext4/extents_status.c
+++ b/fs/ext4/extents_status.c
@@ -147,11 +147,12 @@ static int __es_remove_extent(struct inode *inode, ext4_lblk_t lblk,
 			      ext4_lblk_t end);
 static int __es_try_to_reclaim_extents(struct ext4_inode_info *ei,
 				       int nr_to_scan);
-static int ext4_es_reclaim_extents_count(struct super_block *sb);
 
 int __init ext4_init_es(void)
 {
-	ext4_es_cachep = KMEM_CACHE(extent_status, SLAB_RECLAIM_ACCOUNT);
+	ext4_es_cachep = kmem_cache_create("ext4_extent_status",
+					   sizeof(struct extent_status),
+					   0, (SLAB_RECLAIM_ACCOUNT), NULL);
 	if (ext4_es_cachep == NULL)
 		return -ENOMEM;
 	return 0;
@@ -302,8 +303,10 @@ ext4_es_alloc_extent(struct inode *inode, ext4_lblk_t lblk, ext4_lblk_t len,
 	/*
 	 * We don't count delayed extent because we never try to reclaim them
 	 */
-	if (!ext4_es_is_delayed(es))
+	if (!ext4_es_is_delayed(es)) {
 		EXT4_I(inode)-&gt;i_es_lru_nr++;
+		atomic_inc(&amp;EXT4_SB(inode-&gt;i_sb)-&gt;s_extent_cache_cnt);
+	}
 
 	return es;
 }
@@ -314,6 +317,7 @@ static void ext4_es_free_extent(struct inode *inode, struct extent_status *es)
 	if (!ext4_es_is_delayed(es)) {
 		BUG_ON(EXT4_I(inode)-&gt;i_es_lru_nr == 0);
 		EXT4_I(inode)-&gt;i_es_lru_nr--;
+		atomic_dec(&amp;EXT4_SB(inode-&gt;i_sb)-&gt;s_extent_cache_cnt);
 	}
 
 	kmem_cache_free(ext4_es_cachep, es);
@@ -674,10 +678,11 @@ static int ext4_es_shrink(struct shrinker *shrink, struct shrink_control *sc)
 	int nr_to_scan = sc-&gt;nr_to_scan;
 	int ret, nr_shrunk = 0;
 
-	trace_ext4_es_shrink_enter(sbi-&gt;s_sb, nr_to_scan);
+	ret = atomic_read(&amp;sbi-&gt;s_extent_cache_cnt);
+	trace_ext4_es_shrink_enter(sbi-&gt;s_sb, nr_to_scan, ret);
 
 	if (!nr_to_scan)
-		return ext4_es_reclaim_extents_count(sbi-&gt;s_sb);
+		return ret;
 
 	INIT_LIST_HEAD(&amp;scanned);
 
@@ -705,9 +710,10 @@ static int ext4_es_shrink(struct shrinker *shrink, struct shrink_control *sc)
 	}
 	list_splice_tail(&amp;scanned, &amp;sbi-&gt;s_es_lru);
 	spin_unlock(&amp;sbi-&gt;s_es_lru_lock);
-	trace_ext4_es_shrink_exit(sbi-&gt;s_sb, nr_shrunk);
 
-	return ext4_es_reclaim_extents_count(sbi-&gt;s_sb);
+	ret = atomic_read(&amp;sbi-&gt;s_extent_cache_cnt);
+	trace_ext4_es_shrink_exit(sbi-&gt;s_sb, nr_shrunk, ret);
+	return ret;
 }
 
 void ext4_es_register_shrinker(struct super_block *sb)
@@ -751,25 +757,6 @@ void ext4_es_lru_del(struct inode *inode)
 	spin_unlock(&amp;sbi-&gt;s_es_lru_lock);
 }
 
-static int ext4_es_reclaim_extents_count(struct super_block *sb)
-{
-	struct ext4_sb_info *sbi = EXT4_SB(sb);
-	struct ext4_inode_info *ei;
-	struct list_head *cur;
-	int nr_cached = 0;
-
-	spin_lock(&amp;sbi-&gt;s_es_lru_lock);
-	list_for_each(cur, &amp;sbi-&gt;s_es_lru) {
-		ei = list_entry(cur, struct ext4_inode_info, i_es_lru);
-		read_lock(&amp;ei-&gt;i_es_lock);
-		nr_cached += ei-&gt;i_es_lru_nr;
-		read_unlock(&amp;ei-&gt;i_es_lock);
-	}
-	spin_unlock(&amp;sbi-&gt;s_es_lru_lock);
-	trace_ext4_es_reclaim_extents_count(sb, nr_cached);
-	return nr_cached;
-}
-
 static int __es_try_to_reclaim_extents(struct ext4_inode_info *ei,
 				       int nr_to_scan)
 {
diff --git a/include/trace/events/ext4.h b/include/trace/events/ext4.h
index c0457c0d1a68..4ee471003859 100644
--- a/include/trace/events/ext4.h
+++ b/include/trace/events/ext4.h
@@ -2255,64 +2255,48 @@ TRACE_EVENT(ext4_es_lookup_extent_exit,
 		  __entry-&gt;found ? __entry-&gt;status : 0)
 );
 
-TRACE_EVENT(ext4_es_reclaim_extents_count,
-	TP_PROTO(struct super_block *sb, int nr_cached),
-
-	TP_ARGS(sb, nr_cached),
-
-	TP_STRUCT__entry(
-		__field(	dev_t,	dev			)
-		__field(	int,	nr_cached		)
-	),
-
-	TP_fast_assign(
-		__entry-&gt;dev		= sb-&gt;s_dev;
-		__entry-&gt;nr_cached	= nr_cached;
-	),
-
-	TP_printk("dev %d,%d cached objects nr %d",
-		  MAJOR(__entry-&gt;dev), MINOR(__entry-&gt;dev),
-		  __entry-&gt;nr_cached)
-);
-
 TRACE_EVENT(ext4_es_shrink_enter,
-	TP_PROTO(struct super_block *sb, int nr_to_scan),
+	TP_PROTO(struct super_block *sb, int nr_to_scan, int cache_cnt),
 
-	TP_ARGS(sb, nr_to_scan),
+	TP_ARGS(sb, nr_to_scan, cache_cnt),
 
 	TP_STRUCT__entry(
 		__field(	dev_t,	dev			)
 		__field(	int,	nr_to_scan		)
+		__field(	int,	cache_cnt		)
 	),
 
 	TP_fast_assign(
 		__entry-&gt;dev		= sb-&gt;s_dev;
 		__entry-&gt;nr_to_scan	= nr_to_scan;
+		__entry-&gt;cache_cnt	= cache_cnt;
 	),
 
-	TP_printk("dev %d,%d nr to scan %d",
+	TP_printk("dev %d,%d nr_to_scan %d cache_cnt %d",
 		  MAJOR(__entry-&gt;dev), MINOR(__entry-&gt;dev),
-		  __entry-&gt;nr_to_scan)
+		  __entry-&gt;nr_to_scan, __entry-&gt;cache_cnt)
 );
 
 TRACE_EVENT(ext4_es_shrink_exit,
-	TP_PROTO(struct super_block *sb, int shrunk_nr),
+	TP_PROTO(struct super_block *sb, int shrunk_nr, int cache_cnt),
 
-	TP_ARGS(sb, shrunk_nr),
+	TP_ARGS(sb, shrunk_nr, cache_cnt),
 
 	TP_STRUCT__entry(
 		__field(	dev_t,	dev			)
 		__field(	int,	shrunk_nr		)
+		__field(	int,	cache_cnt		)
 	),
 
 	TP_fast_assign(
 		__entry-&gt;dev		= sb-&gt;s_dev;
 		__entry-&gt;shrunk_nr	= shrunk_nr;
+		__entry-&gt;cache_cnt	= cache_cnt;
 	),
 
-	TP_printk("dev %d,%d nr to scan %d",
+	TP_printk("dev %d,%d shrunk_nr %d cache_cnt %d",
 		  MAJOR(__entry-&gt;dev), MINOR(__entry-&gt;dev),
-		  __entry-&gt;shrunk_nr)
+		  __entry-&gt;shrunk_nr, __entry-&gt;cache_cnt)
 );
 
 #endif /* _TRACE_EXT4_H */</pre><hr><pre>commit 8e919d13048cd5acaadb2b15b48acbfb8832d3c2
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Feb 27 14:54:37 2013 -0500

    ext4: fix extent status tree regression for file systems &gt; 512GB
    
    This fixes a regression introduced by commit f7fec032aa782.  The
    problem was that the extents status flags caused us to mask out block
    numbers smaller than 2**28 blocks.  Since we didn't test with file
    systems smaller than 512GB, we didn't notice this during the
    development cycle.
    
    A typical failure looks like this:
    
    EXT4-fs error (device sdb1): htree_dirblock_to_tree:919: inode #172235804: block
    152052301: comm ls: bad entry in directory: rec_len is smaller than minimal -
    offset=0(0), inode=0, rec_len=0, name_len=0
    
    ... where 'debugfs -R "stat &lt;172235804&gt;" /dev/sdb1' reports that the
    inode has block number 688923213.  When viewed in hex, block number
    152052301 (from the syslog) is 0x910224D, while block number 688923213
    is 0x2910224D.  Note the missing "0x20000000" in the block number.
    
    Reported-by: Markus Trippelsdorf &lt;markus@trippelsdorf.de&gt;
    Verified-by: Markus Trippelsdorf &lt;markus@trippelsdorf.de&gt;
    Reported-by: Dave Jones &lt;davej@redhat.com&gt;
    Verified-by: Dave Jones &lt;davej@redhat.com&gt;
    Cc: Zheng Liu &lt;gnehzuil.liu@gmail.com&gt;
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/extents_status.h b/fs/ext4/extents_status.h
index cf83e77b16cb..f190dfe969da 100644
--- a/fs/ext4/extents_status.h
+++ b/fs/ext4/extents_status.h
@@ -20,10 +20,13 @@
 #define es_debug(fmt, ...)	no_printk(fmt, ##__VA_ARGS__)
 #endif
 
-#define EXTENT_STATUS_WRITTEN	0x80000000	/* written extent */
-#define EXTENT_STATUS_UNWRITTEN	0x40000000	/* unwritten extent */
-#define EXTENT_STATUS_DELAYED	0x20000000	/* delayed extent */
-#define EXTENT_STATUS_HOLE	0x10000000	/* hole */
+/*
+ * These flags live in the high bits of extent_status.es_pblk
+ */
+#define EXTENT_STATUS_WRITTEN	(1ULL &lt;&lt; 63)
+#define EXTENT_STATUS_UNWRITTEN (1ULL &lt;&lt; 62)
+#define EXTENT_STATUS_DELAYED	(1ULL &lt;&lt; 61)
+#define EXTENT_STATUS_HOLE	(1ULL &lt;&lt; 60)
 
 #define EXTENT_STATUS_FLAGS	(EXTENT_STATUS_WRITTEN | \
 				 EXTENT_STATUS_UNWRITTEN | \
@@ -58,22 +61,22 @@ extern int ext4_es_lookup_extent(struct inode *inode, ext4_lblk_t lblk,
 
 static inline int ext4_es_is_written(struct extent_status *es)
 {
-	return (es-&gt;es_pblk &amp; EXTENT_STATUS_WRITTEN);
+	return (es-&gt;es_pblk &amp; EXTENT_STATUS_WRITTEN) != 0;
 }
 
 static inline int ext4_es_is_unwritten(struct extent_status *es)
 {
-	return (es-&gt;es_pblk &amp; EXTENT_STATUS_UNWRITTEN);
+	return (es-&gt;es_pblk &amp; EXTENT_STATUS_UNWRITTEN) != 0;
 }
 
 static inline int ext4_es_is_delayed(struct extent_status *es)
 {
-	return (es-&gt;es_pblk &amp; EXTENT_STATUS_DELAYED);
+	return (es-&gt;es_pblk &amp; EXTENT_STATUS_DELAYED) != 0;
 }
 
 static inline int ext4_es_is_hole(struct extent_status *es)
 {
-	return (es-&gt;es_pblk &amp; EXTENT_STATUS_HOLE);
+	return (es-&gt;es_pblk &amp; EXTENT_STATUS_HOLE) != 0;
 }
 
 static inline ext4_fsblk_t ext4_es_status(struct extent_status *es)</pre>
    <div class="pagination">
        <a href='1_51.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><span>[52]</span><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_53.html'>Next&gt;&gt;</a>
    <div>
</body>

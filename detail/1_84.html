<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_83.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><span>[84]</span><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_85.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 8e48dcfbd7c0892b4cfd064d682cc4c95a29df32
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun May 16 18:00:00 2010 -0400

    ext4: Use our own write_cache_pages()
    
    Make a copy of write_cache_pages() for the benefit of
    ext4_da_writepages().  This allows us to simplify the code some, and
    will allow us to further customize the code in future patches.
    
    There are some nasty hacks in write_cache_pages(), which Linus has
    (correctly) characterized as vile.  I've just copied it into
    write_cache_pages_da(), without trying to clean those bits up lest I
    break something in the ext4's delalloc implementation, which is a bit
    fragile right now.  This will allow Dave Chinner to clean up
    write_cache_pages() in mm/page-writeback.c, without worrying about
    breaking ext4.  Eventually write_cache_pages_da() will go away when I
    rewrite ext4's delayed allocation and create a general
    ext4_writepages() which is used for all of ext4's writeback.  Until
    now this is the lowest risk way to clean up the core
    write_cache_pages() function.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;
    Cc: Dave Chinner &lt;david@fromorbit.com&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 6aa0442811d2..830336d3911b 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2426,17 +2426,6 @@ static int __mpage_da_writepage(struct page *page,
 	struct buffer_head *bh, *head;
 	sector_t logical;
 
-	if (mpd-&gt;io_done) {
-		/*
-		 * Rest of the page in the page_vec
-		 * redirty then and skip then. We will
-		 * try to write them again after
-		 * starting a new transaction
-		 */
-		redirty_page_for_writepage(wbc, page);
-		unlock_page(page);
-		return MPAGE_DA_EXTENT_TAIL;
-	}
 	/*
 	 * Can we merge this page to current extent?
 	 */
@@ -2831,6 +2820,124 @@ static int ext4_da_writepages_trans_blocks(struct inode *inode)
 	return ext4_chunk_trans_blocks(inode, max_blocks);
 }
 
+/*
+ * write_cache_pages_da - walk the list of dirty pages of the given
+ * address space and call the callback function (which usually writes
+ * the pages).
+ *
+ * This is a forked version of write_cache_pages().  Differences:
+ *	Range cyclic is ignored.
+ *	no_nrwrite_index_update is always presumed true
+ */
+static int write_cache_pages_da(struct address_space *mapping,
+				struct writeback_control *wbc,
+				struct mpage_da_data *mpd)
+{
+	int ret = 0;
+	int done = 0;
+	struct pagevec pvec;
+	int nr_pages;
+	pgoff_t index;
+	pgoff_t end;		/* Inclusive */
+	long nr_to_write = wbc-&gt;nr_to_write;
+
+	pagevec_init(&amp;pvec, 0);
+	index = wbc-&gt;range_start &gt;&gt; PAGE_CACHE_SHIFT;
+	end = wbc-&gt;range_end &gt;&gt; PAGE_CACHE_SHIFT;
+
+	while (!done &amp;&amp; (index &lt;= end)) {
+		int i;
+
+		nr_pages = pagevec_lookup_tag(&amp;pvec, mapping, &amp;index,
+			      PAGECACHE_TAG_DIRTY,
+			      min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1);
+		if (nr_pages == 0)
+			break;
+
+		for (i = 0; i &lt; nr_pages; i++) {
+			struct page *page = pvec.pages[i];
+
+			/*
+			 * At this point, the page may be truncated or
+			 * invalidated (changing page-&gt;mapping to NULL), or
+			 * even swizzled back from swapper_space to tmpfs file
+			 * mapping. However, page-&gt;index will not change
+			 * because we have a reference on the page.
+			 */
+			if (page-&gt;index &gt; end) {
+				done = 1;
+				break;
+			}
+
+			lock_page(page);
+
+			/*
+			 * Page truncated or invalidated. We can freely skip it
+			 * then, even for data integrity operations: the page
+			 * has disappeared concurrently, so there could be no
+			 * real expectation of this data interity operation
+			 * even if there is now a new, dirty page at the same
+			 * pagecache address.
+			 */
+			if (unlikely(page-&gt;mapping != mapping)) {
+continue_unlock:
+				unlock_page(page);
+				continue;
+			}
+
+			if (!PageDirty(page)) {
+				/* someone wrote it for us */
+				goto continue_unlock;
+			}
+
+			if (PageWriteback(page)) {
+				if (wbc-&gt;sync_mode != WB_SYNC_NONE)
+					wait_on_page_writeback(page);
+				else
+					goto continue_unlock;
+			}
+
+			BUG_ON(PageWriteback(page));
+			if (!clear_page_dirty_for_io(page))
+				goto continue_unlock;
+
+			ret = __mpage_da_writepage(page, wbc, mpd);
+			if (unlikely(ret)) {
+				if (ret == AOP_WRITEPAGE_ACTIVATE) {
+					unlock_page(page);
+					ret = 0;
+				} else {
+					done = 1;
+					break;
+				}
+			}
+
+			if (nr_to_write &gt; 0) {
+				nr_to_write--;
+				if (nr_to_write == 0 &amp;&amp;
+				    wbc-&gt;sync_mode == WB_SYNC_NONE) {
+					/*
+					 * We stop writing back only if we are
+					 * not doing integrity sync. In case of
+					 * integrity sync we have to keep going
+					 * because someone may be concurrently
+					 * dirtying pages, and we might have
+					 * synced a lot of newly appeared dirty
+					 * pages, but have not synced all of the
+					 * old dirty pages.
+					 */
+					done = 1;
+					break;
+				}
+			}
+		}
+		pagevec_release(&amp;pvec);
+		cond_resched();
+	}
+	return ret;
+}
+
+
 static int ext4_da_writepages(struct address_space *mapping,
 			      struct writeback_control *wbc)
 {
@@ -2839,7 +2946,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 	handle_t *handle = NULL;
 	struct mpage_da_data mpd;
 	struct inode *inode = mapping-&gt;host;
-	int no_nrwrite_index_update;
 	int pages_written = 0;
 	long pages_skipped;
 	unsigned int max_pages;
@@ -2919,12 +3025,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 	mpd.wbc = wbc;
 	mpd.inode = mapping-&gt;host;
 
-	/*
-	 * we don't want write_cache_pages to update
-	 * nr_to_write and writeback_index
-	 */
-	no_nrwrite_index_update = wbc-&gt;no_nrwrite_index_update;
-	wbc-&gt;no_nrwrite_index_update = 1;
 	pages_skipped = wbc-&gt;pages_skipped;
 
 retry:
@@ -2966,8 +3066,7 @@ static int ext4_da_writepages(struct address_space *mapping,
 		mpd.io_done = 0;
 		mpd.pages_written = 0;
 		mpd.retval = 0;
-		ret = write_cache_pages(mapping, wbc, __mpage_da_writepage,
-					&amp;mpd);
+		ret = write_cache_pages_da(mapping, wbc, &amp;mpd);
 		/*
 		 * If we have a contiguous extent of pages and we
 		 * haven't done the I/O yet, map the blocks and submit
@@ -3033,8 +3132,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 		mapping-&gt;writeback_index = index;
 
 out_writepages:
-	if (!no_nrwrite_index_update)
-		wbc-&gt;no_nrwrite_index_update = 0;
 	wbc-&gt;nr_to_write -= nr_to_writebump;
 	wbc-&gt;range_start = range_start;
 	trace_ext4_da_writepages_result(inode, wbc, ret, pages_written);</pre><hr><pre>commit c35a56a090eacefca07afeb994029b57d8dd8025
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun May 16 05:00:00 2010 -0400

    jbd2: Improve scalability by not taking j_state_lock in jbd2_journal_stop()
    
    One of the most contended locks in the jbd2 layer is j_state_lock when
    running dbench.  This is especially true if using the real-time kernel
    with its "sleeping spinlocks" patch that replaces spinlocks with
    priority inheriting mutexes --- but it also shows up on large SMP
    benchmarks.
    
    Thanks to John Stultz for pointing this out.
    
    Reviewed by Mingming Cao and Jan Kara.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/jbd2/transaction.c b/fs/jbd2/transaction.c
index bfc70f57900f..e214d68620ac 100644
--- a/fs/jbd2/transaction.c
+++ b/fs/jbd2/transaction.c
@@ -1311,7 +1311,6 @@ int jbd2_journal_stop(handle_t *handle)
 	if (handle-&gt;h_sync)
 		transaction-&gt;t_synchronous_commit = 1;
 	current-&gt;journal_info = NULL;
-	spin_lock(&amp;journal-&gt;j_state_lock);
 	spin_lock(&amp;transaction-&gt;t_handle_lock);
 	transaction-&gt;t_outstanding_credits -= handle-&gt;h_buffer_credits;
 	transaction-&gt;t_updates--;
@@ -1340,8 +1339,7 @@ int jbd2_journal_stop(handle_t *handle)
 		jbd_debug(2, "transaction too old, requesting commit for "
 					"handle %p\n", handle);
 		/* This is non-blocking */
-		__jbd2_log_start_commit(journal, transaction-&gt;t_tid);
-		spin_unlock(&amp;journal-&gt;j_state_lock);
+		jbd2_log_start_commit(journal, transaction-&gt;t_tid);
 
 		/*
 		 * Special case: JBD2_SYNC synchronous updates require us
@@ -1351,7 +1349,6 @@ int jbd2_journal_stop(handle_t *handle)
 			err = jbd2_log_wait_commit(journal, tid);
 	} else {
 		spin_unlock(&amp;transaction-&gt;t_handle_lock);
-		spin_unlock(&amp;journal-&gt;j_state_lock);
 	}
 
 	lock_map_release(&amp;handle-&gt;h_lockdep_map);</pre><hr><pre>commit b90f687018e6d6c77d981b09203780f7001407e5
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Tue Apr 20 16:51:59 2010 -0400

    ext4: Issue the discard operation *before* releasing the blocks to be reused
    
    Otherwise, we can end up having data corruption because the blocks
    could get reused and then discarded!
    
    https://bugzilla.kernel.org/show_bug.cgi?id=15579
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 54df209d2eed..e5ab41b559c0 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2534,6 +2534,17 @@ static void release_blocks_on_commit(journal_t *journal, transaction_t *txn)
 		mb_debug(1, "gonna free %u blocks in group %u (0x%p):",
 			 entry-&gt;count, entry-&gt;group, entry);
 
+		if (test_opt(sb, DISCARD)) {
+			ext4_fsblk_t discard_block;
+
+			discard_block = entry-&gt;start_blk +
+				ext4_group_first_block_no(sb, entry-&gt;group);
+			trace_ext4_discard_blocks(sb,
+					(unsigned long long)discard_block,
+					entry-&gt;count);
+			sb_issue_discard(sb, discard_block, entry-&gt;count);
+		}
+
 		err = ext4_mb_load_buddy(sb, entry-&gt;group, &amp;e4b);
 		/* we expect to find existing buddy because it's pinned */
 		BUG_ON(err != 0);
@@ -2555,16 +2566,6 @@ static void release_blocks_on_commit(journal_t *journal, transaction_t *txn)
 			page_cache_release(e4b.bd_bitmap_page);
 		}
 		ext4_unlock_group(sb, entry-&gt;group);
-		if (test_opt(sb, DISCARD)) {
-			ext4_fsblk_t discard_block;
-
-			discard_block = entry-&gt;start_blk +
-				ext4_group_first_block_no(sb, entry-&gt;group);
-			trace_ext4_discard_blocks(sb,
-					(unsigned long long)discard_block,
-					entry-&gt;count);
-			sb_issue_discard(sb, discard_block, entry-&gt;count);
-		}
 		kmem_cache_free(ext4_free_ext_cachep, entry);
 		ext4_mb_release_desc(&amp;e4b);
 	}</pre><hr><pre>commit 37f328eb60a94779dd020084209fc4db2d6444a0
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Mar 24 20:06:41 2010 -0400

    ext4: Fix spelling of CONTIG_FS_EXT3 to CONFIG_FS_EXT3
    
    Oops.  (Blush.)
    
    Thanks to Sedat Dilek for pointing this out.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index ce84a6ed4a48..f4b038f51f31 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -4068,7 +4068,7 @@ static int ext4_get_sb(struct file_system_type *fs_type, int flags,
 	return get_sb_bdev(fs_type, flags, dev_name, data, ext4_fill_super,mnt);
 }
 
-#if !defined(CONTIG_EXT2_FS) &amp;&amp; !defined(CONFIG_EXT2_FS_MODULE) &amp;&amp; defined(CONFIG_EXT4_USE_FOR_EXT23)
+#if !defined(CONFIG_EXT2_FS) &amp;&amp; !defined(CONFIG_EXT2_FS_MODULE) &amp;&amp; defined(CONFIG_EXT4_USE_FOR_EXT23)
 static struct file_system_type ext2_fs_type = {
 	.owner		= THIS_MODULE,
 	.name		= "ext2",
@@ -4095,7 +4095,7 @@ static inline void register_as_ext2(void) { }
 static inline void unregister_as_ext2(void) { }
 #endif
 
-#if !defined(CONTIG_EXT3_FS) &amp;&amp; !defined(CONFIG_EXT3_FS_MODULE) &amp;&amp; defined(CONFIG_EXT4_USE_FOR_EXT23)
+#if !defined(CONFIG_EXT3_FS) &amp;&amp; !defined(CONFIG_EXT3_FS_MODULE) &amp;&amp; defined(CONFIG_EXT4_USE_FOR_EXT23)
 static struct file_system_type ext3_fs_type = {
 	.owner		= THIS_MODULE,
 	.name		= "ext3",</pre><hr><pre>commit f710b4b96ba292dfed2153afc47e9063b0abfd89
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Jan 25 03:31:32 2010 -0500

    ext4: Reserve INCOMPAT_EA_INODE and INCOMPAT_DIRDATA feature codepoints
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index ffe1334c891e..61cf3b3cde4e 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -284,6 +284,7 @@ struct flex_groups {
 #define EXT4_TOPDIR_FL			0x00020000 /* Top of directory hierarchies*/
 #define EXT4_HUGE_FILE_FL               0x00040000 /* Set to each huge file */
 #define EXT4_EXTENTS_FL			0x00080000 /* Inode uses extents */
+#define EXT4_EA_INODE_FL	        0x00200000 /* Inode used for large EA */
 #define EXT4_RESERVED_FL		0x80000000 /* reserved for ext4 lib */
 
 #define EXT4_FL_USER_VISIBLE		0x000BDFFF /* User visible flags */
@@ -1144,6 +1145,8 @@ static inline void ext4_clear_inode_state(struct inode *inode, int bit)
 #define EXT4_FEATURE_INCOMPAT_64BIT		0x0080
 #define EXT4_FEATURE_INCOMPAT_MMP               0x0100
 #define EXT4_FEATURE_INCOMPAT_FLEX_BG		0x0200
+#define EXT4_FEATURE_INCOMPAT_EA_INODE		0x0400 /* EA in inode */
+#define EXT4_FEATURE_INCOMPAT_DIRDATA		0x1000 /* data in dirent */
 
 #define EXT4_FEATURE_COMPAT_SUPP	EXT2_FEATURE_COMPAT_EXT_ATTR
 #define EXT4_FEATURE_INCOMPAT_SUPP	(EXT4_FEATURE_INCOMPAT_FILETYPE| \</pre><hr><pre>commit 19f5fb7ad679bb361222c7916086435020c37cce
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun Jan 24 14:34:07 2010 -0500

    ext4: Use bitops to read/modify EXT4_I(inode)-&gt;i_state
    
    At several places we modify EXT4_I(inode)-&gt;i_state without holding
    i_mutex (ext4_release_file, ext4_bmap, ext4_journalled_writepage,
    ext4_do_update_inode, ...). These modifications are racy and we can
    lose updates to i_state. So convert handling of i_state to use bitops
    which are atomic.
    
    Cc: Jan Kara &lt;jack@suse.cz&gt;
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 307ecd13a762..ffe1334c891e 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -313,17 +313,6 @@ static inline __u32 ext4_mask_flags(umode_t mode, __u32 flags)
 		return flags &amp; EXT4_OTHER_FLMASK;
 }
 
-/*
- * Inode dynamic state flags
- */
-#define EXT4_STATE_JDATA		0x00000001 /* journaled data exists */
-#define EXT4_STATE_NEW			0x00000002 /* inode is newly created */
-#define EXT4_STATE_XATTR		0x00000004 /* has in-inode xattrs */
-#define EXT4_STATE_NO_EXPAND		0x00000008 /* No space for expansion */
-#define EXT4_STATE_DA_ALLOC_CLOSE	0x00000010 /* Alloc DA blks on close */
-#define EXT4_STATE_EXT_MIGRATE		0x00000020 /* Inode is migrating */
-#define EXT4_STATE_DIO_UNWRITTEN	0x00000040 /* need convert on dio done*/
-
 /* Used to pass group descriptor data when online resize is done */
 struct ext4_new_group_input {
 	__u32 group;		/* Group number for this data */
@@ -631,7 +620,7 @@ struct ext4_inode_info {
 	 * near to their parent directory's inode.
 	 */
 	ext4_group_t	i_block_group;
-	__u32	i_state;		/* Dynamic state flags for ext4 */
+	unsigned long	i_state_flags;		/* Dynamic state flags */
 
 	ext4_lblk_t		i_dir_start_lookup;
 #ifdef CONFIG_EXT4_FS_XATTR
@@ -1051,6 +1040,34 @@ static inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)
 		(ino &gt;= EXT4_FIRST_INO(sb) &amp;&amp;
 		 ino &lt;= le32_to_cpu(EXT4_SB(sb)-&gt;s_es-&gt;s_inodes_count));
 }
+
+/*
+ * Inode dynamic state flags
+ */
+enum {
+	EXT4_STATE_JDATA,		/* journaled data exists */
+	EXT4_STATE_NEW,			/* inode is newly created */
+	EXT4_STATE_XATTR,		/* has in-inode xattrs */
+	EXT4_STATE_NO_EXPAND,		/* No space for expansion */
+	EXT4_STATE_DA_ALLOC_CLOSE,	/* Alloc DA blks on close */
+	EXT4_STATE_EXT_MIGRATE,		/* Inode is migrating */
+	EXT4_STATE_DIO_UNWRITTEN,	/* need convert on dio done*/
+};
+
+static inline int ext4_test_inode_state(struct inode *inode, int bit)
+{
+	return test_bit(bit, &amp;EXT4_I(inode)-&gt;i_state_flags);
+}
+
+static inline void ext4_set_inode_state(struct inode *inode, int bit)
+{
+	set_bit(bit, &amp;EXT4_I(inode)-&gt;i_state_flags);
+}
+
+static inline void ext4_clear_inode_state(struct inode *inode, int bit)
+{
+	clear_bit(bit, &amp;EXT4_I(inode)-&gt;i_state_flags);
+}
 #else
 /* Assume that user mode programs are passing in an ext4fs superblock, not
  * a kernel struct super_block.  This will allow us to call the feature-test
diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index c56877972b0e..54616157c0f3 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -3076,7 +3076,7 @@ ext4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,
 		if (io)
 			io-&gt;flag = DIO_AIO_UNWRITTEN;
 		else
-			EXT4_I(inode)-&gt;i_state |= EXT4_STATE_DIO_UNWRITTEN;
+			ext4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);
 		goto out;
 	}
 	/* async DIO end_io complete, convert the filled extent to written */
@@ -3362,8 +3362,8 @@ int ext4_ext_get_blocks(handle_t *handle, struct inode *inode,
 			if (io)
 				io-&gt;flag = DIO_AIO_UNWRITTEN;
 			else
-				EXT4_I(inode)-&gt;i_state |=
-					EXT4_STATE_DIO_UNWRITTEN;;
+				ext4_set_inode_state(inode,
+						     EXT4_STATE_DIO_UNWRITTEN);
 		}
 	}
 	err = ext4_ext_insert_extent(handle, inode, path, &amp;newex, flags);
@@ -3739,7 +3739,7 @@ static int ext4_xattr_fiemap(struct inode *inode,
 	int error = 0;
 
 	/* in-inode? */
-	if (EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_XATTR) {
+	if (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {
 		struct ext4_iloc iloc;
 		int offset;	/* offset of xattr in inode */
 
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index 9630583cef28..f6071ce0b155 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -35,9 +35,9 @@
  */
 static int ext4_release_file(struct inode *inode, struct file *filp)
 {
-	if (EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_DA_ALLOC_CLOSE) {
+	if (ext4_test_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE)) {
 		ext4_alloc_da_blocks(inode);
-		EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_DA_ALLOC_CLOSE;
+		ext4_clear_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE);
 	}
 	/* if we are the last writer on the inode, drop the block reservation */
 	if ((filp-&gt;f_mode &amp; FMODE_WRITE) &amp;&amp;
diff --git a/fs/ext4/ialloc.c b/fs/ext4/ialloc.c
index f3624ead4f6c..2fab5adae1e2 100644
--- a/fs/ext4/ialloc.c
+++ b/fs/ext4/ialloc.c
@@ -1029,7 +1029,8 @@ struct inode *ext4_new_inode(handle_t *handle, struct inode *dir, int mode,
 	inode-&gt;i_generation = sbi-&gt;s_next_generation++;
 	spin_unlock(&amp;sbi-&gt;s_next_gen_lock);
 
-	ei-&gt;i_state = EXT4_STATE_NEW;
+	ei-&gt;i_state_flags = 0;
+	ext4_set_inode_state(inode, EXT4_STATE_NEW);
 
 	ei-&gt;i_extra_isize = EXT4_SB(sb)-&gt;s_want_extra_isize;
 
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 1a3d7b232cd7..3e530119d7f0 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1307,7 +1307,7 @@ int ext4_get_blocks(handle_t *handle, struct inode *inode, sector_t block,
 			 * i_data's format changing.  Force the migrate
 			 * to fail by clearing migrate flags
 			 */
-			EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_EXT_MIGRATE;
+			ext4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);
 		}
 
 		/*
@@ -1794,7 +1794,7 @@ static int ext4_journalled_write_end(struct file *file,
 	new_i_size = pos + copied;
 	if (new_i_size &gt; inode-&gt;i_size)
 		i_size_write(inode, pos+copied);
-	EXT4_I(inode)-&gt;i_state |= EXT4_STATE_JDATA;
+	ext4_set_inode_state(inode, EXT4_STATE_JDATA);
 	if (new_i_size &gt; EXT4_I(inode)-&gt;i_disksize) {
 		ext4_update_i_disksize(inode, new_i_size);
 		ret2 = ext4_mark_inode_dirty(handle, inode);
@@ -2632,7 +2632,7 @@ static int __ext4_journalled_writepage(struct page *page,
 		ret = err;
 
 	walk_page_buffers(handle, page_bufs, 0, len, NULL, bput_one);
-	EXT4_I(inode)-&gt;i_state |= EXT4_STATE_JDATA;
+	ext4_set_inode_state(inode, EXT4_STATE_JDATA);
 out:
 	return ret;
 }
@@ -3303,7 +3303,8 @@ static sector_t ext4_bmap(struct address_space *mapping, sector_t block)
 		filemap_write_and_wait(mapping);
 	}
 
-	if (EXT4_JOURNAL(inode) &amp;&amp; EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_JDATA) {
+	if (EXT4_JOURNAL(inode) &amp;&amp;
+	    ext4_test_inode_state(inode, EXT4_STATE_JDATA)) {
 		/*
 		 * This is a REALLY heavyweight approach, but the use of
 		 * bmap on dirty files is expected to be extremely rare:
@@ -3322,7 +3323,7 @@ static sector_t ext4_bmap(struct address_space *mapping, sector_t block)
 		 * everything they get.
 		 */
 
-		EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_JDATA;
+		ext4_clear_inode_state(inode, EXT4_STATE_JDATA);
 		journal = EXT4_JOURNAL(inode);
 		jbd2_journal_lock_updates(journal);
 		err = jbd2_journal_flush(journal);
@@ -3790,8 +3791,8 @@ static ssize_t ext4_ext_direct_IO(int rw, struct kiocb *iocb,
 		if (ret != -EIOCBQUEUED &amp;&amp; ret &lt;= 0 &amp;&amp; iocb-&gt;private) {
 			ext4_free_io_end(iocb-&gt;private);
 			iocb-&gt;private = NULL;
-		} else if (ret &gt; 0 &amp;&amp; (EXT4_I(inode)-&gt;i_state &amp;
-				       EXT4_STATE_DIO_UNWRITTEN)) {
+		} else if (ret &gt; 0 &amp;&amp; ext4_test_inode_state(inode,
+						EXT4_STATE_DIO_UNWRITTEN)) {
 			int err;
 			/*
 			 * for non AIO case, since the IO is already
@@ -3801,7 +3802,7 @@ static ssize_t ext4_ext_direct_IO(int rw, struct kiocb *iocb,
 							     offset, ret);
 			if (err &lt; 0)
 				ret = err;
-			EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_DIO_UNWRITTEN;
+			ext4_clear_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);
 		}
 		return ret;
 	}
@@ -4457,7 +4458,7 @@ void ext4_truncate(struct inode *inode)
 		return;
 
 	if (inode-&gt;i_size == 0 &amp;&amp; !test_opt(inode-&gt;i_sb, NO_AUTO_DA_ALLOC))
-		ei-&gt;i_state |= EXT4_STATE_DA_ALLOC_CLOSE;
+		ext4_set_inode_state(inode, EXT4_STATE_DA_ALLOC_CLOSE);
 
 	if (EXT4_I(inode)-&gt;i_flags &amp; EXT4_EXTENTS_FL) {
 		ext4_ext_truncate(inode);
@@ -4743,7 +4744,7 @@ int ext4_get_inode_loc(struct inode *inode, struct ext4_iloc *iloc)
 {
 	/* We have all inode data except xattrs in memory here. */
 	return __ext4_get_inode_loc(inode, iloc,
-		!(EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_XATTR));
+		!ext4_test_inode_state(inode, EXT4_STATE_XATTR));
 }
 
 void ext4_set_inode_flags(struct inode *inode)
@@ -4837,7 +4838,7 @@ struct inode *ext4_iget(struct super_block *sb, unsigned long ino)
 	}
 	inode-&gt;i_nlink = le16_to_cpu(raw_inode-&gt;i_links_count);
 
-	ei-&gt;i_state = 0;
+	ei-&gt;i_state_flags = 0;
 	ei-&gt;i_dir_start_lookup = 0;
 	ei-&gt;i_dtime = le32_to_cpu(raw_inode-&gt;i_dtime);
 	/* We now have enough fields to check if the inode was active or not.
@@ -4920,7 +4921,7 @@ struct inode *ext4_iget(struct super_block *sb, unsigned long ino)
 					EXT4_GOOD_OLD_INODE_SIZE +
 					ei-&gt;i_extra_isize;
 			if (*magic == cpu_to_le32(EXT4_XATTR_MAGIC))
-				ei-&gt;i_state |= EXT4_STATE_XATTR;
+				ext4_set_inode_state(inode, EXT4_STATE_XATTR);
 		}
 	} else
 		ei-&gt;i_extra_isize = 0;
@@ -5060,7 +5061,7 @@ static int ext4_do_update_inode(handle_t *handle,
 
 	/* For fields not not tracking in the in-memory inode,
 	 * initialise them to zero for new inodes. */
-	if (ei-&gt;i_state &amp; EXT4_STATE_NEW)
+	if (ext4_test_inode_state(inode, EXT4_STATE_NEW))
 		memset(raw_inode, 0, EXT4_SB(inode-&gt;i_sb)-&gt;s_inode_size);
 
 	ext4_get_inode_flags(ei);
@@ -5156,7 +5157,7 @@ static int ext4_do_update_inode(handle_t *handle,
 	rc = ext4_handle_dirty_metadata(handle, inode, bh);
 	if (!err)
 		err = rc;
-	ei-&gt;i_state &amp;= ~EXT4_STATE_NEW;
+	ext4_clear_inode_state(inode, EXT4_STATE_NEW);
 
 	ext4_update_inode_fsync_trans(handle, inode, 0);
 out_brelse:
@@ -5580,8 +5581,8 @@ static int ext4_expand_extra_isize(struct inode *inode,
 	entry = IFIRST(header);
 
 	/* No extended attributes present */
-	if (!(EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_XATTR) ||
-		header-&gt;h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {
+	if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR) ||
+	    header-&gt;h_magic != cpu_to_le32(EXT4_XATTR_MAGIC)) {
 		memset((void *)raw_inode + EXT4_GOOD_OLD_INODE_SIZE, 0,
 			new_extra_isize);
 		EXT4_I(inode)-&gt;i_extra_isize = new_extra_isize;
@@ -5625,7 +5626,7 @@ int ext4_mark_inode_dirty(handle_t *handle, struct inode *inode)
 	err = ext4_reserve_inode_write(handle, inode, &amp;iloc);
 	if (ext4_handle_valid(handle) &amp;&amp;
 	    EXT4_I(inode)-&gt;i_extra_isize &lt; sbi-&gt;s_want_extra_isize &amp;&amp;
-	    !(EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_NO_EXPAND)) {
+	    !ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND)) {
 		/*
 		 * We need extra buffer credits since we may write into EA block
 		 * with this same handle. If journal_extend fails, then it will
@@ -5639,7 +5640,8 @@ int ext4_mark_inode_dirty(handle_t *handle, struct inode *inode)
 						      sbi-&gt;s_want_extra_isize,
 						      iloc, handle);
 			if (ret) {
-				EXT4_I(inode)-&gt;i_state |= EXT4_STATE_NO_EXPAND;
+				ext4_set_inode_state(inode,
+						     EXT4_STATE_NO_EXPAND);
 				if (mnt_count !=
 					le16_to_cpu(sbi-&gt;s_es-&gt;s_mnt_count)) {
 					ext4_warning(inode-&gt;i_sb, __func__,
diff --git a/fs/ext4/migrate.c b/fs/ext4/migrate.c
index 81415814b00b..46a4101e0aec 100644
--- a/fs/ext4/migrate.c
+++ b/fs/ext4/migrate.c
@@ -365,12 +365,12 @@ static int ext4_ext_swap_inode_data(handle_t *handle, struct inode *inode,
 	 * happened after we started the migrate. We need to
 	 * fail the migrate
 	 */
-	if (!(EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_EXT_MIGRATE)) {
+	if (!ext4_test_inode_state(inode, EXT4_STATE_EXT_MIGRATE)) {
 		retval = -EAGAIN;
 		up_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
 		goto err_out;
 	} else
-		EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_EXT_MIGRATE;
+		ext4_clear_inode_state(inode, EXT4_STATE_EXT_MIGRATE);
 	/*
 	 * We have the extent map build with the tmp inode.
 	 * Now copy the i_data across
@@ -533,7 +533,7 @@ int ext4_ext_migrate(struct inode *inode)
 	 * allocation.
 	 */
 	down_read((&amp;EXT4_I(inode)-&gt;i_data_sem));
-	EXT4_I(inode)-&gt;i_state |= EXT4_STATE_EXT_MIGRATE;
+	ext4_set_inode_state(inode, EXT4_STATE_EXT_MIGRATE);
 	up_read((&amp;EXT4_I(inode)-&gt;i_data_sem));
 
 	handle = ext4_journal_start(inode, 1);
diff --git a/fs/ext4/xattr.c b/fs/ext4/xattr.c
index f3a2f7ed45aa..c619a7ea670d 100644
--- a/fs/ext4/xattr.c
+++ b/fs/ext4/xattr.c
@@ -267,7 +267,7 @@ ext4_xattr_ibody_get(struct inode *inode, int name_index, const char *name,
 	void *end;
 	int error;
 
-	if (!(EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_XATTR))
+	if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR))
 		return -ENODATA;
 	error = ext4_get_inode_loc(inode, &amp;iloc);
 	if (error)
@@ -396,7 +396,7 @@ ext4_xattr_ibody_list(struct dentry *dentry, char *buffer, size_t buffer_size)
 	void *end;
 	int error;
 
-	if (!(EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_XATTR))
+	if (!ext4_test_inode_state(inode, EXT4_STATE_XATTR))
 		return 0;
 	error = ext4_get_inode_loc(inode, &amp;iloc);
 	if (error)
@@ -908,7 +908,7 @@ ext4_xattr_ibody_find(struct inode *inode, struct ext4_xattr_info *i,
 	is-&gt;s.base = is-&gt;s.first = IFIRST(header);
 	is-&gt;s.here = is-&gt;s.first;
 	is-&gt;s.end = (void *)raw_inode + EXT4_SB(inode-&gt;i_sb)-&gt;s_inode_size;
-	if (EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_XATTR) {
+	if (ext4_test_inode_state(inode, EXT4_STATE_XATTR)) {
 		error = ext4_xattr_check_names(IFIRST(header), is-&gt;s.end);
 		if (error)
 			return error;
@@ -940,10 +940,10 @@ ext4_xattr_ibody_set(handle_t *handle, struct inode *inode,
 	header = IHDR(inode, ext4_raw_inode(&amp;is-&gt;iloc));
 	if (!IS_LAST_ENTRY(s-&gt;first)) {
 		header-&gt;h_magic = cpu_to_le32(EXT4_XATTR_MAGIC);
-		EXT4_I(inode)-&gt;i_state |= EXT4_STATE_XATTR;
+		ext4_set_inode_state(inode, EXT4_STATE_XATTR);
 	} else {
 		header-&gt;h_magic = cpu_to_le32(0);
-		EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_XATTR;
+		ext4_clear_inode_state(inode, EXT4_STATE_XATTR);
 	}
 	return 0;
 }
@@ -986,8 +986,8 @@ ext4_xattr_set_handle(handle_t *handle, struct inode *inode, int name_index,
 	if (strlen(name) &gt; 255)
 		return -ERANGE;
 	down_write(&amp;EXT4_I(inode)-&gt;xattr_sem);
-	no_expand = EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_NO_EXPAND;
-	EXT4_I(inode)-&gt;i_state |= EXT4_STATE_NO_EXPAND;
+	no_expand = ext4_test_inode_state(inode, EXT4_STATE_NO_EXPAND);
+	ext4_set_inode_state(inode, EXT4_STATE_NO_EXPAND);
 
 	error = ext4_get_inode_loc(inode, &amp;is.iloc);
 	if (error)
@@ -997,10 +997,10 @@ ext4_xattr_set_handle(handle_t *handle, struct inode *inode, int name_index,
 	if (error)
 		goto cleanup;
 
-	if (EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_NEW) {
+	if (ext4_test_inode_state(inode, EXT4_STATE_NEW)) {
 		struct ext4_inode *raw_inode = ext4_raw_inode(&amp;is.iloc);
 		memset(raw_inode, 0, EXT4_SB(inode-&gt;i_sb)-&gt;s_inode_size);
-		EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_NEW;
+		ext4_clear_inode_state(inode, EXT4_STATE_NEW);
 	}
 
 	error = ext4_xattr_ibody_find(inode, &amp;i, &amp;is);
@@ -1052,7 +1052,7 @@ ext4_xattr_set_handle(handle_t *handle, struct inode *inode, int name_index,
 		ext4_xattr_update_super_block(handle, inode-&gt;i_sb);
 		inode-&gt;i_ctime = ext4_current_time(inode);
 		if (!value)
-			EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_NO_EXPAND;
+			ext4_clear_inode_state(inode, EXT4_STATE_NO_EXPAND);
 		error = ext4_mark_iloc_dirty(handle, inode, &amp;is.iloc);
 		/*
 		 * The bh is consumed by ext4_mark_iloc_dirty, even with
@@ -1067,7 +1067,7 @@ ext4_xattr_set_handle(handle_t *handle, struct inode *inode, int name_index,
 	brelse(is.iloc.bh);
 	brelse(bs.bh);
 	if (no_expand == 0)
-		EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_NO_EXPAND;
+		ext4_clear_inode_state(inode, EXT4_STATE_NO_EXPAND);
 	up_write(&amp;EXT4_I(inode)-&gt;xattr_sem);
 	return error;
 }</pre><hr><pre>commit 1d3c36ad4122651018599d4e3c9be0cccfbfb939
Author: Andrew Lutomirski &lt;luto@mit.edu&gt;
Date:   Mon Dec 21 10:10:22 2009 -0500

    drm/i915: Fix RC6 suspend/resume
    
    We restored RC6 twice on resume, even with modesetting off.  Instead,
    only restore it once and skip RC6 initialization entirely in non-KMS mode.
    
    Signed-off-by: Andy Lutomirski &lt;luto@mit.edu&gt;
    Tested-by: Jeff Chua &lt;jeff.chua.linux@gmail.com&gt;
    Signed-off-by: Eric Anholt &lt;eric@anholt.net&gt;

diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 7eb4ad51034d..29dd67626967 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -307,8 +307,6 @@ typedef struct drm_i915_private {
 	u32 saveDSPACNTR;
 	u32 saveDSPBCNTR;
 	u32 saveDSPARB;
-	u32 saveRENDERSTANDBY;
-	u32 savePWRCTXA;
 	u32 saveHWS;
 	u32 savePIPEACONF;
 	u32 savePIPEBCONF;
diff --git a/drivers/gpu/drm/i915/i915_suspend.c b/drivers/gpu/drm/i915/i915_suspend.c
index d5ebb00a9d49..a3b90c9561dc 100644
--- a/drivers/gpu/drm/i915/i915_suspend.c
+++ b/drivers/gpu/drm/i915/i915_suspend.c
@@ -732,12 +732,6 @@ int i915_save_state(struct drm_device *dev)
 
 	pci_read_config_byte(dev-&gt;pdev, LBB, &amp;dev_priv-&gt;saveLBB);
 
-	/* Render Standby */
-	if (I915_HAS_RC6(dev)) {
-		dev_priv-&gt;saveRENDERSTANDBY = I915_READ(MCHBAR_RENDER_STANDBY);
-		dev_priv-&gt;savePWRCTXA = I915_READ(PWRCTXA);
-	}
-
 	/* Hardware status page */
 	dev_priv-&gt;saveHWS = I915_READ(HWS_PGA);
 
@@ -793,12 +787,6 @@ int i915_restore_state(struct drm_device *dev)
 
 	pci_write_config_byte(dev-&gt;pdev, LBB, dev_priv-&gt;saveLBB);
 
-	/* Render Standby */
-	if (I915_HAS_RC6(dev)) {
-		I915_WRITE(MCHBAR_RENDER_STANDBY, dev_priv-&gt;saveRENDERSTANDBY);
-		I915_WRITE(PWRCTXA, dev_priv-&gt;savePWRCTXA);
-	}
-
 	/* Hardware status page */
 	I915_WRITE(HWS_PGA, dev_priv-&gt;saveHWS);
 
diff --git a/drivers/gpu/drm/i915/intel_display.c b/drivers/gpu/drm/i915/intel_display.c
index c21dede6461c..089b1df5448b 100644
--- a/drivers/gpu/drm/i915/intel_display.c
+++ b/drivers/gpu/drm/i915/intel_display.c
@@ -4593,7 +4593,7 @@ void intel_init_clock_gating(struct drm_device *dev)
 	 * GPU can automatically power down the render unit if given a page
 	 * to save state.
 	 */
-	if (I915_HAS_RC6(dev)) {
+	if (I915_HAS_RC6(dev) &amp;&amp; drm_core_check_feature(dev, DRIVER_MODESET)) {
 		struct drm_gem_object *pwrctx;
 		struct drm_i915_gem_object *obj_priv;
 		int ret;</pre><hr><pre>commit 9d0be50230b333005635967f7ecd4897dbfd181b
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Jan 1 02:41:30 2010 -0500

    ext4: Calculate metadata requirements more accurately
    
    In the past, ext4_calc_metadata_amount(), and its sub-functions
    ext4_ext_calc_metadata_amount() and ext4_indirect_calc_metadata_amount()
    badly over-estimated the number of metadata blocks that might be
    required for delayed allocation blocks.  This didn't matter as much
    when functions which managed the reserved metadata blocks were more
    aggressive about dropping reserved metadata blocks as delayed
    allocation blocks were written, but unfortunately they were too
    aggressive.  This was fixed in commit 0637c6f, but as a result the
    over-estimation by ext4_calc_metadata_amount() would lead to reserving
    2-3 times the number of pending delayed allocation blocks as
    potentially required metadata blocks.  So if there are 1 megabytes of
    blocks which have been not yet been allocation, up to 3 megabytes of
    space would get reserved out of the user's quota and from the file
    system free space pool until all of the inode's data blocks have been
    allocated.
    
    This commit addresses this problem by much more accurately estimating
    the number of metadata blocks that will be required.  It will still
    somewhat over-estimate the number of blocks needed, since it must make
    a worst case estimate not knowing which physical blocks will be
    needed, but it is much more accurate than before.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 56f9271ee8cc..af7b62699ea9 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -699,6 +699,8 @@ struct ext4_inode_info {
 	unsigned int i_reserved_meta_blocks;
 	unsigned int i_allocated_meta_blocks;
 	unsigned short i_delalloc_reserved_flag;
+	sector_t i_da_metadata_calc_last_lblock;
+	int i_da_metadata_calc_len;
 
 	/* on-disk additional length */
 	__u16 i_extra_isize;
diff --git a/fs/ext4/ext4_extents.h b/fs/ext4/ext4_extents.h
index 2ca686454e87..bdb6ce7e2eb4 100644
--- a/fs/ext4/ext4_extents.h
+++ b/fs/ext4/ext4_extents.h
@@ -225,7 +225,8 @@ static inline void ext4_ext_mark_initialized(struct ext4_extent *ext)
 	ext-&gt;ee_len = cpu_to_le16(ext4_ext_get_actual_len(ext));
 }
 
-extern int ext4_ext_calc_metadata_amount(struct inode *inode, int blocks);
+extern int ext4_ext_calc_metadata_amount(struct inode *inode,
+					 sector_t lblocks);
 extern ext4_fsblk_t ext_pblock(struct ext4_extent *ex);
 extern ext4_fsblk_t idx_pblock(struct ext4_extent_idx *);
 extern void ext4_ext_store_pblock(struct ext4_extent *, ext4_fsblk_t);
diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index 91ae46098ea4..7d7b74e94687 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -296,29 +296,44 @@ static inline int ext4_ext_space_root_idx(struct inode *inode, int check)
  * to allocate @blocks
  * Worse case is one block per extent
  */
-int ext4_ext_calc_metadata_amount(struct inode *inode, int blocks)
+int ext4_ext_calc_metadata_amount(struct inode *inode, sector_t lblock)
 {
-	int lcap, icap, rcap, leafs, idxs, num;
-	int newextents = blocks;
-
-	rcap = ext4_ext_space_root_idx(inode, 0);
-	lcap = ext4_ext_space_block(inode, 0);
-	icap = ext4_ext_space_block_idx(inode, 0);
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	int idxs, num = 0;
 
-	/* number of new leaf blocks needed */
-	num = leafs = (newextents + lcap - 1) / lcap;
+	idxs = ((inode-&gt;i_sb-&gt;s_blocksize - sizeof(struct ext4_extent_header))
+		/ sizeof(struct ext4_extent_idx));
 
 	/*
-	 * Worse case, we need separate index block(s)
-	 * to link all new leaf blocks
+	 * If the new delayed allocation block is contiguous with the
+	 * previous da block, it can share index blocks with the
+	 * previous block, so we only need to allocate a new index
+	 * block every idxs leaf blocks.  At ldxs**2 blocks, we need
+	 * an additional index block, and at ldxs**3 blocks, yet
+	 * another index blocks.
 	 */
-	idxs = (leafs + icap - 1) / icap;
-	do {
-		num += idxs;
-		idxs = (idxs + icap - 1) / icap;
-	} while (idxs &gt; rcap);
+	if (ei-&gt;i_da_metadata_calc_len &amp;&amp;
+	    ei-&gt;i_da_metadata_calc_last_lblock+1 == lblock) {
+		if ((ei-&gt;i_da_metadata_calc_len % idxs) == 0)
+			num++;
+		if ((ei-&gt;i_da_metadata_calc_len % (idxs*idxs)) == 0)
+			num++;
+		if ((ei-&gt;i_da_metadata_calc_len % (idxs*idxs*idxs)) == 0) {
+			num++;
+			ei-&gt;i_da_metadata_calc_len = 0;
+		} else
+			ei-&gt;i_da_metadata_calc_len++;
+		ei-&gt;i_da_metadata_calc_last_lblock++;
+		return num;
+	}
 
-	return num;
+	/*
+	 * In the worst case we need a new set of index blocks at
+	 * every level of the inode's extent tree.
+	 */
+	ei-&gt;i_da_metadata_calc_len = 1;
+	ei-&gt;i_da_metadata_calc_last_lblock = lblock;
+	return ext_depth(inode) + 1;
 }
 
 static int
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index bdaa92a29e0e..c818972c8302 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1009,38 +1009,44 @@ qsize_t *ext4_get_reserved_space(struct inode *inode)
 	return &amp;EXT4_I(inode)-&gt;i_reserved_quota;
 }
 #endif
+
 /*
  * Calculate the number of metadata blocks need to reserve
- * to allocate @blocks for non extent file based file
+ * to allocate a new block at @lblocks for non extent file based file
  */
-static int ext4_indirect_calc_metadata_amount(struct inode *inode, int blocks)
+static int ext4_indirect_calc_metadata_amount(struct inode *inode,
+					      sector_t lblock)
 {
-	int icap = EXT4_ADDR_PER_BLOCK(inode-&gt;i_sb);
-	int ind_blks, dind_blks, tind_blks;
-
-	/* number of new indirect blocks needed */
-	ind_blks = (blocks + icap - 1) / icap;
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	int dind_mask = EXT4_ADDR_PER_BLOCK(inode-&gt;i_sb) - 1;
+	int blk_bits;
 
-	dind_blks = (ind_blks + icap - 1) / icap;
+	if (lblock &lt; EXT4_NDIR_BLOCKS)
+		return 0;
 
-	tind_blks = 1;
+	lblock -= EXT4_NDIR_BLOCKS;
 
-	return ind_blks + dind_blks + tind_blks;
+	if (ei-&gt;i_da_metadata_calc_len &amp;&amp;
+	    (lblock &amp; dind_mask) == ei-&gt;i_da_metadata_calc_last_lblock) {
+		ei-&gt;i_da_metadata_calc_len++;
+		return 0;
+	}
+	ei-&gt;i_da_metadata_calc_last_lblock = lblock &amp; dind_mask;
+	ei-&gt;i_da_metadata_calc_len = 1;
+	blk_bits = roundup_pow_of_two(lblock + 1);
+	return (blk_bits / EXT4_ADDR_PER_BLOCK_BITS(inode-&gt;i_sb)) + 1;
 }
 
 /*
  * Calculate the number of metadata blocks need to reserve
- * to allocate given number of blocks
+ * to allocate a block located at @lblock
  */
-static int ext4_calc_metadata_amount(struct inode *inode, int blocks)
+static int ext4_calc_metadata_amount(struct inode *inode, sector_t lblock)
 {
-	if (!blocks)
-		return 0;
-
 	if (EXT4_I(inode)-&gt;i_flags &amp; EXT4_EXTENTS_FL)
-		return ext4_ext_calc_metadata_amount(inode, blocks);
+		return ext4_ext_calc_metadata_amount(inode, lblock);
 
-	return ext4_indirect_calc_metadata_amount(inode, blocks);
+	return ext4_indirect_calc_metadata_amount(inode, lblock);
 }
 
 /*
@@ -1078,6 +1084,7 @@ static void ext4_da_update_reserve_space(struct inode *inode, int used)
 		 */
 		mdb_free = ei-&gt;i_reserved_meta_blocks;
 		ei-&gt;i_reserved_meta_blocks = 0;
+		ei-&gt;i_da_metadata_calc_len = 0;
 		percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, mdb_free);
 	}
 	spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
@@ -1802,12 +1809,15 @@ static int ext4_journalled_write_end(struct file *file,
 	return ret ? ret : copied;
 }
 
-static int ext4_da_reserve_space(struct inode *inode, int nrblocks)
+/*
+ * Reserve a single block located at lblock
+ */
+static int ext4_da_reserve_space(struct inode *inode, sector_t lblock)
 {
 	int retries = 0;
 	struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
 	struct ext4_inode_info *ei = EXT4_I(inode);
-	unsigned long md_needed, md_reserved, total = 0;
+	unsigned long md_needed, md_reserved;
 
 	/*
 	 * recalculate the amount of metadata blocks to reserve
@@ -1817,8 +1827,7 @@ static int ext4_da_reserve_space(struct inode *inode, int nrblocks)
 repeat:
 	spin_lock(&amp;ei-&gt;i_block_reservation_lock);
 	md_reserved = ei-&gt;i_reserved_meta_blocks;
-	md_needed = ext4_calc_metadata_amount(inode, nrblocks);
-	total = md_needed + nrblocks;
+	md_needed = ext4_calc_metadata_amount(inode, lblock);
 	spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
 
 	/*
@@ -1826,7 +1835,7 @@ static int ext4_da_reserve_space(struct inode *inode, int nrblocks)
 	 * later. Real quota accounting is done at pages writeout
 	 * time.
 	 */
-	if (vfs_dq_reserve_block(inode, total)) {
+	if (vfs_dq_reserve_block(inode, md_needed + 1)) {
 		/* 
 		 * We tend to badly over-estimate the amount of
 		 * metadata blocks which are needed, so if we have
@@ -1838,8 +1847,8 @@ static int ext4_da_reserve_space(struct inode *inode, int nrblocks)
 		return -EDQUOT;
 	}
 
-	if (ext4_claim_free_blocks(sbi, total)) {
-		vfs_dq_release_reservation_block(inode, total);
+	if (ext4_claim_free_blocks(sbi, md_needed + 1)) {
+		vfs_dq_release_reservation_block(inode, md_needed + 1);
 		if (ext4_should_retry_alloc(inode-&gt;i_sb, &amp;retries)) {
 		retry:
 			if (md_reserved)
@@ -1850,7 +1859,7 @@ static int ext4_da_reserve_space(struct inode *inode, int nrblocks)
 		return -ENOSPC;
 	}
 	spin_lock(&amp;ei-&gt;i_block_reservation_lock);
-	ei-&gt;i_reserved_data_blocks += nrblocks;
+	ei-&gt;i_reserved_data_blocks++;
 	ei-&gt;i_reserved_meta_blocks += md_needed;
 	spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
 
@@ -1891,6 +1900,7 @@ static void ext4_da_release_space(struct inode *inode, int to_free)
 		 */
 		to_free += ei-&gt;i_reserved_meta_blocks;
 		ei-&gt;i_reserved_meta_blocks = 0;
+		ei-&gt;i_da_metadata_calc_len = 0;
 	}
 
 	/* update fs dirty blocks counter */
@@ -2504,7 +2514,7 @@ static int ext4_da_get_block_prep(struct inode *inode, sector_t iblock,
 		 * XXX: __block_prepare_write() unmaps passed block,
 		 * is it OK?
 		 */
-		ret = ext4_da_reserve_space(inode, 1);
+		ret = ext4_da_reserve_space(inode, iblock);
 		if (ret)
 			/* not enough space to reserve */
 			return ret;
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 7cccb35c0f4d..735c20d5fd56 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -702,6 +702,7 @@ static struct inode *ext4_alloc_inode(struct super_block *sb)
 	ei-&gt;i_reserved_data_blocks = 0;
 	ei-&gt;i_reserved_meta_blocks = 0;
 	ei-&gt;i_allocated_meta_blocks = 0;
+	ei-&gt;i_da_metadata_calc_len = 0;
 	ei-&gt;i_delalloc_reserved_flag = 0;
 	spin_lock_init(&amp;(ei-&gt;i_block_reservation_lock));
 #ifdef CONFIG_QUOTA</pre><hr><pre>commit ee5f4d9cdf32fd99172d11665c592a288c2b1ff4
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Jan 1 02:36:15 2010 -0500

    ext4: Fix accounting of reserved metadata blocks
    
    Commit 0637c6f had a typo which caused the reserved metadata blocks to
    not be released correctly.   Fix this.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 84eeb8f515a3..bdaa92a29e0e 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1076,9 +1076,9 @@ static void ext4_da_update_reserve_space(struct inode *inode, int used)
 		 * only when we have written all of the delayed
 		 * allocation blocks.
 		 */
-		mdb_free = ei-&gt;i_allocated_meta_blocks;
+		mdb_free = ei-&gt;i_reserved_meta_blocks;
+		ei-&gt;i_reserved_meta_blocks = 0;
 		percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, mdb_free);
-		ei-&gt;i_allocated_meta_blocks = 0;
 	}
 	spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 
@@ -1889,8 +1889,8 @@ static void ext4_da_release_space(struct inode *inode, int to_free)
 		 * only when we have written all of the delayed
 		 * allocation blocks.
 		 */
-		to_free += ei-&gt;i_allocated_meta_blocks;
-		ei-&gt;i_allocated_meta_blocks = 0;
+		to_free += ei-&gt;i_reserved_meta_blocks;
+		ei-&gt;i_reserved_meta_blocks = 0;
 	}
 
 	/* update fs dirty blocks counter */</pre><hr><pre>commit 0637c6f4135f592f094207c7c21e7c0fc5557834
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Dec 30 14:20:45 2009 -0500

    ext4: Patch up how we claim metadata blocks for quota purposes
    
    As reported in Kernel Bugzilla #14936, commit d21cd8f triggered a BUG
    in the function ext4_da_update_reserve_space() found in
    fs/ext4/inode.c.  The root cause of this BUG() was caused by the fact
    that ext4_calc_metadata_amount() can severely over-estimate how many
    metadata blocks will be needed, especially when using direct
    block-mapped files.
    
    In addition, it can also badly *under* estimate how much space is
    needed, since ext4_calc_metadata_amount() assumes that the blocks are
    contiguous, and this is not always true.  If the application is
    writing blocks to a sparse file, the number of metadata blocks
    necessary can be severly underestimated by the functions
    ext4_da_reserve_space(), ext4_da_update_reserve_space() and
    ext4_da_release_space().  This was the cause of the dq_claim_space
    reports found on kerneloops.org.
    
    Unfortunately, doing this right means that we need to massively
    over-estimate the amount of free space needed.  So in some cases we
    may need to force the inode to be written to disk asynchronously in
    to avoid spurious quota failures.
    
    http://bugzilla.kernel.org/show_bug.cgi?id=14936
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 3e3b45458ef8..84eeb8f515a3 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1043,43 +1043,47 @@ static int ext4_calc_metadata_amount(struct inode *inode, int blocks)
 	return ext4_indirect_calc_metadata_amount(inode, blocks);
 }
 
+/*
+ * Called with i_data_sem down, which is important since we can call
+ * ext4_discard_preallocations() from here.
+ */
 static void ext4_da_update_reserve_space(struct inode *inode, int used)
 {
 	struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
-	int total, mdb, mdb_free, mdb_claim = 0;
-
-	spin_lock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
-	/* recalculate the number of metablocks still need to be reserved */
-	total = EXT4_I(inode)-&gt;i_reserved_data_blocks - used;
-	mdb = ext4_calc_metadata_amount(inode, total);
-
-	/* figure out how many metablocks to release */
-	BUG_ON(mdb &gt; EXT4_I(inode)-&gt;i_reserved_meta_blocks);
-	mdb_free = EXT4_I(inode)-&gt;i_reserved_meta_blocks - mdb;
-
-	if (mdb_free) {
-		/* Account for allocated meta_blocks */
-		mdb_claim = EXT4_I(inode)-&gt;i_allocated_meta_blocks;
-		BUG_ON(mdb_free &lt; mdb_claim);
-		mdb_free -= mdb_claim;
-
-		/* update fs dirty blocks counter */
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	int mdb_free = 0;
+
+	spin_lock(&amp;ei-&gt;i_block_reservation_lock);
+	if (unlikely(used &gt; ei-&gt;i_reserved_data_blocks)) {
+		ext4_msg(inode-&gt;i_sb, KERN_NOTICE, "%s: ino %lu, used %d "
+			 "with only %d reserved data blocks\n",
+			 __func__, inode-&gt;i_ino, used,
+			 ei-&gt;i_reserved_data_blocks);
+		WARN_ON(1);
+		used = ei-&gt;i_reserved_data_blocks;
+	}
+
+	/* Update per-inode reservations */
+	ei-&gt;i_reserved_data_blocks -= used;
+	used += ei-&gt;i_allocated_meta_blocks;
+	ei-&gt;i_reserved_meta_blocks -= ei-&gt;i_allocated_meta_blocks;
+	ei-&gt;i_allocated_meta_blocks = 0;
+	percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, used);
+
+	if (ei-&gt;i_reserved_data_blocks == 0) {
+		/*
+		 * We can release all of the reserved metadata blocks
+		 * only when we have written all of the delayed
+		 * allocation blocks.
+		 */
+		mdb_free = ei-&gt;i_allocated_meta_blocks;
 		percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, mdb_free);
-		EXT4_I(inode)-&gt;i_allocated_meta_blocks = 0;
-		EXT4_I(inode)-&gt;i_reserved_meta_blocks = mdb;
+		ei-&gt;i_allocated_meta_blocks = 0;
 	}
-
-	/* update per-inode reservations */
-	BUG_ON(used  &gt; EXT4_I(inode)-&gt;i_reserved_data_blocks);
-	EXT4_I(inode)-&gt;i_reserved_data_blocks -= used;
-	percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, used + mdb_claim);
 	spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 
-	vfs_dq_claim_block(inode, used + mdb_claim);
-
-	/*
-	 * free those over-booking quota for metadata blocks
-	 */
+	/* Update quota subsystem */
+	vfs_dq_claim_block(inode, used);
 	if (mdb_free)
 		vfs_dq_release_reservation_block(inode, mdb_free);
 
@@ -1088,7 +1092,8 @@ static void ext4_da_update_reserve_space(struct inode *inode, int used)
 	 * there aren't any writers on the inode, we can discard the
 	 * inode's preallocations.
 	 */
-	if (!total &amp;&amp; (atomic_read(&amp;inode-&gt;i_writecount) == 0))
+	if ((ei-&gt;i_reserved_data_blocks == 0) &amp;&amp;
+	    (atomic_read(&amp;inode-&gt;i_writecount) == 0))
 		ext4_discard_preallocations(inode);
 }
 
@@ -1801,7 +1806,8 @@ static int ext4_da_reserve_space(struct inode *inode, int nrblocks)
 {
 	int retries = 0;
 	struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
-	unsigned long md_needed, mdblocks, total = 0;
+	struct ext4_inode_info *ei = EXT4_I(inode);
+	unsigned long md_needed, md_reserved, total = 0;
 
 	/*
 	 * recalculate the amount of metadata blocks to reserve
@@ -1809,35 +1815,44 @@ static int ext4_da_reserve_space(struct inode *inode, int nrblocks)
 	 * worse case is one extent per block
 	 */
 repeat:
-	spin_lock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
-	total = EXT4_I(inode)-&gt;i_reserved_data_blocks + nrblocks;
-	mdblocks = ext4_calc_metadata_amount(inode, total);
-	BUG_ON(mdblocks &lt; EXT4_I(inode)-&gt;i_reserved_meta_blocks);
-
-	md_needed = mdblocks - EXT4_I(inode)-&gt;i_reserved_meta_blocks;
+	spin_lock(&amp;ei-&gt;i_block_reservation_lock);
+	md_reserved = ei-&gt;i_reserved_meta_blocks;
+	md_needed = ext4_calc_metadata_amount(inode, nrblocks);
 	total = md_needed + nrblocks;
-	spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
+	spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
 
 	/*
 	 * Make quota reservation here to prevent quota overflow
 	 * later. Real quota accounting is done at pages writeout
 	 * time.
 	 */
-	if (vfs_dq_reserve_block(inode, total))
+	if (vfs_dq_reserve_block(inode, total)) {
+		/* 
+		 * We tend to badly over-estimate the amount of
+		 * metadata blocks which are needed, so if we have
+		 * reserved any metadata blocks, try to force out the
+		 * inode and see if we have any better luck.
+		 */
+		if (md_reserved &amp;&amp; retries++ &lt;= 3)
+			goto retry;
 		return -EDQUOT;
+	}
 
 	if (ext4_claim_free_blocks(sbi, total)) {
 		vfs_dq_release_reservation_block(inode, total);
 		if (ext4_should_retry_alloc(inode-&gt;i_sb, &amp;retries)) {
+		retry:
+			if (md_reserved)
+				write_inode_now(inode, (retries == 3));
 			yield();
 			goto repeat;
 		}
 		return -ENOSPC;
 	}
-	spin_lock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
-	EXT4_I(inode)-&gt;i_reserved_data_blocks += nrblocks;
-	EXT4_I(inode)-&gt;i_reserved_meta_blocks += md_needed;
-	spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
+	spin_lock(&amp;ei-&gt;i_block_reservation_lock);
+	ei-&gt;i_reserved_data_blocks += nrblocks;
+	ei-&gt;i_reserved_meta_blocks += md_needed;
+	spin_unlock(&amp;ei-&gt;i_block_reservation_lock);
 
 	return 0;       /* success */
 }
@@ -1845,49 +1860,45 @@ static int ext4_da_reserve_space(struct inode *inode, int nrblocks)
 static void ext4_da_release_space(struct inode *inode, int to_free)
 {
 	struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
-	int total, mdb, mdb_free, release;
+	struct ext4_inode_info *ei = EXT4_I(inode);
 
 	if (!to_free)
 		return;		/* Nothing to release, exit */
 
 	spin_lock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 
-	if (!EXT4_I(inode)-&gt;i_reserved_data_blocks) {
+	if (unlikely(to_free &gt; ei-&gt;i_reserved_data_blocks)) {
 		/*
-		 * if there is no reserved blocks, but we try to free some
-		 * then the counter is messed up somewhere.
-		 * but since this function is called from invalidate
-		 * page, it's harmless to return without any action
+		 * if there aren't enough reserved blocks, then the
+		 * counter is messed up somewhere.  Since this
+		 * function is called from invalidate page, it's
+		 * harmless to return without any action.
 		 */
-		printk(KERN_INFO "ext4 delalloc try to release %d reserved "
-			    "blocks for inode %lu, but there is no reserved "
-			    "data blocks\n", to_free, inode-&gt;i_ino);
-		spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
-		return;
+		ext4_msg(inode-&gt;i_sb, KERN_NOTICE, "ext4_da_release_space: "
+			 "ino %lu, to_free %d with only %d reserved "
+			 "data blocks\n", inode-&gt;i_ino, to_free,
+			 ei-&gt;i_reserved_data_blocks);
+		WARN_ON(1);
+		to_free = ei-&gt;i_reserved_data_blocks;
 	}
+	ei-&gt;i_reserved_data_blocks -= to_free;
 
-	/* recalculate the number of metablocks still need to be reserved */
-	total = EXT4_I(inode)-&gt;i_reserved_data_blocks - to_free;
-	mdb = ext4_calc_metadata_amount(inode, total);
-
-	/* figure out how many metablocks to release */
-	BUG_ON(mdb &gt; EXT4_I(inode)-&gt;i_reserved_meta_blocks);
-	mdb_free = EXT4_I(inode)-&gt;i_reserved_meta_blocks - mdb;
-
-	release = to_free + mdb_free;
-
-	/* update fs dirty blocks counter for truncate case */
-	percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, release);
+	if (ei-&gt;i_reserved_data_blocks == 0) {
+		/*
+		 * We can release all of the reserved metadata blocks
+		 * only when we have written all of the delayed
+		 * allocation blocks.
+		 */
+		to_free += ei-&gt;i_allocated_meta_blocks;
+		ei-&gt;i_allocated_meta_blocks = 0;
+	}
 
-	/* update per-inode reservations */
-	BUG_ON(to_free &gt; EXT4_I(inode)-&gt;i_reserved_data_blocks);
-	EXT4_I(inode)-&gt;i_reserved_data_blocks -= to_free;
+	/* update fs dirty blocks counter */
+	percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, to_free);
 
-	BUG_ON(mdb &gt; EXT4_I(inode)-&gt;i_reserved_meta_blocks);
-	EXT4_I(inode)-&gt;i_reserved_meta_blocks = mdb;
 	spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 
-	vfs_dq_release_reservation_block(inode, release);
+	vfs_dq_release_reservation_block(inode, to_free);
 }
 
 static void ext4_da_page_release_reservation(struct page *page,</pre>
    <div class="pagination">
        <a href='1_83.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><span>[84]</span><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_85.html'>Next&gt;&gt;</a>
    <div>
</body>

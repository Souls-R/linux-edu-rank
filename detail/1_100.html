<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_99.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><span>[100]</span><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_101.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 7d8f9f7d150dded7b68e61ca6403a1f166fb4edf
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Tue Feb 24 08:21:14 2009 -0500

    ext4: Automatically allocate delay allocated blocks on close
    
    When closing a file that had been previously truncated, force any
    delay allocated blocks that to be allocated so that if the filesystem
    is mounted with data=ordered, the data blocks will be pushed out to
    disk along with the journal commit.  Many application programs expect
    this, so we do this to avoid zero length files if the system crashes
    unexpectedly.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index b0ea70cc94db..1b0c17364631 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -270,6 +270,7 @@ static inline __u32 ext4_mask_flags(umode_t mode, __u32 flags)
 #define EXT4_STATE_NEW			0x00000002 /* inode is newly created */
 #define EXT4_STATE_XATTR		0x00000004 /* has in-inode xattrs */
 #define EXT4_STATE_NO_EXPAND		0x00000008 /* No space for expansion */
+#define EXT4_STATE_DA_ALLOC_CLOSE	0x00000010 /* Alloc DA blks on close */
 
 /* Used to pass group descriptor data when online resize is done */
 struct ext4_new_group_input {
diff --git a/fs/ext4/file.c b/fs/ext4/file.c
index f731cb545a03..06df8272c639 100644
--- a/fs/ext4/file.c
+++ b/fs/ext4/file.c
@@ -33,6 +33,10 @@
  */
 static int ext4_release_file(struct inode *inode, struct file *filp)
 {
+	if (EXT4_I(inode)-&gt;i_state &amp; EXT4_STATE_DA_ALLOC_CLOSE) {
+		ext4_alloc_da_blocks(inode);
+		EXT4_I(inode)-&gt;i_state &amp;= ~EXT4_STATE_DA_ALLOC_CLOSE;
+	}
 	/* if we are the last writer on the inode, drop the block reservation */
 	if ((filp-&gt;f_mode &amp; FMODE_WRITE) &amp;&amp;
 			(atomic_read(&amp;inode-&gt;i_writecount) == 1))
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 8dd3d5de5861..80ed6dc9c9d2 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -3901,6 +3901,9 @@ void ext4_truncate(struct inode *inode)
 	if (!ext4_can_truncate(inode))
 		return;
 
+	if (inode-&gt;i_size == 0)
+		ei-&gt;i_state |= EXT4_STATE_DA_ALLOC_CLOSE;
+
 	if (EXT4_I(inode)-&gt;i_flags &amp; EXT4_EXTENTS_FL) {
 		ext4_ext_truncate(inode);
 		return;</pre><hr><pre>commit ccd2506bd43113659aa904d5bea5d1300605e2a6
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Feb 26 01:04:07 2009 -0500

    ext4: add EXT4_IOC_ALLOC_DA_BLKS ioctl
    
    Add an ioctl which forces all of the delay allocated blocks to be
    allocated.  This also provides a function ext4_alloc_da_blocks() which
    will be used by the following commits to force files to be fully
    allocated to preserve application-expected ext3 behaviour.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 096456c8559b..b0ea70cc94db 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -317,7 +317,9 @@ struct ext4_new_group_data {
 #define EXT4_IOC_GROUP_EXTEND		_IOW('f', 7, unsigned long)
 #define EXT4_IOC_GROUP_ADD		_IOW('f', 8, struct ext4_new_group_input)
 #define EXT4_IOC_MIGRATE		_IO('f', 9)
+ /* note ioctl 10 reserved for an early version of the FIEMAP ioctl */
  /* note ioctl 11 reserved for filesystem-independent FIEMAP ioctl */
+#define EXT4_IOC_ALLOC_DA_BLKS		_IO('f', 12)
 
 /*
  * ioctl commands in 32 bit emulation
@@ -1094,6 +1096,7 @@ extern int ext4_can_truncate(struct inode *inode);
 extern void ext4_truncate(struct inode *);
 extern void ext4_set_inode_flags(struct inode *);
 extern void ext4_get_inode_flags(struct ext4_inode_info *);
+extern int ext4_alloc_da_blocks(struct inode *inode);
 extern void ext4_set_aops(struct inode *inode);
 extern int ext4_writepage_trans_blocks(struct inode *);
 extern int ext4_meta_trans_blocks(struct inode *, int nrblocks, int idxblocks);
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 24d0f9d2b320..8dd3d5de5861 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2837,6 +2837,48 @@ static void ext4_da_invalidatepage(struct page *page, unsigned long offset)
 	return;
 }
 
+/*
+ * Force all delayed allocation blocks to be allocated for a given inode.
+ */
+int ext4_alloc_da_blocks(struct inode *inode)
+{
+	if (!EXT4_I(inode)-&gt;i_reserved_data_blocks &amp;&amp;
+	    !EXT4_I(inode)-&gt;i_reserved_meta_blocks)
+		return 0;
+
+	/*
+	 * We do something simple for now.  The filemap_flush() will
+	 * also start triggering a write of the data blocks, which is
+	 * not strictly speaking necessary (and for users of
+	 * laptop_mode, not even desirable).  However, to do otherwise
+	 * would require replicating code paths in:
+	 * 
+	 * ext4_da_writepages() -&gt;
+	 *    write_cache_pages() ---&gt; (via passed in callback function)
+	 *        __mpage_da_writepage() --&gt;
+	 *           mpage_add_bh_to_extent()
+	 *           mpage_da_map_blocks()
+	 *
+	 * The problem is that write_cache_pages(), located in
+	 * mm/page-writeback.c, marks pages clean in preparation for
+	 * doing I/O, which is not desirable if we're not planning on
+	 * doing I/O at all.
+	 *
+	 * We could call write_cache_pages(), and then redirty all of
+	 * the pages by calling redirty_page_for_writeback() but that
+	 * would be ugly in the extreme.  So instead we would need to
+	 * replicate parts of the code in the above functions,
+	 * simplifying them becuase we wouldn't actually intend to
+	 * write out the pages, but rather only collect contiguous
+	 * logical block extents, call the multi-block allocator, and
+	 * then update the buffer heads with the block allocations.
+	 * 
+	 * For now, though, we'll cheat by calling filemap_flush(),
+	 * which will map the blocks, and start the I/O, but not
+	 * actually wait for the I/O to complete.
+	 */
+	return filemap_flush(inode-&gt;i_mapping);
+}
 
 /*
  * bmap() is special.  It gets used by applications such as lilo and by
diff --git a/fs/ext4/ioctl.c b/fs/ext4/ioctl.c
index 22dd29f3ebc9..91e75f7a9e73 100644
--- a/fs/ext4/ioctl.c
+++ b/fs/ext4/ioctl.c
@@ -262,6 +262,20 @@ long ext4_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 		return err;
 	}
 
+	case EXT4_IOC_ALLOC_DA_BLKS:
+	{
+		int err;
+		if (!is_owner_or_cap(inode))
+			return -EACCES;
+
+		err = mnt_want_write(filp-&gt;f_path.mnt);
+		if (err)
+			return err;
+		err = ext4_alloc_da_blocks(inode);
+		mnt_drop_write(filp-&gt;f_path.mnt);
+		return err;
+	}
+
 	default:
 		return -ENOTTY;
 	}</pre><hr><pre>commit f63e6005bc63acc0a6bc3bdb8f971dcfbd827185
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Feb 23 16:42:39 2009 -0500

    ext4: Simplify delalloc code by removing mpage_da_writepages()
    
    The mpage_da_writepages() function is only used in one place, so
    inline it to simplify the call stack and make the code easier to
    understand.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 008b28a859d0..24d0f9d2b320 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2241,47 +2241,6 @@ static int __mpage_da_writepage(struct page *page,
 	return 0;
 }
 
-/*
- * mpage_da_writepages - walk the list of dirty pages of the given
- * address space, allocates non-allocated blocks, maps newly-allocated
- * blocks to existing bhs and issue IO them
- *
- * @mapping: address space structure to write
- * @wbc: subtract the number of written pages from *@wbc-&gt;nr_to_write
- *
- * This is a library function, which implements the writepages()
- * address_space_operation.
- */
-static int mpage_da_writepages(struct address_space *mapping,
-			       struct writeback_control *wbc,
-			       struct mpage_da_data *mpd)
-{
-	int ret;
-
-	mpd-&gt;b_size = 0;
-	mpd-&gt;b_state = 0;
-	mpd-&gt;b_blocknr = 0;
-	mpd-&gt;first_page = 0;
-	mpd-&gt;next_page = 0;
-	mpd-&gt;io_done = 0;
-	mpd-&gt;pages_written = 0;
-	mpd-&gt;retval = 0;
-
-	ret = write_cache_pages(mapping, wbc, __mpage_da_writepage, mpd);
-	/*
-	 * Handle last extent of pages
-	 */
-	if (!mpd-&gt;io_done &amp;&amp; mpd-&gt;next_page != mpd-&gt;first_page) {
-		if (mpage_da_map_blocks(mpd) == 0)
-			mpage_da_submit_io(mpd);
-
-		mpd-&gt;io_done = 1;
-		ret = MPAGE_DA_EXTENT_TAIL;
-	}
-	wbc-&gt;nr_to_write -= mpd-&gt;pages_written;
-	return ret;
-}
-
 /*
  * this is a special callback for -&gt;write_begin() only
  * it's intention is to return mapped block or reserve space
@@ -2571,7 +2530,38 @@ static int ext4_da_writepages(struct address_space *mapping,
 			dump_stack();
 			goto out_writepages;
 		}
-		ret = mpage_da_writepages(mapping, wbc, &amp;mpd);
+
+		/*
+		 * Now call __mpage_da_writepage to find the next
+		 * contiguous region of logical blocks that need
+		 * blocks to be allocated by ext4.  We don't actually
+		 * submit the blocks for I/O here, even though
+		 * write_cache_pages thinks it will, and will set the
+		 * pages as clean for write before calling
+		 * __mpage_da_writepage().
+		 */
+		mpd.b_size = 0;
+		mpd.b_state = 0;
+		mpd.b_blocknr = 0;
+		mpd.first_page = 0;
+		mpd.next_page = 0;
+		mpd.io_done = 0;
+		mpd.pages_written = 0;
+		mpd.retval = 0;
+		ret = write_cache_pages(mapping, wbc, __mpage_da_writepage,
+					&amp;mpd);
+		/*
+		 * If we have a contigous extent of pages and we
+		 * haven't done the I/O yet, map the blocks and submit
+		 * them for I/O.
+		 */
+		if (!mpd.io_done &amp;&amp; mpd.next_page != mpd.first_page) {
+			if (mpage_da_map_blocks(&amp;mpd) == 0)
+				mpage_da_submit_io(&amp;mpd);
+			mpd.io_done = 1;
+			ret = MPAGE_DA_EXTENT_TAIL;
+		}
+		wbc-&gt;nr_to_write -= mpd.pages_written;
 
 		ext4_journal_stop(handle);
 </pre><hr><pre>commit 8dc207c0e7a259e7122ddfaf56b8bbbc3c92d685
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Feb 23 06:46:01 2009 -0500

    ext4: Save stack space by removing fake buffer heads
    
    Struct mpage_da_data and mpage_add_bh_to_extent() use a fake struct
    buffer_head which is 104 bytes on an x86_64 system, but only use 24
    bytes of the structure.  On systems that use a spinlock for atomic_t,
    the stack savings will be even greater.
    
    It turns out that using a fake struct buffer_head doesn't even save
    that much code, and it makes the code more confusing since it's not
    used as a "real" buffer head.  So just store pass b_size and b_state
    in mpage_add_bh_to_extent(), and store b_size, b_state, and b_block_nr
    in the mpage_da_data structure.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 924ba8afc227..008b28a859d0 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1703,7 +1703,9 @@ static void ext4_da_page_release_reservation(struct page *page,
 
 struct mpage_da_data {
 	struct inode *inode;
-	struct buffer_head lbh;			/* extent of blocks */
+	sector_t b_blocknr;		/* start block number of extent */
+	size_t b_size;			/* size of extent */
+	unsigned long b_state;		/* state of the extent */
 	unsigned long first_page, next_page;	/* extent of pages */
 	struct writeback_control *wbc;
 	int io_done;
@@ -1737,7 +1739,7 @@ static int mpage_da_submit_io(struct mpage_da_data *mpd)
 	/*
 	 * We need to start from the first_page to the next_page - 1
 	 * to make sure we also write the mapped dirty buffer_heads.
-	 * If we look at mpd-&gt;lbh.b_blocknr we would only be looking
+	 * If we look at mpd-&gt;b_blocknr we would only be looking
 	 * at the currently mapped buffer_heads.
 	 */
 	index = mpd-&gt;first_page;
@@ -1975,7 +1977,7 @@ static int ext4_da_get_block_write(struct inode *inode, sector_t iblock,
 /*
  * mpage_da_map_blocks - go through given space
  *
- * @mpd-&gt;lbh - bh describing space
+ * @mpd - bh describing space
  *
  * The function skips space we know is already mapped to disk blocks.
  *
@@ -1984,18 +1986,18 @@ static int mpage_da_map_blocks(struct mpage_da_data *mpd)
 {
 	int err = 0;
 	struct buffer_head new;
-	struct buffer_head *lbh = &amp;mpd-&gt;lbh;
 	sector_t next;
 
 	/*
 	 * We consider only non-mapped and non-allocated blocks
 	 */
-	if (buffer_mapped(lbh) &amp;&amp; !buffer_delay(lbh))
+	if ((mpd-&gt;b_state  &amp; (1 &lt;&lt; BH_Mapped)) &amp;&amp;
+	    !(mpd-&gt;b_state &amp; (1 &lt;&lt; BH_Delay)))
 		return 0;
-	new.b_state = lbh-&gt;b_state;
+	new.b_state = mpd-&gt;b_state;
 	new.b_blocknr = 0;
-	new.b_size = lbh-&gt;b_size;
-	next = lbh-&gt;b_blocknr;
+	new.b_size = mpd-&gt;b_size;
+	next = mpd-&gt;b_blocknr;
 	/*
 	 * If we didn't accumulate anything
 	 * to write simply return
@@ -2031,7 +2033,7 @@ static int mpage_da_map_blocks(struct mpage_da_data *mpd)
 				  "%zd with error %d\n",
 				  __func__, mpd-&gt;inode-&gt;i_ino,
 				  (unsigned long long)next,
-				  lbh-&gt;b_size &gt;&gt; mpd-&gt;inode-&gt;i_blkbits, err);
+				  mpd-&gt;b_size &gt;&gt; mpd-&gt;inode-&gt;i_blkbits, err);
 		printk(KERN_EMERG "This should not happen.!! "
 					"Data will be lost\n");
 		if (err == -ENOSPC) {
@@ -2039,7 +2041,7 @@ static int mpage_da_map_blocks(struct mpage_da_data *mpd)
 		}
 		/* invlaidate all the pages */
 		ext4_da_block_invalidatepages(mpd, next,
-				lbh-&gt;b_size &gt;&gt; mpd-&gt;inode-&gt;i_blkbits);
+				mpd-&gt;b_size &gt;&gt; mpd-&gt;inode-&gt;i_blkbits);
 		return err;
 	}
 	BUG_ON(new.b_size == 0);
@@ -2051,7 +2053,8 @@ static int mpage_da_map_blocks(struct mpage_da_data *mpd)
 	 * If blocks are delayed marked, we need to
 	 * put actual blocknr and drop delayed bit
 	 */
-	if (buffer_delay(lbh) || buffer_unwritten(lbh))
+	if ((mpd-&gt;b_state &amp; (1 &lt;&lt; BH_Delay)) ||
+	    (mpd-&gt;b_state &amp; (1 &lt;&lt; BH_Unwritten)))
 		mpage_put_bnr_to_bhs(mpd, next, &amp;new);
 
 	return 0;
@@ -2070,12 +2073,11 @@ static int mpage_da_map_blocks(struct mpage_da_data *mpd)
  * the function is used to collect contig. blocks in same state
  */
 static void mpage_add_bh_to_extent(struct mpage_da_data *mpd,
-				   sector_t logical, struct buffer_head *bh)
+				   sector_t logical, size_t b_size,
+				   unsigned long b_state)
 {
 	sector_t next;
-	size_t b_size = bh-&gt;b_size;
-	struct buffer_head *lbh = &amp;mpd-&gt;lbh;
-	int nrblocks = lbh-&gt;b_size &gt;&gt; mpd-&gt;inode-&gt;i_blkbits;
+	int nrblocks = mpd-&gt;b_size &gt;&gt; mpd-&gt;inode-&gt;i_blkbits;
 
 	/* check if thereserved journal credits might overflow */
 	if (!(EXT4_I(mpd-&gt;inode)-&gt;i_flags &amp; EXT4_EXTENTS_FL)) {
@@ -2102,19 +2104,19 @@ static void mpage_add_bh_to_extent(struct mpage_da_data *mpd,
 	/*
 	 * First block in the extent
 	 */
-	if (lbh-&gt;b_size == 0) {
-		lbh-&gt;b_blocknr = logical;
-		lbh-&gt;b_size = b_size;
-		lbh-&gt;b_state = bh-&gt;b_state &amp; BH_FLAGS;
+	if (mpd-&gt;b_size == 0) {
+		mpd-&gt;b_blocknr = logical;
+		mpd-&gt;b_size = b_size;
+		mpd-&gt;b_state = b_state &amp; BH_FLAGS;
 		return;
 	}
 
-	next = lbh-&gt;b_blocknr + nrblocks;
+	next = mpd-&gt;b_blocknr + nrblocks;
 	/*
 	 * Can we merge the block to our big extent?
 	 */
-	if (logical == next &amp;&amp; (bh-&gt;b_state &amp; BH_FLAGS) == lbh-&gt;b_state) {
-		lbh-&gt;b_size += b_size;
+	if (logical == next &amp;&amp; (b_state &amp; BH_FLAGS) == mpd-&gt;b_state) {
+		mpd-&gt;b_size += b_size;
 		return;
 	}
 
@@ -2143,7 +2145,7 @@ static int __mpage_da_writepage(struct page *page,
 {
 	struct mpage_da_data *mpd = data;
 	struct inode *inode = mpd-&gt;inode;
-	struct buffer_head *bh, *head, fake;
+	struct buffer_head *bh, *head;
 	sector_t logical;
 
 	if (mpd-&gt;io_done) {
@@ -2185,9 +2187,9 @@ static int __mpage_da_writepage(struct page *page,
 		/*
 		 * ... and blocks
 		 */
-		mpd-&gt;lbh.b_size = 0;
-		mpd-&gt;lbh.b_state = 0;
-		mpd-&gt;lbh.b_blocknr = 0;
+		mpd-&gt;b_size = 0;
+		mpd-&gt;b_state = 0;
+		mpd-&gt;b_blocknr = 0;
 	}
 
 	mpd-&gt;next_page = page-&gt;index + 1;
@@ -2195,16 +2197,8 @@ static int __mpage_da_writepage(struct page *page,
 		  (PAGE_CACHE_SHIFT - inode-&gt;i_blkbits);
 
 	if (!page_has_buffers(page)) {
-		/*
-		 * There is no attached buffer heads yet (mmap?)
-		 * we treat the page asfull of dirty blocks
-		 */
-		bh = &amp;fake;
-		bh-&gt;b_size = PAGE_CACHE_SIZE;
-		bh-&gt;b_state = 0;
-		set_buffer_dirty(bh);
-		set_buffer_uptodate(bh);
-		mpage_add_bh_to_extent(mpd, logical, bh);
+		mpage_add_bh_to_extent(mpd, logical, PAGE_CACHE_SIZE,
+				       (1 &lt;&lt; BH_Dirty) | (1 &lt;&lt; BH_Uptodate));
 		if (mpd-&gt;io_done)
 			return MPAGE_DA_EXTENT_TAIL;
 	} else {
@@ -2222,8 +2216,10 @@ static int __mpage_da_writepage(struct page *page,
 			 * with the page in ext4_da_writepage
 			 */
 			if (buffer_dirty(bh) &amp;&amp;
-				(!buffer_mapped(bh) || buffer_delay(bh))) {
-				mpage_add_bh_to_extent(mpd, logical, bh);
+			    (!buffer_mapped(bh) || buffer_delay(bh))) {
+				mpage_add_bh_to_extent(mpd, logical,
+						       bh-&gt;b_size,
+						       bh-&gt;b_state);
 				if (mpd-&gt;io_done)
 					return MPAGE_DA_EXTENT_TAIL;
 			} else if (buffer_dirty(bh) &amp;&amp; (buffer_mapped(bh))) {
@@ -2235,9 +2231,8 @@ static int __mpage_da_writepage(struct page *page,
 				 * unmapped buffer_head later we need to
 				 * use the b_state flag of that buffer_head.
 				 */
-				if (mpd-&gt;lbh.b_size == 0)
-					mpd-&gt;lbh.b_state =
-						bh-&gt;b_state &amp; BH_FLAGS;
+				if (mpd-&gt;b_size == 0)
+					mpd-&gt;b_state = bh-&gt;b_state &amp; BH_FLAGS;
 			}
 			logical++;
 		} while ((bh = bh-&gt;b_this_page) != head);
@@ -2263,9 +2258,9 @@ static int mpage_da_writepages(struct address_space *mapping,
 {
 	int ret;
 
-	mpd-&gt;lbh.b_size = 0;
-	mpd-&gt;lbh.b_state = 0;
-	mpd-&gt;lbh.b_blocknr = 0;
+	mpd-&gt;b_size = 0;
+	mpd-&gt;b_state = 0;
+	mpd-&gt;b_blocknr = 0;
 	mpd-&gt;first_page = 0;
 	mpd-&gt;next_page = 0;
 	mpd-&gt;io_done = 0;</pre><hr><pre>commit ed5bde0bf8995d7d8c0b5a9c33e624a945f333ef
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Feb 23 10:48:07 2009 -0500

    ext4: Simplify delalloc implementation by removing mpd.get_block
    
    This parameter was always set to ext4_da_get_block_write().
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index cd0399db0ef1..924ba8afc227 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1705,7 +1705,6 @@ struct mpage_da_data {
 	struct inode *inode;
 	struct buffer_head lbh;			/* extent of blocks */
 	unsigned long first_page, next_page;	/* extent of pages */
-	get_block_t *get_block;
 	struct writeback_control *wbc;
 	int io_done;
 	int pages_written;
@@ -1719,7 +1718,6 @@ struct mpage_da_data {
  * @mpd-&gt;inode: inode
  * @mpd-&gt;first_page: first page of the extent
  * @mpd-&gt;next_page: page after the last page of the extent
- * @mpd-&gt;get_block: the filesystem's block mapper function
  *
  * By the time mpage_da_submit_io() is called we expect all blocks
  * to be allocated. this may be wrong if allocation failed.
@@ -1929,16 +1927,60 @@ static void ext4_print_free_blocks(struct inode *inode)
 	return;
 }
 
+#define		EXT4_DELALLOC_RSVED	1
+static int ext4_da_get_block_write(struct inode *inode, sector_t iblock,
+				   struct buffer_head *bh_result, int create)
+{
+	int ret;
+	unsigned max_blocks = bh_result-&gt;b_size &gt;&gt; inode-&gt;i_blkbits;
+	loff_t disksize = EXT4_I(inode)-&gt;i_disksize;
+	handle_t *handle = NULL;
+
+	handle = ext4_journal_current_handle();
+	BUG_ON(!handle);
+	ret = ext4_get_blocks_wrap(handle, inode, iblock, max_blocks,
+				   bh_result, create, 0, EXT4_DELALLOC_RSVED);
+	if (ret &lt;= 0)
+		return ret;
+
+	bh_result-&gt;b_size = (ret &lt;&lt; inode-&gt;i_blkbits);
+
+	if (ext4_should_order_data(inode)) {
+		int retval;
+		retval = ext4_jbd2_file_inode(handle, inode);
+		if (retval)
+			/*
+			 * Failed to add inode for ordered mode. Don't
+			 * update file size
+			 */
+			return retval;
+	}
+
+	/*
+	 * Update on-disk size along with block allocation we don't
+	 * use 'extend_disksize' as size may change within already
+	 * allocated block -bzzz
+	 */
+	disksize = ((loff_t) iblock + ret) &lt;&lt; inode-&gt;i_blkbits;
+	if (disksize &gt; i_size_read(inode))
+		disksize = i_size_read(inode);
+	if (disksize &gt; EXT4_I(inode)-&gt;i_disksize) {
+		ext4_update_i_disksize(inode, disksize);
+		ret = ext4_mark_inode_dirty(handle, inode);
+		return ret;
+	}
+	return 0;
+}
+
 /*
  * mpage_da_map_blocks - go through given space
  *
  * @mpd-&gt;lbh - bh describing space
- * @mpd-&gt;get_block - the filesystem's block mapper function
  *
  * The function skips space we know is already mapped to disk blocks.
  *
  */
-static int  mpage_da_map_blocks(struct mpage_da_data *mpd)
+static int mpage_da_map_blocks(struct mpage_da_data *mpd)
 {
 	int err = 0;
 	struct buffer_head new;
@@ -1960,30 +2002,29 @@ static int  mpage_da_map_blocks(struct mpage_da_data *mpd)
 	 */
 	if (!new.b_size)
 		return 0;
-	err = mpd-&gt;get_block(mpd-&gt;inode, next, &amp;new, 1);
-	if (err) {
 
-		/* If get block returns with error
-		 * we simply return. Later writepage
-		 * will redirty the page and writepages
-		 * will find the dirty page again
+	err = ext4_da_get_block_write(mpd-&gt;inode, next, &amp;new, 1);
+	if (err) {
+		/*
+		 * If get block returns with error we simply
+		 * return. Later writepage will redirty the page and
+		 * writepages will find the dirty page again
 		 */
 		if (err == -EAGAIN)
 			return 0;
 
 		if (err == -ENOSPC &amp;&amp;
-				ext4_count_free_blocks(mpd-&gt;inode-&gt;i_sb)) {
+		    ext4_count_free_blocks(mpd-&gt;inode-&gt;i_sb)) {
 			mpd-&gt;retval = err;
 			return 0;
 		}
 
 		/*
-		 * get block failure will cause us
-		 * to loop in writepages. Because
-		 * a_ops-&gt;writepage won't be able to
-		 * make progress. The page will be redirtied
-		 * by writepage and writepages will again
-		 * try to write the same.
+		 * get block failure will cause us to loop in
+		 * writepages, because a_ops-&gt;writepage won't be able
+		 * to make progress. The page will be redirtied by
+		 * writepage and writepages will again try to write
+		 * the same.
 		 */
 		printk(KERN_EMERG "%s block allocation failed for inode %lu "
 				  "at logical offset %llu with max blocks "
@@ -2212,7 +2253,6 @@ static int __mpage_da_writepage(struct page *page,
  *
  * @mapping: address space structure to write
  * @wbc: subtract the number of written pages from *@wbc-&gt;nr_to_write
- * @get_block: the filesystem's block mapper function.
  *
  * This is a library function, which implements the writepages()
  * address_space_operation.
@@ -2223,9 +2263,6 @@ static int mpage_da_writepages(struct address_space *mapping,
 {
 	int ret;
 
-	if (!mpd-&gt;get_block)
-		return generic_writepages(mapping, wbc);
-
 	mpd-&gt;lbh.b_size = 0;
 	mpd-&gt;lbh.b_state = 0;
 	mpd-&gt;lbh.b_blocknr = 0;
@@ -2289,51 +2326,6 @@ static int ext4_da_get_block_prep(struct inode *inode, sector_t iblock,
 
 	return ret;
 }
-#define		EXT4_DELALLOC_RSVED	1
-static int ext4_da_get_block_write(struct inode *inode, sector_t iblock,
-				   struct buffer_head *bh_result, int create)
-{
-	int ret;
-	unsigned max_blocks = bh_result-&gt;b_size &gt;&gt; inode-&gt;i_blkbits;
-	loff_t disksize = EXT4_I(inode)-&gt;i_disksize;
-	handle_t *handle = NULL;
-
-	handle = ext4_journal_current_handle();
-	BUG_ON(!handle);
-	ret = ext4_get_blocks_wrap(handle, inode, iblock, max_blocks,
-			bh_result, create, 0, EXT4_DELALLOC_RSVED);
-	if (ret &gt; 0) {
-
-		bh_result-&gt;b_size = (ret &lt;&lt; inode-&gt;i_blkbits);
-
-		if (ext4_should_order_data(inode)) {
-			int retval;
-			retval = ext4_jbd2_file_inode(handle, inode);
-			if (retval)
-				/*
-				 * Failed to add inode for ordered
-				 * mode. Don't update file size
-				 */
-				return retval;
-		}
-
-		/*
-		 * Update on-disk size along with block allocation
-		 * we don't use 'extend_disksize' as size may change
-		 * within already allocated block -bzzz
-		 */
-		disksize = ((loff_t) iblock + ret) &lt;&lt; inode-&gt;i_blkbits;
-		if (disksize &gt; i_size_read(inode))
-			disksize = i_size_read(inode);
-		if (disksize &gt; EXT4_I(inode)-&gt;i_disksize) {
-			ext4_update_i_disksize(inode, disksize);
-			ret = ext4_mark_inode_dirty(handle, inode);
-			return ret;
-		}
-		ret = 0;
-	}
-	return ret;
-}
 
 static int ext4_bh_unmapped_or_delay(handle_t *handle, struct buffer_head *bh)
 {
@@ -2584,7 +2576,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 			dump_stack();
 			goto out_writepages;
 		}
-		mpd.get_block = ext4_da_get_block_write;
 		ret = mpage_da_writepages(mapping, wbc, &amp;mpd);
 
 		ext4_journal_stop(handle);</pre><hr><pre>commit 722bde6875bfb49a0c84e5601eb82dd7ac02d27c
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Feb 23 00:51:57 2009 -0500

    ext4: Add fine print for the 32000 subdirectory limit
    
    Some poeple are reading the ext4 feature list too literally and create
    dubious test cases involving very long filenames and 1k blocksize and
    then complain when they run into an htree-imposed limit.  So add fine
    print to the "fix 32000 subdirectory limit" ext4 feature.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/Documentation/filesystems/ext4.txt b/Documentation/filesystems/ext4.txt
index cec829bc7291..5c484aec2bab 100644
--- a/Documentation/filesystems/ext4.txt
+++ b/Documentation/filesystems/ext4.txt
@@ -85,7 +85,7 @@ Note: More extensive information for getting started with ext4 can be
 * extent format more robust in face of on-disk corruption due to magics,
 * internal redundancy in tree
 * improved file allocation (multi-block alloc)
-* fix 32000 subdirectory limit
+* lift 32000 subdirectory limit imposed by i_links_count[1]
 * nsec timestamps for mtime, atime, ctime, create time
 * inode version field on disk (NFSv4, Lustre)
 * reduced e2fsck time via uninit_bg feature
@@ -100,6 +100,9 @@ Note: More extensive information for getting started with ext4 can be
 * efficent new ordered mode in JBD2 and ext4(avoid using buffer head to force
   the ordering)
 
+[1] Filesystems with a block size of 1k may see a limit imposed by the
+directory hash tree having a maximum depth of two.
+
 2.2 Candidate features for future inclusion
 
 * Online defrag (patches available but not well tested)</pre><hr><pre>commit a4912123b688e057084e6557cef8924f7ae5bbde
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Mar 12 12:18:34 2009 -0400

    ext4: New inode/block allocation algorithms for flex_bg filesystems
    
    The find_group_flex() inode allocator is now only used if the
    filesystem is mounted using the "oldalloc" mount option.  It is
    replaced with the original Orlov allocator that has been updated for
    flex_bg filesystems (it should behave the same way if flex_bg is
    disabled).  The inode allocator now functions by taking into account
    each flex_bg group, instead of each block group, when deciding whether
    or not it's time to allocate a new directory into a fresh flex_bg.
    
    The block allocator has also been changed so that the first block
    group in each flex_bg is preferred for use for storing directory
    blocks.  This keeps directory blocks close together, which is good for
    speeding up e2fsck since large directories are more likely to look
    like this:
    
    debugfs:  stat /home/tytso/Maildir/cur
    Inode: 1844562   Type: directory    Mode:  0700   Flags: 0x81000
    Generation: 1132745781    Version: 0x00000000:0000ad71
    User: 15806   Group: 15806   Size: 1060864
    File ACL: 0    Directory ACL: 0
    Links: 2   Blockcount: 2072
    Fragment:  Address: 0    Number: 0    Size: 0
     ctime: 0x499c0ff4:164961f4 -- Wed Feb 18 08:41:08 2009
     atime: 0x499c0ff4:00000000 -- Wed Feb 18 08:41:08 2009
     mtime: 0x49957f51:00000000 -- Fri Feb 13 09:10:25 2009
    crtime: 0x499c0f57:00d51440 -- Wed Feb 18 08:38:31 2009
    Size of extra inode fields: 28
    BLOCKS:
    (0):7348651, (1-258):7348654-7348911
    TOTAL: 259
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 2cbfc0b04d37..096456c8559b 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -827,6 +827,12 @@ static inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)
 #define EXT4_DEF_MIN_BATCH_TIME	0
 #define EXT4_DEF_MAX_BATCH_TIME	15000 /* 15ms */
 
+/*
+ * Minimum number of groups in a flexgroup before we separate out
+ * directories into the first block group of a flexgroup
+ */
+#define EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME	4
+
 /*
  * Structure of a directory entry
  */
diff --git a/fs/ext4/ext4_i.h b/fs/ext4/ext4_i.h
index 2d516c0a22af..4ce2187123aa 100644
--- a/fs/ext4/ext4_i.h
+++ b/fs/ext4/ext4_i.h
@@ -122,6 +122,9 @@ struct ext4_inode_info {
 	struct list_head i_prealloc_list;
 	spinlock_t i_prealloc_lock;
 
+	/* ialloc */
+	ext4_group_t	i_last_alloc_group;
+
 	/* allocation reservation info for delalloc */
 	unsigned int i_reserved_data_blocks;
 	unsigned int i_reserved_meta_blocks;
diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index e0aa4fe4f596..aa3431856c9a 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -152,6 +152,8 @@ static ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,
 	ext4_fsblk_t bg_start;
 	ext4_fsblk_t last_block;
 	ext4_grpblk_t colour;
+	ext4_group_t block_group;
+	int flex_size = ext4_flex_bg_size(EXT4_SB(inode-&gt;i_sb));
 	int depth;
 
 	if (path) {
@@ -170,10 +172,31 @@ static ext4_fsblk_t ext4_ext_find_goal(struct inode *inode,
 	}
 
 	/* OK. use inode's group */
-	bg_start = (ei-&gt;i_block_group * EXT4_BLOCKS_PER_GROUP(inode-&gt;i_sb)) +
+	block_group = ei-&gt;i_block_group;
+	if (flex_size &gt;= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {
+		/*
+		 * If there are at least EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME
+		 * block groups per flexgroup, reserve the first block 
+		 * group for directories and special files.  Regular 
+		 * files will start at the second block group.  This
+		 * tends to speed up directory access and improves 
+		 * fsck times.
+		 */
+		block_group &amp;= ~(flex_size-1);
+		if (S_ISREG(inode-&gt;i_mode))
+			block_group++;
+	}
+	bg_start = (block_group * EXT4_BLOCKS_PER_GROUP(inode-&gt;i_sb)) +
 		le32_to_cpu(EXT4_SB(inode-&gt;i_sb)-&gt;s_es-&gt;s_first_data_block);
 	last_block = ext4_blocks_count(EXT4_SB(inode-&gt;i_sb)-&gt;s_es) - 1;
 
+	/*
+	 * If we are doing delayed allocation, we don't need take
+	 * colour into account.
+	 */
+	if (test_opt(inode-&gt;i_sb, DELALLOC))
+		return bg_start;
+
 	if (bg_start + EXT4_BLOCKS_PER_GROUP(inode-&gt;i_sb) &lt;= last_block)
 		colour = (current-&gt;pid % 16) *
 			(EXT4_BLOCKS_PER_GROUP(inode-&gt;i_sb) / 16);
diff --git a/fs/ext4/ialloc.c b/fs/ext4/ialloc.c
index ae3eb57dccdd..617f5a2d800a 100644
--- a/fs/ext4/ialloc.c
+++ b/fs/ext4/ialloc.c
@@ -410,6 +410,43 @@ static int find_group_flex(struct super_block *sb, struct inode *parent,
 	return 0;
 }
 
+struct orlov_stats {
+	__u32 free_inodes;
+	__u32 free_blocks;
+	__u32 used_dirs;
+};
+
+/*
+ * Helper function for Orlov's allocator; returns critical information
+ * for a particular block group or flex_bg.  If flex_size is 1, then g
+ * is a block group number; otherwise it is flex_bg number.
+ */
+void get_orlov_stats(struct super_block *sb, ext4_group_t g,
+		       int flex_size, struct orlov_stats *stats)
+{
+	struct ext4_group_desc *desc;
+	ext4_group_t		ngroups = EXT4_SB(sb)-&gt;s_groups_count;
+	int			i;
+
+	stats-&gt;free_inodes = 0;
+	stats-&gt;free_blocks = 0;
+	stats-&gt;used_dirs = 0;
+
+	g *= flex_size;
+
+	for (i = 0; i &lt; flex_size; i++) {
+		if (g &gt;= ngroups)
+			break;
+		desc = ext4_get_group_desc(sb, g++, NULL);
+		if (!desc)
+			continue;
+
+		stats-&gt;free_inodes += ext4_free_inodes_count(sb, desc);
+		stats-&gt;free_blocks += ext4_free_blks_count(sb, desc);
+		stats-&gt;used_dirs += ext4_used_dirs_count(sb, desc);
+	}
+}
+
 /*
  * Orlov's allocator for directories.
  *
@@ -425,35 +462,34 @@ static int find_group_flex(struct super_block *sb, struct inode *parent,
  * it has too many directories already (max_dirs) or
  * it has too few free inodes left (min_inodes) or
  * it has too few free blocks left (min_blocks) or
- * it's already running too large debt (max_debt).
  * Parent's group is preferred, if it doesn't satisfy these
  * conditions we search cyclically through the rest. If none
  * of the groups look good we just look for a group with more
  * free inodes than average (starting at parent's group).
- *
- * Debt is incremented each time we allocate a directory and decremented
- * when we allocate an inode, within 0--255.
  */
 
-#define INODE_COST 64
-#define BLOCK_COST 256
-
 static int find_group_orlov(struct super_block *sb, struct inode *parent,
-				ext4_group_t *group)
+			    ext4_group_t *group, int mode)
 {
 	ext4_group_t parent_group = EXT4_I(parent)-&gt;i_block_group;
 	struct ext4_sb_info *sbi = EXT4_SB(sb);
-	struct ext4_super_block *es = sbi-&gt;s_es;
 	ext4_group_t ngroups = sbi-&gt;s_groups_count;
 	int inodes_per_group = EXT4_INODES_PER_GROUP(sb);
 	unsigned int freei, avefreei;
 	ext4_fsblk_t freeb, avefreeb;
-	ext4_fsblk_t blocks_per_dir;
 	unsigned int ndirs;
-	int max_debt, max_dirs, min_inodes;
+	int max_dirs, min_inodes;
 	ext4_grpblk_t min_blocks;
-	ext4_group_t i;
+	ext4_group_t i, grp, g;
 	struct ext4_group_desc *desc;
+	struct orlov_stats stats;
+	int flex_size = ext4_flex_bg_size(sbi);
+
+	if (flex_size &gt; 1) {
+		ngroups = (ngroups + flex_size - 1) &gt;&gt;
+			sbi-&gt;s_log_groups_per_flex;
+		parent_group &gt;&gt;= sbi-&gt;s_log_groups_per_flex;
+	}
 
 	freei = percpu_counter_read_positive(&amp;sbi-&gt;s_freeinodes_counter);
 	avefreei = freei / ngroups;
@@ -462,71 +498,97 @@ static int find_group_orlov(struct super_block *sb, struct inode *parent,
 	do_div(avefreeb, ngroups);
 	ndirs = percpu_counter_read_positive(&amp;sbi-&gt;s_dirs_counter);
 
-	if ((parent == sb-&gt;s_root-&gt;d_inode) ||
-	    (EXT4_I(parent)-&gt;i_flags &amp; EXT4_TOPDIR_FL)) {
+	if (S_ISDIR(mode) &amp;&amp;
+	    ((parent == sb-&gt;s_root-&gt;d_inode) ||
+	     (EXT4_I(parent)-&gt;i_flags &amp; EXT4_TOPDIR_FL))) {
 		int best_ndir = inodes_per_group;
-		ext4_group_t grp;
 		int ret = -1;
 
 		get_random_bytes(&amp;grp, sizeof(grp));
 		parent_group = (unsigned)grp % ngroups;
 		for (i = 0; i &lt; ngroups; i++) {
-			grp = (parent_group + i) % ngroups;
-			desc = ext4_get_group_desc(sb, grp, NULL);
-			if (!desc || !ext4_free_inodes_count(sb, desc))
+			g = (parent_group + i) % ngroups;
+			get_orlov_stats(sb, g, flex_size, &amp;stats);
+			if (!stats.free_inodes)
 				continue;
-			if (ext4_used_dirs_count(sb, desc) &gt;= best_ndir)
+			if (stats.used_dirs &gt;= best_ndir)
 				continue;
-			if (ext4_free_inodes_count(sb, desc) &lt; avefreei)
+			if (stats.free_inodes &lt; avefreei)
 				continue;
-			if (ext4_free_blks_count(sb, desc) &lt; avefreeb)
+			if (stats.free_blocks &lt; avefreeb)
 				continue;
-			*group = grp;
+			grp = g;
 			ret = 0;
-			best_ndir = ext4_used_dirs_count(sb, desc);
+			best_ndir = stats.used_dirs;
+		}
+		if (ret)
+			goto fallback;
+	found_flex_bg:
+		if (flex_size == 1) {
+			*group = grp;
+			return 0;
+		}
+
+		/*
+		 * We pack inodes at the beginning of the flexgroup's
+		 * inode tables.  Block allocation decisions will do
+		 * something similar, although regular files will
+		 * start at 2nd block group of the flexgroup.  See
+		 * ext4_ext_find_goal() and ext4_find_near().
+		 */
+		grp *= flex_size;
+		for (i = 0; i &lt; flex_size; i++) {
+			if (grp+i &gt;= sbi-&gt;s_groups_count)
+				break;
+			desc = ext4_get_group_desc(sb, grp+i, NULL);
+			if (desc &amp;&amp; ext4_free_inodes_count(sb, desc)) {
+				*group = grp+i;
+				return 0;
+			}
 		}
-		if (ret == 0)
-			return ret;
 		goto fallback;
 	}
 
-	blocks_per_dir = ext4_blocks_count(es) - freeb;
-	do_div(blocks_per_dir, ndirs);
-
 	max_dirs = ndirs / ngroups + inodes_per_group / 16;
-	min_inodes = avefreei - inodes_per_group / 4;
-	min_blocks = avefreeb - EXT4_BLOCKS_PER_GROUP(sb) / 4;
-
-	max_debt = EXT4_BLOCKS_PER_GROUP(sb);
-	max_debt /= max_t(int, blocks_per_dir, BLOCK_COST);
-	if (max_debt * INODE_COST &gt; inodes_per_group)
-		max_debt = inodes_per_group / INODE_COST;
-	if (max_debt &gt; 255)
-		max_debt = 255;
-	if (max_debt == 0)
-		max_debt = 1;
+	min_inodes = avefreei - inodes_per_group*flex_size / 4;
+	if (min_inodes &lt; 1)
+		min_inodes = 1;
+	min_blocks = avefreeb - EXT4_BLOCKS_PER_GROUP(sb)*flex_size / 4;
+
+	/*
+	 * Start looking in the flex group where we last allocated an
+	 * inode for this parent directory
+	 */
+	if (EXT4_I(parent)-&gt;i_last_alloc_group != ~0) {
+		parent_group = EXT4_I(parent)-&gt;i_last_alloc_group;
+		if (flex_size &gt; 1)
+			parent_group &gt;&gt;= sbi-&gt;s_log_groups_per_flex;
+	}
 
 	for (i = 0; i &lt; ngroups; i++) {
-		*group = (parent_group + i) % ngroups;
-		desc = ext4_get_group_desc(sb, *group, NULL);
-		if (!desc || !ext4_free_inodes_count(sb, desc))
-			continue;
-		if (ext4_used_dirs_count(sb, desc) &gt;= max_dirs)
+		grp = (parent_group + i) % ngroups;
+		get_orlov_stats(sb, grp, flex_size, &amp;stats);
+		if (stats.used_dirs &gt;= max_dirs)
 			continue;
-		if (ext4_free_inodes_count(sb, desc) &lt; min_inodes)
+		if (stats.free_inodes &lt; min_inodes)
 			continue;
-		if (ext4_free_blks_count(sb, desc) &lt; min_blocks)
+		if (stats.free_blocks &lt; min_blocks)
 			continue;
-		return 0;
+		goto found_flex_bg;
 	}
 
 fallback:
+	ngroups = sbi-&gt;s_groups_count;
+	avefreei = freei / ngroups;
+	parent_group = EXT4_I(parent)-&gt;i_block_group;
 	for (i = 0; i &lt; ngroups; i++) {
-		*group = (parent_group + i) % ngroups;
-		desc = ext4_get_group_desc(sb, *group, NULL);
+		grp = (parent_group + i) % ngroups;
+		desc = ext4_get_group_desc(sb, grp, NULL);
 		if (desc &amp;&amp; ext4_free_inodes_count(sb, desc) &amp;&amp;
-			ext4_free_inodes_count(sb, desc) &gt;= avefreei)
+		    ext4_free_inodes_count(sb, desc) &gt;= avefreei) {
+			*group = grp;
 			return 0;
+		}
 	}
 
 	if (avefreei) {
@@ -542,12 +604,51 @@ static int find_group_orlov(struct super_block *sb, struct inode *parent,
 }
 
 static int find_group_other(struct super_block *sb, struct inode *parent,
-				ext4_group_t *group)
+			    ext4_group_t *group, int mode)
 {
 	ext4_group_t parent_group = EXT4_I(parent)-&gt;i_block_group;
 	ext4_group_t ngroups = EXT4_SB(sb)-&gt;s_groups_count;
 	struct ext4_group_desc *desc;
-	ext4_group_t i;
+	ext4_group_t i, last;
+	int flex_size = ext4_flex_bg_size(EXT4_SB(sb));
+
+	/*
+	 * Try to place the inode is the same flex group as its
+	 * parent.  If we can't find space, use the Orlov algorithm to
+	 * find another flex group, and store that information in the
+	 * parent directory's inode information so that use that flex
+	 * group for future allocations.
+	 */
+	if (flex_size &gt; 1) {
+		int retry = 0;
+
+	try_again:
+		parent_group &amp;= ~(flex_size-1);
+		last = parent_group + flex_size;
+		if (last &gt; ngroups)
+			last = ngroups;
+		for  (i = parent_group; i &lt; last; i++) {
+			desc = ext4_get_group_desc(sb, i, NULL);
+			if (desc &amp;&amp; ext4_free_inodes_count(sb, desc)) {
+				*group = i;
+				return 0;
+			}
+		}
+		if (!retry &amp;&amp; EXT4_I(parent)-&gt;i_last_alloc_group != ~0) {
+			retry = 1;
+			parent_group = EXT4_I(parent)-&gt;i_last_alloc_group;
+			goto try_again;
+		}
+		/*
+		 * If this didn't work, use the Orlov search algorithm
+		 * to find a new flex group; we pass in the mode to
+		 * avoid the topdir algorithms.
+		 */
+		*group = parent_group + flex_size;
+		if (*group &gt; ngroups)
+			*group = 0;
+		return find_group_orlov(sb, parent, group, mode);
+	}
 
 	/*
 	 * Try to place the inode in its parent directory
@@ -716,10 +817,10 @@ struct inode *ext4_new_inode(handle_t *handle, struct inode *dir, int mode)
 	sbi = EXT4_SB(sb);
 	es = sbi-&gt;s_es;
 
-	if (sbi-&gt;s_log_groups_per_flex) {
+	if (sbi-&gt;s_log_groups_per_flex &amp;&amp; test_opt(sb, OLDALLOC)) {
 		ret2 = find_group_flex(sb, dir, &amp;group);
 		if (ret2 == -1) {
-			ret2 = find_group_other(sb, dir, &amp;group);
+			ret2 = find_group_other(sb, dir, &amp;group, mode);
 			if (ret2 == 0 &amp;&amp; once)
 				once = 0;
 				printk(KERN_NOTICE "ext4: find_group_flex "
@@ -733,11 +834,12 @@ struct inode *ext4_new_inode(handle_t *handle, struct inode *dir, int mode)
 		if (test_opt(sb, OLDALLOC))
 			ret2 = find_group_dir(sb, dir, &amp;group);
 		else
-			ret2 = find_group_orlov(sb, dir, &amp;group);
+			ret2 = find_group_orlov(sb, dir, &amp;group, mode);
 	} else
-		ret2 = find_group_other(sb, dir, &amp;group);
+		ret2 = find_group_other(sb, dir, &amp;group, mode);
 
 got_group:
+	EXT4_I(dir)-&gt;i_last_alloc_group = group;
 	err = -ENOSPC;
 	if (ret2 == -1)
 		goto out;
@@ -894,6 +996,7 @@ struct inode *ext4_new_inode(handle_t *handle, struct inode *dir, int mode)
 	ei-&gt;i_file_acl = 0;
 	ei-&gt;i_dtime = 0;
 	ei-&gt;i_block_group = group;
+	ei-&gt;i_last_alloc_group = ~0;
 
 	ext4_set_inode_flags(inode);
 	if (IS_DIRSYNC(inode))
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 71d3ecd5db79..25811507d2b0 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -459,6 +459,8 @@ static ext4_fsblk_t ext4_find_near(struct inode *inode, Indirect *ind)
 	ext4_fsblk_t bg_start;
 	ext4_fsblk_t last_block;
 	ext4_grpblk_t colour;
+	ext4_group_t block_group;
+	int flex_size = ext4_flex_bg_size(EXT4_SB(inode-&gt;i_sb));
 
 	/* Try to find previous block */
 	for (p = ind-&gt;p - 1; p &gt;= start; p--) {
@@ -474,9 +476,22 @@ static ext4_fsblk_t ext4_find_near(struct inode *inode, Indirect *ind)
 	 * It is going to be referred to from the inode itself? OK, just put it
 	 * into the same cylinder group then.
 	 */
-	bg_start = ext4_group_first_block_no(inode-&gt;i_sb, ei-&gt;i_block_group);
+	block_group = ei-&gt;i_block_group;
+	if (flex_size &gt;= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) {
+		block_group &amp;= ~(flex_size-1);
+		if (S_ISREG(inode-&gt;i_mode))
+			block_group++;
+	}
+	bg_start = ext4_group_first_block_no(inode-&gt;i_sb, block_group);
 	last_block = ext4_blocks_count(EXT4_SB(inode-&gt;i_sb)-&gt;s_es) - 1;
 
+	/*
+	 * If we are doing delayed allocation, we don't need take
+	 * colour into account.
+	 */
+	if (test_opt(inode-&gt;i_sb, DELALLOC))
+		return bg_start;
+
 	if (bg_start + EXT4_BLOCKS_PER_GROUP(inode-&gt;i_sb) &lt;= last_block)
 		colour = (current-&gt;pid % 16) *
 			(EXT4_BLOCKS_PER_GROUP(inode-&gt;i_sb) / 16);
@@ -4287,6 +4302,7 @@ struct inode *ext4_iget(struct super_block *sb, unsigned long ino)
 	ei-&gt;i_disksize = inode-&gt;i_size;
 	inode-&gt;i_generation = le32_to_cpu(raw_inode-&gt;i_generation);
 	ei-&gt;i_block_group = iloc.block_group;
+	ei-&gt;i_last_alloc_group = ~0;
 	/*
 	 * NOTE! The in-memory inode i_data array is in little-endian order
 	 * even on big-endian machines: we do NOT byteswap the block numbers!
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index b038188bd039..b0d6022eaa67 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -1726,6 +1726,7 @@ static int ext4_mb_good_group(struct ext4_allocation_context *ac,
 {
 	unsigned free, fragments;
 	unsigned i, bits;
+	int flex_size = ext4_flex_bg_size(EXT4_SB(ac-&gt;ac_sb));
 	struct ext4_group_desc *desc;
 	struct ext4_group_info *grp = ext4_get_group_info(ac-&gt;ac_sb, group);
 
@@ -1747,6 +1748,12 @@ static int ext4_mb_good_group(struct ext4_allocation_context *ac,
 		if (desc-&gt;bg_flags &amp; cpu_to_le16(EXT4_BG_BLOCK_UNINIT))
 			return 0;
 
+		/* Avoid using the first bg of a flexgroup for data files */
+		if ((ac-&gt;ac_flags &amp; EXT4_MB_HINT_DATA) &amp;&amp;
+		    (flex_size &gt;= EXT4_FLEX_SIZE_DIR_ALLOC_SCHEME) &amp;&amp;
+		    ((group % flex_size) == 0))
+			return 0;
+
 		bits = ac-&gt;ac_sb-&gt;s_blocksize_bits + 1;
 		for (i = ac-&gt;ac_2order; i &lt;= bits; i++)
 			if (grp-&gt;bb_counters[i] &gt; 0)</pre><hr><pre>commit 05bf9e839d9de4e8a094274a0a2fd07beb47eaf1
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 21 12:13:24 2009 -0500

    ext4: Add fallback for find_group_flex
    
    This is a workaround for find_group_flex() which badly needs to be
    replaced.  One of its problems (besides ignoring the Orlov algorithm)
    is that it is a bit hyperactive about returning failure under
    suspicious circumstances.  This can lead to spurious ENOSPC failures
    even when there are inodes still available.
    
    Work around this for now by retrying the search using
    find_group_other() if find_group_flex() returns -1.  If
    find_group_other() succeeds when find_group_flex() has failed, log a
    warning message.
    
    A better block/inode allocator that will fix this problem for real has
    been queued up for the next merge window.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ialloc.c b/fs/ext4/ialloc.c
index 4fb86a0061d0..f18a919be70b 100644
--- a/fs/ext4/ialloc.c
+++ b/fs/ext4/ialloc.c
@@ -715,6 +715,13 @@ struct inode *ext4_new_inode(handle_t *handle, struct inode *dir, int mode)
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ret2 = find_group_flex(sb, dir, &amp;group);
+		if (ret2 == -1) {
+			ret2 = find_group_other(sb, dir, &amp;group);
+			if (ret2 == 0 &amp;&amp; printk_ratelimit())
+				printk(KERN_NOTICE "ext4: find_group_flex "
+				       "failed, fallback succeeded dir %lu\n",
+				       dir-&gt;i_ino);
+		}
 		goto got_group;
 	}
 </pre><hr><pre>commit 8bad4597c2d71365adfa846ea1ca6cf99161a455
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 14 21:46:54 2009 -0500

    ext4: Use unsigned int for blocksize in dx_make_map() and dx_pack_dirents()
    
    Signed-off-by: Wei Yongjun &lt;yjwei@cn.fujitsu.com&gt;
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/namei.c b/fs/ext4/namei.c
index 83410244d3ee..04824958cba5 100644
--- a/fs/ext4/namei.c
+++ b/fs/ext4/namei.c
@@ -161,12 +161,12 @@ static struct dx_frame *dx_probe(const struct qstr *d_name,
 				 struct dx_frame *frame,
 				 int *err);
 static void dx_release(struct dx_frame *frames);
-static int dx_make_map(struct ext4_dir_entry_2 *de, int size,
+static int dx_make_map(struct ext4_dir_entry_2 *de, unsigned blocksize,
 		       struct dx_hash_info *hinfo, struct dx_map_entry map[]);
 static void dx_sort_map(struct dx_map_entry *map, unsigned count);
 static struct ext4_dir_entry_2 *dx_move_dirents(char *from, char *to,
 		struct dx_map_entry *offsets, int count);
-static struct ext4_dir_entry_2* dx_pack_dirents(char *base, int size);
+static struct ext4_dir_entry_2* dx_pack_dirents(char *base, unsigned blocksize);
 static void dx_insert_block(struct dx_frame *frame,
 					u32 hash, ext4_lblk_t block);
 static int ext4_htree_next_block(struct inode *dir, __u32 hash,
@@ -713,15 +713,15 @@ int ext4_htree_fill_tree(struct file *dir_file, __u32 start_hash,
  * Create map of hash values, offsets, and sizes, stored at end of block.
  * Returns number of entries mapped.
  */
-static int dx_make_map (struct ext4_dir_entry_2 *de, int size,
-			struct dx_hash_info *hinfo, struct dx_map_entry *map_tail)
+static int dx_make_map(struct ext4_dir_entry_2 *de, unsigned blocksize,
+		       struct dx_hash_info *hinfo,
+		       struct dx_map_entry *map_tail)
 {
 	int count = 0;
 	char *base = (char *) de;
 	struct dx_hash_info h = *hinfo;
 
-	while ((char *) de &lt; base + size)
-	{
+	while ((char *) de &lt; base + blocksize) {
 		if (de-&gt;name_len &amp;&amp; de-&gt;inode) {
 			ext4fs_dirhash(de-&gt;name, de-&gt;name_len, &amp;h);
 			map_tail--;
@@ -1130,13 +1130,13 @@ dx_move_dirents(char *from, char *to, struct dx_map_entry *map, int count)
  * Compact each dir entry in the range to the minimal rec_len.
  * Returns pointer to last entry in range.
  */
-static struct ext4_dir_entry_2* dx_pack_dirents(char *base, int size)
+static struct ext4_dir_entry_2* dx_pack_dirents(char *base, unsigned blocksize)
 {
 	struct ext4_dir_entry_2 *next, *to, *prev, *de = (struct ext4_dir_entry_2 *) base;
 	unsigned rec_len = 0;
 
 	prev = to = de;
-	while ((char*)de &lt; base + size) {
+	while ((char*)de &lt; base + blocksize) {
 		next = ext4_next_entry(de);
 		if (de-&gt;inode &amp;&amp; de-&gt;name_len) {
 			rec_len = EXT4_DIR_REC_LEN(de-&gt;name_len);</pre><hr><pre>commit e187c6588d6ef3169db53c389b3de9dfde3b16cc
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Feb 6 16:23:37 2009 -0500

    ext4: remove call to ext4_group_desc() in ext4_group_used_meta_blocks()
    
    The static function ext4_group_used_meta_blocks() only has one caller,
    who already has access to the block group's group descriptor.  So it's
    better to have ext4_init_block_bitmap() pass the group descriptor to
    ext4_group_used_meta_blocks(), so it doesn't need to call
    ext4_group_desc().  Previously this function did not check if
    ext4_group_desc() returned NULL due to an error, potentially causing a
    kernel OOPS report.  This avoids the issue entirely.
    
    Signed-off-by: Thadeu Lima de Souza Cascardo &lt;cascardo@holoscopio.com&gt;
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/balloc.c b/fs/ext4/balloc.c
index 38f40d55899c..b37b12875582 100644
--- a/fs/ext4/balloc.c
+++ b/fs/ext4/balloc.c
@@ -55,7 +55,8 @@ static int ext4_block_in_group(struct super_block *sb, ext4_fsblk_t block,
 }
 
 static int ext4_group_used_meta_blocks(struct super_block *sb,
-				ext4_group_t block_group)
+				       ext4_group_t block_group,
+				       struct ext4_group_desc *gdp)
 {
 	ext4_fsblk_t tmp;
 	struct ext4_sb_info *sbi = EXT4_SB(sb);
@@ -63,10 +64,6 @@ static int ext4_group_used_meta_blocks(struct super_block *sb,
 	int used_blocks = sbi-&gt;s_itb_per_group + 2;
 
 	if (EXT4_HAS_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_FLEX_BG)) {
-		struct ext4_group_desc *gdp;
-		struct buffer_head *bh;
-
-		gdp = ext4_get_group_desc(sb, block_group, &amp;bh);
 		if (!ext4_block_in_group(sb, ext4_block_bitmap(sb, gdp),
 					block_group))
 			used_blocks--;
@@ -177,7 +174,7 @@ unsigned ext4_init_block_bitmap(struct super_block *sb, struct buffer_head *bh,
 		 */
 		mark_bitmap_end(group_blocks, sb-&gt;s_blocksize * 8, bh-&gt;b_data);
 	}
-	return free_blocks - ext4_group_used_meta_blocks(sb, block_group);
+	return free_blocks - ext4_group_used_meta_blocks(sb, block_group, gdp);
 }
 
 </pre>
    <div class="pagination">
        <a href='1_99.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><span>[100]</span><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_101.html'>Next&gt;&gt;</a>
    <div>
</body>

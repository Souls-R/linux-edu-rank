<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_35.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><span>[36]</span><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_37.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit e2bfb088fac03c0f621886a04cffc7faa2b49b1d
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun Oct 5 22:47:07 2014 -0400

    ext4: don't orphan or truncate the boot loader inode
    
    The boot loader inode (inode #5) should never be visible in the
    directory hierarchy, but it's possible if the file system is corrupted
    that there will be a directory entry that points at inode #5.  In
    order to avoid accidentally trashing it, when such a directory inode
    is opened, the inode will be marked as a bad inode, so that it's not
    possible to modify (or read) the inode from userspace.
    
    Unfortunately, when we unlink this (invalid/illegal) directory entry,
    we will put the bad inode on the ophan list, and then when try to
    unlink the directory, we don't actually remove the bad inode from the
    orphan list before freeing in-memory inode structure.  This means the
    in-memory orphan list is corrupted, leading to a kernel oops.
    
    In addition, avoid truncating a bad inode in ext4_destroy_inode(),
    since truncating the boot loader inode is not a smart thing to do.
    
    Reported-by: Sami Liedes &lt;sami.liedes@iki.fi&gt;
    Reviewed-by: Jan Kara &lt;jack@suse.cz&gt;
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: stable@vger.kernel.org

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 41c4f97c39d3..59983b28a93c 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -224,16 +224,15 @@ void ext4_evict_inode(struct inode *inode)
 		goto no_delete;
 	}
 
-	if (!is_bad_inode(inode))
-		dquot_initialize(inode);
+	if (is_bad_inode(inode))
+		goto no_delete;
+	dquot_initialize(inode);
 
 	if (ext4_should_order_data(inode))
 		ext4_begin_ordered_truncate(inode, 0);
 	truncate_inode_pages_final(&amp;inode-&gt;i_data);
 
 	WARN_ON(atomic_read(&amp;EXT4_I(inode)-&gt;i_ioend_count));
-	if (is_bad_inode(inode))
-		goto no_delete;
 
 	/*
 	 * Protect us against freezing - iput() caller didn't have to have any
diff --git a/fs/ext4/namei.c b/fs/ext4/namei.c
index 51705f8c4116..a2a9d40522d2 100644
--- a/fs/ext4/namei.c
+++ b/fs/ext4/namei.c
@@ -2544,7 +2544,7 @@ int ext4_orphan_add(handle_t *handle, struct inode *inode)
 	int err = 0, rc;
 	bool dirty = false;
 
-	if (!sbi-&gt;s_journal)
+	if (!sbi-&gt;s_journal || is_bad_inode(inode))
 		return 0;
 
 	WARN_ON_ONCE(!(inode-&gt;i_state &amp; (I_NEW | I_FREEING)) &amp;&amp;</pre><hr><pre>commit f6e63f90809946d410c42045577cb159fedabf8c
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 18 17:12:30 2014 -0400

    ext4: fold ext4_nojournal_sops into ext4_sops
    
    There's no longer any need to have a separate set of super_operations
    for nojournal mode.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 4db537b3a162..1070d6e521c6 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -1123,27 +1123,6 @@ static const struct super_operations ext4_sops = {
 	.bdev_try_to_free_page = bdev_try_to_free_page,
 };
 
-static const struct super_operations ext4_nojournal_sops = {
-	.alloc_inode	= ext4_alloc_inode,
-	.destroy_inode	= ext4_destroy_inode,
-	.write_inode	= ext4_write_inode,
-	.dirty_inode	= ext4_dirty_inode,
-	.drop_inode	= ext4_drop_inode,
-	.evict_inode	= ext4_evict_inode,
-	.sync_fs	= ext4_sync_fs,
-	.freeze_fs	= ext4_freeze,
-	.unfreeze_fs	= ext4_unfreeze,
-	.put_super	= ext4_put_super,
-	.statfs		= ext4_statfs,
-	.remount_fs	= ext4_remount,
-	.show_options	= ext4_show_options,
-#ifdef CONFIG_QUOTA
-	.quota_read	= ext4_quota_read,
-	.quota_write	= ext4_quota_write,
-#endif
-	.bdev_try_to_free_page = bdev_try_to_free_page,
-};
-
 static const struct export_operations ext4_export_ops = {
 	.fh_to_dentry = ext4_fh_to_dentry,
 	.fh_to_parent = ext4_fh_to_parent,
@@ -3941,11 +3920,7 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	/*
 	 * set up enough so that it can read an inode
 	 */
-	if (!test_opt(sb, NOLOAD) &amp;&amp;
-	    EXT4_HAS_COMPAT_FEATURE(sb, EXT4_FEATURE_COMPAT_HAS_JOURNAL))
-		sb-&gt;s_op = &amp;ext4_sops;
-	else
-		sb-&gt;s_op = &amp;ext4_nojournal_sops;
+	sb-&gt;s_op = &amp;ext4_sops;
 	sb-&gt;s_export_op = &amp;ext4_export_ops;
 	sb-&gt;s_xattr = ext4_xattr_handlers;
 #ifdef CONFIG_QUOTA</pre><hr><pre>commit bb0445765866e5b1607af81e2f48ca5a8efbeed8
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 18 17:12:02 2014 -0400

    ext4: support freezing ext2 (nojournal) file systems
    
    Through an oversight, when we added nojournal support to ext4, we
    didn't add support to allow file system freezing.  This is relatively
    easy to add, so let's do it.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Reported-by: Dexuan Cui &lt;decui@microsoft.com&gt;

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 4770c98bdb61..4db537b3a162 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -1131,6 +1131,8 @@ static const struct super_operations ext4_nojournal_sops = {
 	.drop_inode	= ext4_drop_inode,
 	.evict_inode	= ext4_evict_inode,
 	.sync_fs	= ext4_sync_fs,
+	.freeze_fs	= ext4_freeze,
+	.unfreeze_fs	= ext4_unfreeze,
 	.put_super	= ext4_put_super,
 	.statfs		= ext4_statfs,
 	.remount_fs	= ext4_remount,
@@ -4758,23 +4760,26 @@ static int ext4_freeze(struct super_block *sb)
 
 	journal = EXT4_SB(sb)-&gt;s_journal;
 
-	/* Now we set up the journal barrier. */
-	jbd2_journal_lock_updates(journal);
+	if (journal) {
+		/* Now we set up the journal barrier. */
+		jbd2_journal_lock_updates(journal);
 
-	/*
-	 * Don't clear the needs_recovery flag if we failed to flush
-	 * the journal.
-	 */
-	error = jbd2_journal_flush(journal);
-	if (error &lt; 0)
-		goto out;
+		/*
+		 * Don't clear the needs_recovery flag if we failed to
+		 * flush the journal.
+		 */
+		error = jbd2_journal_flush(journal);
+		if (error &lt; 0)
+			goto out;
+	}
 
 	/* Journal blocked and flushed, clear needs_recovery flag. */
 	EXT4_CLEAR_INCOMPAT_FEATURE(sb, EXT4_FEATURE_INCOMPAT_RECOVER);
 	error = ext4_commit_super(sb, 1);
 out:
-	/* we rely on upper layer to stop further updates */
-	jbd2_journal_unlock_updates(EXT4_SB(sb)-&gt;s_journal);
+	if (journal)
+		/* we rely on upper layer to stop further updates */
+		jbd2_journal_unlock_updates(journal);
 	return error;
 }
 </pre><hr><pre>commit bda3253043c54a705c8352096194ab6216e2e5c1
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 18 16:12:37 2014 -0400

    ext4: fold ext4_sync_fs_nojournal() into ext4_sync_fs()
    
    This allows us to eliminate duplicate code, and eventually allow us to
    also fold ext4_sops and ext4_nojournal_sops together.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 115e27d855ef..4770c98bdb61 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -70,7 +70,6 @@ static void ext4_mark_recovery_complete(struct super_block *sb,
 static void ext4_clear_journal_err(struct super_block *sb,
 				   struct ext4_super_block *es);
 static int ext4_sync_fs(struct super_block *sb, int wait);
-static int ext4_sync_fs_nojournal(struct super_block *sb, int wait);
 static int ext4_remount(struct super_block *sb, int *flags, char *data);
 static int ext4_statfs(struct dentry *dentry, struct kstatfs *buf);
 static int ext4_unfreeze(struct super_block *sb);
@@ -1131,7 +1130,7 @@ static const struct super_operations ext4_nojournal_sops = {
 	.dirty_inode	= ext4_dirty_inode,
 	.drop_inode	= ext4_drop_inode,
 	.evict_inode	= ext4_evict_inode,
-	.sync_fs	= ext4_sync_fs_nojournal,
+	.sync_fs	= ext4_sync_fs,
 	.put_super	= ext4_put_super,
 	.statfs		= ext4_statfs,
 	.remount_fs	= ext4_remount,
@@ -4718,15 +4717,19 @@ static int ext4_sync_fs(struct super_block *sb, int wait)
 	 * being sent at the end of the function. But we can skip it if
 	 * transaction_commit will do it for us.
 	 */
-	target = jbd2_get_latest_transaction(sbi-&gt;s_journal);
-	if (wait &amp;&amp; sbi-&gt;s_journal-&gt;j_flags &amp; JBD2_BARRIER &amp;&amp;
-	    !jbd2_trans_will_send_data_barrier(sbi-&gt;s_journal, target))
+	if (sbi-&gt;s_journal) {
+		target = jbd2_get_latest_transaction(sbi-&gt;s_journal);
+		if (wait &amp;&amp; sbi-&gt;s_journal-&gt;j_flags &amp; JBD2_BARRIER &amp;&amp;
+		    !jbd2_trans_will_send_data_barrier(sbi-&gt;s_journal, target))
+			needs_barrier = true;
+
+		if (jbd2_journal_start_commit(sbi-&gt;s_journal, &amp;target)) {
+			if (wait)
+				ret = jbd2_log_wait_commit(sbi-&gt;s_journal,
+							   target);
+		}
+	} else if (wait &amp;&amp; test_opt(sb, BARRIER))
 		needs_barrier = true;
-
-	if (jbd2_journal_start_commit(sbi-&gt;s_journal, &amp;target)) {
-		if (wait)
-			ret = jbd2_log_wait_commit(sbi-&gt;s_journal, target);
-	}
 	if (needs_barrier) {
 		int err;
 		err = blkdev_issue_flush(sb-&gt;s_bdev, GFP_KERNEL, NULL);
@@ -4737,19 +4740,6 @@ static int ext4_sync_fs(struct super_block *sb, int wait)
 	return ret;
 }
 
-static int ext4_sync_fs_nojournal(struct super_block *sb, int wait)
-{
-	int ret = 0;
-
-	trace_ext4_sync_fs(sb, wait);
-	flush_workqueue(EXT4_SB(sb)-&gt;rsv_conversion_wq);
-	dquot_writeback_dquots(sb, -1);
-	if (wait &amp;&amp; test_opt(sb, BARRIER))
-		ret = blkdev_issue_flush(sb-&gt;s_bdev, GFP_KERNEL, NULL);
-
-	return ret;
-}
-
 /*
  * LVM calls this function before a (read-only) snapshot is created.  This
  * gives us a chance to flush the journal completely and mark the fs clean.</pre><hr><pre>commit d26e2c4d72c2f2a38246f618480864fe3224929c
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 4 18:09:29 2014 -0400

    ext4: renumber EXT4_EX_* flags to avoid flag aliasing problems
    
    Suggested-by: Andreas Dilger &lt;adilger@dilger.ca&gt;
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 4855800fcc5d..f70c3fc94296 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -569,7 +569,6 @@ enum {
 #define EXT4_GET_BLOCKS_NO_PUT_HOLE		0x0200
 	/* Convert written extents to unwritten */
 #define EXT4_GET_BLOCKS_CONVERT_UNWRITTEN	0x0400
-/* DO NOT ASSIGN ADDITIONAL FLAG VALUES WITHOUT ADJUSTING THE FLAGS BELOW */
 
 /*
  * The bit position of these flags must not overlap with any of the
@@ -580,8 +579,8 @@ enum {
  * caching the extents when reading from the extent tree while a
  * truncate or punch hole operation is in progress.
  */
-#define EXT4_EX_NOCACHE				0x0800
-#define EXT4_EX_FORCE_CACHE			0x1000
+#define EXT4_EX_NOCACHE				0x40000000
+#define EXT4_EX_FORCE_CACHE			0x20000000
 
 /*
  * Flags used by ext4_free_blocks</pre><hr><pre>commit dc6e8d669cf5cb3ff84707c372c0a2a8a5e80845
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 4 18:09:22 2014 -0400

    jbd2: don't call get_bh() before calling __jbd2_journal_remove_checkpoint()
    
    The __jbd2_journal_remove_checkpoint() doesn't require an elevated
    b_count; indeed, until the jh structure gets released by the call to
    jbd2_journal_put_journal_head(), the bh's b_count is elevated by
    virtue of the existence of the jh structure.
    
    Suggested-by: Jan Kara &lt;jack@suse.cz&gt;
    Reviewed-by: Jan Kara &lt;jack@suse.cz&gt;
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/jbd2/checkpoint.c b/fs/jbd2/checkpoint.c
index 18c7a8d3da13..90d6091d7e18 100644
--- a/fs/jbd2/checkpoint.c
+++ b/fs/jbd2/checkpoint.c
@@ -96,15 +96,8 @@ static int __try_to_free_cp_buf(struct journal_head *jh)
 
 	if (jh-&gt;b_transaction == NULL &amp;&amp; !buffer_locked(bh) &amp;&amp;
 	    !buffer_dirty(bh) &amp;&amp; !buffer_write_io_error(bh)) {
-		/*
-		 * Get our reference so that bh cannot be freed before
-		 * we unlock it
-		 */
-		get_bh(bh);
 		JBUFFER_TRACE(jh, "remove from checkpoint list");
 		ret = __jbd2_journal_remove_checkpoint(jh) + 1;
-		BUFFER_TRACE(bh, "release");
-		__brelse(bh);
 	}
 	return ret;
 }
@@ -216,7 +209,7 @@ int jbd2_log_do_checkpoint(journal_t *journal)
 	struct buffer_head	*bh;
 	transaction_t		*transaction;
 	tid_t			this_tid;
-	int			result, batch_count = 0, done = 0;
+	int			result, batch_count = 0;
 
 	jbd_debug(1, "Start checkpoint\n");
 
@@ -291,11 +284,9 @@ int jbd2_log_do_checkpoint(journal_t *journal)
 		if (!buffer_dirty(bh)) {
 			if (unlikely(buffer_write_io_error(bh)) &amp;&amp; !result)
 				result = -EIO;
-			get_bh(bh);
 			BUFFER_TRACE(bh, "remove from checkpoint");
 			__jbd2_journal_remove_checkpoint(jh);
 			spin_unlock(&amp;journal-&gt;j_list_lock);
-			__brelse(bh);
 			goto retry;
 		}
 		/*
@@ -338,12 +329,12 @@ int jbd2_log_do_checkpoint(journal_t *journal)
 	    transaction-&gt;t_tid != this_tid)
 		goto out;
 
-	while (!done &amp;&amp; transaction-&gt;t_checkpoint_io_list) {
+	while (transaction-&gt;t_checkpoint_io_list) {
 		jh = transaction-&gt;t_checkpoint_io_list;
 		bh = jh2bh(jh);
-		get_bh(bh);
 		if (buffer_locked(bh)) {
 			spin_unlock(&amp;journal-&gt;j_list_lock);
+			get_bh(bh);
 			wait_on_buffer(bh);
 			/* the journal_head may have gone by now */
 			BUFFER_TRACE(bh, "brelse");
@@ -359,8 +350,8 @@ int jbd2_log_do_checkpoint(journal_t *journal)
 		 * know that it has been written out and so we can
 		 * drop it from the list
 		 */
-		done = __jbd2_journal_remove_checkpoint(jh);
-		__brelse(bh);
+		if (__jbd2_journal_remove_checkpoint(jh))
+			break;
 	}
 out:
 	spin_unlock(&amp;journal-&gt;j_list_lock);</pre><hr><pre>commit 754cfed6bbcfdea6afb14f2686f7f8d71e94d4e2
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 4 18:08:22 2014 -0400

    ext4: drop the EXT4_STATE_DELALLOC_RESERVED flag
    
    Having done a full regression test, we can now drop the
    DELALLOC_RESERVED state flag.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Reviewed-by: Jan Kara &lt;jack@suse.cz&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 00fd822ac6e4..4855800fcc5d 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1400,7 +1400,6 @@ enum {
 	EXT4_STATE_EXT_MIGRATE,		/* Inode is migrating */
 	EXT4_STATE_DIO_UNWRITTEN,	/* need convert on dio done*/
 	EXT4_STATE_NEWENTRY,		/* File just added to dir */
-	EXT4_STATE_DELALLOC_RESERVED,	/* blks already reserved for delalloc */
 	EXT4_STATE_DIOREAD_LOCK,	/* Disable support for dio read
 					   nolocking */
 	EXT4_STATE_MAY_INLINE_DATA,	/* may have in-inode data */
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 4a16b0cc02de..d5dd7d46844e 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -595,14 +595,6 @@ int ext4_map_blocks(handle_t *handle, struct inode *inode,
 	 */
 	down_write(&amp;EXT4_I(inode)-&gt;i_data_sem);
 
-	/*
-	 * if the caller is from delayed allocation writeout path
-	 * we have already reserved fs blocks for allocation
-	 * let the underlying get_block() function know to
-	 * avoid double accounting
-	 */
-	if (flags &amp; EXT4_GET_BLOCKS_DELALLOC_RESERVE)
-		ext4_set_inode_state(inode, EXT4_STATE_DELALLOC_RESERVED);
 	/*
 	 * We need to check for EXT4 here because migrate
 	 * could have changed the inode type in between
@@ -631,8 +623,6 @@ int ext4_map_blocks(handle_t *handle, struct inode *inode,
 			(flags &amp; EXT4_GET_BLOCKS_DELALLOC_RESERVE))
 			ext4_da_update_reserve_space(inode, retval, 1);
 	}
-	if (flags &amp; EXT4_GET_BLOCKS_DELALLOC_RESERVE)
-		ext4_clear_inode_state(inode, EXT4_STATE_DELALLOC_RESERVED);
 
 	if (retval &gt; 0) {
 		unsigned int status;
@@ -2004,12 +1994,10 @@ static int mpage_map_one_extent(handle_t *handle, struct mpage_da_data *mpd)
 	 * in data loss.  So use reserved blocks to allocate metadata if
 	 * possible.
 	 *
-	 * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE if the blocks
-	 * in question are delalloc blocks.  This affects functions in many
-	 * different parts of the allocation call path.  This flag exists
-	 * primarily because we don't want to change *many* call functions, so
-	 * ext4_map_blocks() will set the EXT4_STATE_DELALLOC_RESERVED flag
-	 * once the inode's allocation semaphore is taken.
+	 * We pass in the magic EXT4_GET_BLOCKS_DELALLOC_RESERVE if
+	 * the blocks in question are delalloc blocks.  This indicates
+	 * that the blocks and quotas has already been checked when
+	 * the data was copied into the page cache.
 	 */
 	get_blocks_flags = EXT4_GET_BLOCKS_CREATE |
 			   EXT4_GET_BLOCKS_METADATA_NOFAIL;
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 15dffdac5907..65cca2881d71 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -4410,16 +4410,6 @@ ext4_fsblk_t ext4_mb_new_blocks(handle_t *handle,
 	if (IS_NOQUOTA(ar-&gt;inode))
 		ar-&gt;flags |= EXT4_MB_USE_ROOT_BLOCKS;
 
-	/*
-	 * For delayed allocation, we could skip the ENOSPC and
-	 * EDQUOT check, as blocks and quotas have been already
-	 * reserved when data being copied into pagecache.
-	 */
-	if (ext4_test_inode_state(ar-&gt;inode, EXT4_STATE_DELALLOC_RESERVED)) {
-		WARN_ON((ar-&gt;flags &amp; EXT4_MB_DELALLOC_RESERVED) == 0);
-		ar-&gt;flags |= EXT4_MB_DELALLOC_RESERVED;
-	}
-
 	if ((ar-&gt;flags &amp; EXT4_MB_DELALLOC_RESERVED) == 0) {
 		/* Without delayed allocation we need to verify
 		 * there is enough free blocks to do block allocation</pre><hr><pre>commit e3cf5d5d9a86df1c5e413bdd3725c25a16ff854c
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 4 18:07:25 2014 -0400

    ext4: prepare to drop EXT4_STATE_DELALLOC_RESERVED
    
    The EXT4_STATE_DELALLOC_RESERVED flag was originally implemented
    because it was too hard to make sure the mballoc and get_block flags
    could be reliably passed down through all of the codepaths that end up
    calling ext4_mb_new_blocks().
    
    Since then, we have mb_flags passed down through most of the code
    paths, so getting rid of EXT4_STATE_DELALLOC_RESERVED isn't as tricky
    as it used to.
    
    This commit plumbs in the last of what is required, and then adds a
    WARN_ON check to make sure we haven't missed anything.  If this passes
    a full regression test run, we can then drop
    EXT4_STATE_DELALLOC_RESERVED.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Reviewed-by: Jan Kara &lt;jack@suse.cz&gt;

diff --git a/fs/ext4/balloc.c b/fs/ext4/balloc.c
index 581ef40fbe90..d70f154f6da3 100644
--- a/fs/ext4/balloc.c
+++ b/fs/ext4/balloc.c
@@ -636,8 +636,7 @@ ext4_fsblk_t ext4_new_meta_blocks(handle_t *handle, struct inode *inode,
 	 * Account for the allocated meta blocks.  We will never
 	 * fail EDQUOT for metdata, but we do account for it.
 	 */
-	if (!(*errp) &amp;&amp;
-	    ext4_test_inode_state(inode, EXT4_STATE_DELALLOC_RESERVED)) {
+	if (!(*errp) &amp;&amp; (flags &amp; EXT4_MB_DELALLOC_RESERVED)) {
 		spin_lock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 		spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 		dquot_alloc_block_nofail(inode,
diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index 3ac1686efff8..8170b3254767 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -1933,6 +1933,8 @@ int ext4_ext_insert_extent(handle_t *handle, struct inode *inode,
 	ext4_lblk_t next;
 	int mb_flags = 0, unwritten;
 
+	if (gb_flags &amp; EXT4_GET_BLOCKS_DELALLOC_RESERVE)
+		mb_flags |= EXT4_MB_DELALLOC_RESERVED;
 	if (unlikely(ext4_ext_get_actual_len(newext) == 0)) {
 		EXT4_ERROR_INODE(inode, "ext4_ext_get_actual_len(newext) == 0");
 		return -EIO;
@@ -2054,7 +2056,7 @@ int ext4_ext_insert_extent(handle_t *handle, struct inode *inode,
 	 * We're gonna add a new leaf in the tree.
 	 */
 	if (gb_flags &amp; EXT4_GET_BLOCKS_METADATA_NOFAIL)
-		mb_flags = EXT4_MB_USE_RESERVED;
+		mb_flags |= EXT4_MB_USE_RESERVED;
 	err = ext4_ext_create_new_leaf(handle, inode, mb_flags, gb_flags,
 				       ppath, newext);
 	if (err)
@@ -4438,6 +4440,8 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 		ar.flags = 0;
 	if (flags &amp; EXT4_GET_BLOCKS_NO_NORMALIZE)
 		ar.flags |= EXT4_MB_HINT_NOPREALLOC;
+	if (flags &amp; EXT4_GET_BLOCKS_DELALLOC_RESERVE)
+		ar.flags |= EXT4_MB_DELALLOC_RESERVED;
 	newblock = ext4_mb_new_blocks(handle, &amp;ar, &amp;err);
 	if (!newblock)
 		goto out2;
diff --git a/fs/ext4/indirect.c b/fs/ext4/indirect.c
index 69af0cd64724..36b369697a13 100644
--- a/fs/ext4/indirect.c
+++ b/fs/ext4/indirect.c
@@ -333,7 +333,9 @@ static int ext4_alloc_branch(handle_t *handle,
 			new_blocks[i] = ext4_mb_new_blocks(handle, ar, &amp;err);
 		} else
 			ar-&gt;goal = new_blocks[i] = ext4_new_meta_blocks(handle,
-				    ar-&gt;inode, ar-&gt;goal, 0, NULL, &amp;err);
+					ar-&gt;inode, ar-&gt;goal,
+					ar-&gt;flags &amp; EXT4_MB_DELALLOC_RESERVED,
+					NULL, &amp;err);
 		if (err) {
 			i--;
 			goto failed;
@@ -572,6 +574,8 @@ int ext4_ind_map_blocks(handle_t *handle, struct inode *inode,
 	ar.logical = map-&gt;m_lblk;
 	if (S_ISREG(inode-&gt;i_mode))
 		ar.flags = EXT4_MB_HINT_DATA;
+	if (flags &amp; EXT4_GET_BLOCKS_DELALLOC_RESERVE)
+		ar.flags |= EXT4_MB_DELALLOC_RESERVED;
 
 	ar.goal = ext4_find_goal(inode, map-&gt;m_lblk, partial);
 
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 8b0f9ef517d6..15dffdac5907 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -4415,9 +4415,12 @@ ext4_fsblk_t ext4_mb_new_blocks(handle_t *handle,
 	 * EDQUOT check, as blocks and quotas have been already
 	 * reserved when data being copied into pagecache.
 	 */
-	if (ext4_test_inode_state(ar-&gt;inode, EXT4_STATE_DELALLOC_RESERVED))
+	if (ext4_test_inode_state(ar-&gt;inode, EXT4_STATE_DELALLOC_RESERVED)) {
+		WARN_ON((ar-&gt;flags &amp; EXT4_MB_DELALLOC_RESERVED) == 0);
 		ar-&gt;flags |= EXT4_MB_DELALLOC_RESERVED;
-	else {
+	}
+
+	if ((ar-&gt;flags &amp; EXT4_MB_DELALLOC_RESERVED) == 0) {
 		/* Without delayed allocation we need to verify
 		 * there is enough free blocks to do block allocation
 		 * and verify allocation doesn't exceed the quota limits.
@@ -4528,8 +4531,7 @@ ext4_fsblk_t ext4_mb_new_blocks(handle_t *handle,
 	if (inquota &amp;&amp; ar-&gt;len &lt; inquota)
 		dquot_free_block(ar-&gt;inode, EXT4_C2B(sbi, inquota - ar-&gt;len));
 	if (!ar-&gt;len) {
-		if (!ext4_test_inode_state(ar-&gt;inode,
-					   EXT4_STATE_DELALLOC_RESERVED))
+		if ((ar-&gt;flags &amp; EXT4_MB_DELALLOC_RESERVED) == 0)
 			/* release all the reserved blocks if non delalloc */
 			percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
 						reserv_clstrs);
diff --git a/fs/ext4/xattr.c b/fs/ext4/xattr.c
index e7387337060c..da4df703c211 100644
--- a/fs/ext4/xattr.c
+++ b/fs/ext4/xattr.c
@@ -899,14 +899,8 @@ ext4_xattr_block_set(handle_t *handle, struct inode *inode,
 			if (!(ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS)))
 				goal = goal &amp; EXT4_MAX_BLOCK_FILE_PHYS;
 
-			/*
-			 * take i_data_sem because we will test
-			 * i_delalloc_reserved_flag in ext4_mb_new_blocks
-			 */
-			down_read(&amp;EXT4_I(inode)-&gt;i_data_sem);
 			block = ext4_new_meta_blocks(handle, inode, goal, 0,
 						     NULL, &amp;error);
-			up_read((&amp;EXT4_I(inode)-&gt;i_data_sem));
 			if (error)
 				goto cleanup;
 </pre><hr><pre>commit a521100231f816f8cdd9c8e77da14ff1e42c2b17
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 4 18:06:25 2014 -0400

    ext4: pass allocation_request struct to ext4_(alloc,splice)_branch
    
    Instead of initializing the allocation_request structure in
    ext4_alloc_branch(), set it up in ext4_ind_map_blocks(), and then pass
    it to ext4_alloc_branch() and ext4_splice_branch().
    
    This allows ext4_ind_map_blocks to pass flags in the allocation
    request structure without having to add Yet Another argument to
    ext4_alloc_branch().
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Reviewed-by: Jan Kara &lt;jack@suse.cz&gt;

diff --git a/fs/ext4/indirect.c b/fs/ext4/indirect.c
index e75f840000a0..69af0cd64724 100644
--- a/fs/ext4/indirect.c
+++ b/fs/ext4/indirect.c
@@ -318,34 +318,22 @@ static int ext4_blks_to_allocate(Indirect *branch, int k, unsigned int blks,
  *	ext4_alloc_block() (normally -ENOSPC). Otherwise we set the chain
  *	as described above and return 0.
  */
-static int ext4_alloc_branch(handle_t *handle, struct inode *inode,
-			     ext4_lblk_t iblock, int indirect_blks,
-			     int *blks, ext4_fsblk_t goal,
-			     ext4_lblk_t *offsets, Indirect *branch)
+static int ext4_alloc_branch(handle_t *handle,
+			     struct ext4_allocation_request *ar,
+			     int indirect_blks, ext4_lblk_t *offsets,
+			     Indirect *branch)
 {
-	struct ext4_allocation_request	ar;
 	struct buffer_head *		bh;
 	ext4_fsblk_t			b, new_blocks[4];
 	__le32				*p;
 	int				i, j, err, len = 1;
 
-	/*
-	 * Set up for the direct block allocation
-	 */
-	memset(&amp;ar, 0, sizeof(ar));
-	ar.inode = inode;
-	ar.len = *blks;
-	ar.logical = iblock;
-	if (S_ISREG(inode-&gt;i_mode))
-		ar.flags = EXT4_MB_HINT_DATA;
-
 	for (i = 0; i &lt;= indirect_blks; i++) {
 		if (i == indirect_blks) {
-			ar.goal = goal;
-			new_blocks[i] = ext4_mb_new_blocks(handle, &amp;ar, &amp;err);
+			new_blocks[i] = ext4_mb_new_blocks(handle, ar, &amp;err);
 		} else
-			goal = new_blocks[i] = ext4_new_meta_blocks(handle, inode,
-							goal, 0, NULL, &amp;err);
+			ar-&gt;goal = new_blocks[i] = ext4_new_meta_blocks(handle,
+				    ar-&gt;inode, ar-&gt;goal, 0, NULL, &amp;err);
 		if (err) {
 			i--;
 			goto failed;
@@ -354,7 +342,7 @@ static int ext4_alloc_branch(handle_t *handle, struct inode *inode,
 		if (i == 0)
 			continue;
 
-		bh = branch[i].bh = sb_getblk(inode-&gt;i_sb, new_blocks[i-1]);
+		bh = branch[i].bh = sb_getblk(ar-&gt;inode-&gt;i_sb, new_blocks[i-1]);
 		if (unlikely(!bh)) {
 			err = -ENOMEM;
 			goto failed;
@@ -372,7 +360,7 @@ static int ext4_alloc_branch(handle_t *handle, struct inode *inode,
 		b = new_blocks[i];
 
 		if (i == indirect_blks)
-			len = ar.len;
+			len = ar-&gt;len;
 		for (j = 0; j &lt; len; j++)
 			*p++ = cpu_to_le32(b++);
 
@@ -381,11 +369,10 @@ static int ext4_alloc_branch(handle_t *handle, struct inode *inode,
 		unlock_buffer(bh);
 
 		BUFFER_TRACE(bh, "call ext4_handle_dirty_metadata");
-		err = ext4_handle_dirty_metadata(handle, inode, bh);
+		err = ext4_handle_dirty_metadata(handle, ar-&gt;inode, bh);
 		if (err)
 			goto failed;
 	}
-	*blks = ar.len;
 	return 0;
 failed:
 	for (; i &gt;= 0; i--) {
@@ -396,10 +383,10 @@ static int ext4_alloc_branch(handle_t *handle, struct inode *inode,
 		 * existing before ext4_alloc_branch() was called.
 		 */
 		if (i &gt; 0 &amp;&amp; i != indirect_blks &amp;&amp; branch[i].bh)
-			ext4_forget(handle, 1, inode, branch[i].bh,
+			ext4_forget(handle, 1, ar-&gt;inode, branch[i].bh,
 				    branch[i].bh-&gt;b_blocknr);
-		ext4_free_blocks(handle, inode, NULL, new_blocks[i],
-				 (i == indirect_blks) ? ar.len : 1, 0);
+		ext4_free_blocks(handle, ar-&gt;inode, NULL, new_blocks[i],
+				 (i == indirect_blks) ? ar-&gt;len : 1, 0);
 	}
 	return err;
 }
@@ -419,9 +406,9 @@ static int ext4_alloc_branch(handle_t *handle, struct inode *inode,
  * inode (-&gt;i_blocks, etc.). In case of success we end up with the full
  * chain to new block and return 0.
  */
-static int ext4_splice_branch(handle_t *handle, struct inode *inode,
-			      ext4_lblk_t block, Indirect *where, int num,
-			      int blks)
+static int ext4_splice_branch(handle_t *handle,
+			      struct ext4_allocation_request *ar,
+			      Indirect *where, int num)
 {
 	int i;
 	int err = 0;
@@ -446,9 +433,9 @@ static int ext4_splice_branch(handle_t *handle, struct inode *inode,
 	 * Update the host buffer_head or inode to point to more just allocated
 	 * direct blocks blocks
 	 */
-	if (num == 0 &amp;&amp; blks &gt; 1) {
+	if (num == 0 &amp;&amp; ar-&gt;len &gt; 1) {
 		current_block = le32_to_cpu(where-&gt;key) + 1;
-		for (i = 1; i &lt; blks; i++)
+		for (i = 1; i &lt; ar-&gt;len; i++)
 			*(where-&gt;p + i) = cpu_to_le32(current_block++);
 	}
 
@@ -465,14 +452,14 @@ static int ext4_splice_branch(handle_t *handle, struct inode *inode,
 		 */
 		jbd_debug(5, "splicing indirect only\n");
 		BUFFER_TRACE(where-&gt;bh, "call ext4_handle_dirty_metadata");
-		err = ext4_handle_dirty_metadata(handle, inode, where-&gt;bh);
+		err = ext4_handle_dirty_metadata(handle, ar-&gt;inode, where-&gt;bh);
 		if (err)
 			goto err_out;
 	} else {
 		/*
 		 * OK, we spliced it into the inode itself on a direct block.
 		 */
-		ext4_mark_inode_dirty(handle, inode);
+		ext4_mark_inode_dirty(handle, ar-&gt;inode);
 		jbd_debug(5, "splicing direct\n");
 	}
 	return err;
@@ -484,11 +471,11 @@ static int ext4_splice_branch(handle_t *handle, struct inode *inode,
 		 * need to revoke the block, which is why we don't
 		 * need to set EXT4_FREE_BLOCKS_METADATA.
 		 */
-		ext4_free_blocks(handle, inode, where[i].bh, 0, 1,
+		ext4_free_blocks(handle, ar-&gt;inode, where[i].bh, 0, 1,
 				 EXT4_FREE_BLOCKS_FORGET);
 	}
-	ext4_free_blocks(handle, inode, NULL, le32_to_cpu(where[num].key),
-			 blks, 0);
+	ext4_free_blocks(handle, ar-&gt;inode, NULL, le32_to_cpu(where[num].key),
+			 ar-&gt;len, 0);
 
 	return err;
 }
@@ -525,11 +512,11 @@ int ext4_ind_map_blocks(handle_t *handle, struct inode *inode,
 			struct ext4_map_blocks *map,
 			int flags)
 {
+	struct ext4_allocation_request ar;
 	int err = -EIO;
 	ext4_lblk_t offsets[4];
 	Indirect chain[4];
 	Indirect *partial;
-	ext4_fsblk_t goal;
 	int indirect_blks;
 	int blocks_to_boundary = 0;
 	int depth;
@@ -579,7 +566,14 @@ int ext4_ind_map_blocks(handle_t *handle, struct inode *inode,
 		return -ENOSPC;
 	}
 
-	goal = ext4_find_goal(inode, map-&gt;m_lblk, partial);
+	/* Set up for the direct block allocation */
+	memset(&amp;ar, 0, sizeof(ar));
+	ar.inode = inode;
+	ar.logical = map-&gt;m_lblk;
+	if (S_ISREG(inode-&gt;i_mode))
+		ar.flags = EXT4_MB_HINT_DATA;
+
+	ar.goal = ext4_find_goal(inode, map-&gt;m_lblk, partial);
 
 	/* the number of blocks need to allocate for [d,t]indirect blocks */
 	indirect_blks = (chain + depth) - partial - 1;
@@ -588,13 +582,13 @@ int ext4_ind_map_blocks(handle_t *handle, struct inode *inode,
 	 * Next look up the indirect map to count the totoal number of
 	 * direct blocks to allocate for this branch.
 	 */
-	count = ext4_blks_to_allocate(partial, indirect_blks,
-				      map-&gt;m_len, blocks_to_boundary);
+	ar.len = ext4_blks_to_allocate(partial, indirect_blks,
+				       map-&gt;m_len, blocks_to_boundary);
+
 	/*
 	 * Block out ext4_truncate while we alter the tree
 	 */
-	err = ext4_alloc_branch(handle, inode, map-&gt;m_lblk, indirect_blks,
-				&amp;count, goal,
+	err = ext4_alloc_branch(handle, &amp;ar, indirect_blks,
 				offsets + (partial - chain), partial);
 
 	/*
@@ -605,14 +599,14 @@ int ext4_ind_map_blocks(handle_t *handle, struct inode *inode,
 	 * may need to return -EAGAIN upwards in the worst case.  --sct
 	 */
 	if (!err)
-		err = ext4_splice_branch(handle, inode, map-&gt;m_lblk,
-					 partial, indirect_blks, count);
+		err = ext4_splice_branch(handle, &amp;ar, partial, indirect_blks);
 	if (err)
 		goto cleanup;
 
 	map-&gt;m_flags |= EXT4_MAP_NEW;
 
 	ext4_update_inode_fsync_trans(handle, inode, 1);
+	count = ar.len;
 got_it:
 	map-&gt;m_flags |= EXT4_MAP_MAPPED;
 	map-&gt;m_pblk = le32_to_cpu(chain[depth-1].key);</pre><hr><pre>commit a9cfcd63e8d206ce4235c355d857c4fbdf0f4587
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Sep 3 09:33:00 2014 -0400

    ext4: avoid trying to kfree an ERR_PTR pointer
    
    Thanks to Dan Carpenter for extending smatch to find bugs like this.
    (This was found using a development version of smatch.)
    
    Fixes: 36de928641ee48b2078d3fe9514242aaa2f92013
    Reported-by: Dan Carpenter &lt;dan.carpenter@oracle.com
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: stable@vger.kernel.org

diff --git a/fs/ext4/namei.c b/fs/ext4/namei.c
index 90a3cdca3f88..603e4ebbd0ac 100644
--- a/fs/ext4/namei.c
+++ b/fs/ext4/namei.c
@@ -3240,6 +3240,7 @@ static int ext4_rename(struct inode *old_dir, struct dentry *old_dentry,
 				 &amp;new.de, &amp;new.inlined);
 	if (IS_ERR(new.bh)) {
 		retval = PTR_ERR(new.bh);
+		new.bh = NULL;
 		goto end_rename;
 	}
 	if (new.bh) {
@@ -3386,6 +3387,7 @@ static int ext4_cross_rename(struct inode *old_dir, struct dentry *old_dentry,
 				 &amp;new.de, &amp;new.inlined);
 	if (IS_ERR(new.bh)) {
 		retval = PTR_ERR(new.bh);
+		new.bh = NULL;
 		goto end_rename;
 	}
 
diff --git a/fs/ext4/resize.c b/fs/ext4/resize.c
index bb0e80f03e2e..1e43b905ff98 100644
--- a/fs/ext4/resize.c
+++ b/fs/ext4/resize.c
@@ -575,6 +575,7 @@ static int setup_new_flex_group_blocks(struct super_block *sb,
 		bh = bclean(handle, sb, block);
 		if (IS_ERR(bh)) {
 			err = PTR_ERR(bh);
+			bh = NULL;
 			goto out;
 		}
 		overhead = ext4_group_overhead_blocks(sb, group);
@@ -603,6 +604,7 @@ static int setup_new_flex_group_blocks(struct super_block *sb,
 		bh = bclean(handle, sb, block);
 		if (IS_ERR(bh)) {
 			err = PTR_ERR(bh);
+			bh = NULL;
 			goto out;
 		}
 </pre>
    <div class="pagination">
        <a href='1_35.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><span>[36]</span><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_37.html'>Next&gt;&gt;</a>
    <div>
</body>

<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Virginia Tech</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Virginia Tech</h1>
    <div class="pagination">
        <a href='10_11.html'>&lt;&lt;Prev</a><a href='10.html'>1</a><a href='10_2.html'>2</a><a href='10_3.html'>3</a><a href='10_4.html'>4</a><a href='10_5.html'>5</a><a href='10_6.html'>6</a><a href='10_7.html'>7</a><a href='10_8.html'>8</a><a href='10_9.html'>9</a><a href='10_10.html'>10</a><a href='10_11.html'>11</a><span>[12]</span>
    </div>
    <hr>
    <pre>commit 3135806358e8d3d8ac61a13f58f148d0a98a7b9b
Author: Valdis Kletnieks &lt;Valdis.Kletnieks@vt.edu&gt;
Date:   Sat Jan 14 13:21:10 2006 -0800

    [PATCH] quota: make useless quota error message informative
    
    fs/quota_v2.c can, under some conditions, issue a kernel message that says,
    in totality, 'failed read'.  This patch does the following:
    
    1) Gives a hint who issued the error message, so people reading the logs
       don't have to go grepping the entire kernel tree (with 11 false
       positives).
    
    2) Say what amount of data we expected, and actually got.
    
    Signed-off-by: Valdis Kletnieks &lt;valdis.kletnieks@vt.edu&gt;
    Cc: Jan Kara &lt;jack@ucw.cz&gt;
    Signed-off-by: Andrew Morton &lt;akpm@osdl.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/fs/quota_v2.c b/fs/quota_v2.c
index 7afcbb1b9376..a4ef91bb4f3b 100644
--- a/fs/quota_v2.c
+++ b/fs/quota_v2.c
@@ -35,7 +35,8 @@ static int v2_check_quota_file(struct super_block *sb, int type)
  
 	size = sb-&gt;s_op-&gt;quota_read(sb, type, (char *)&amp;dqhead, sizeof(struct v2_disk_dqheader), 0);
 	if (size != sizeof(struct v2_disk_dqheader)) {
-		printk("failed read\n");
+		printk("quota_v2: failed read expected=%d got=%d\n",
+			sizeof(struct v2_disk_dqheader), size);
 		return 0;
 	}
 	if (le32_to_cpu(dqhead.dqh_magic) != quota_magics[type] ||</pre><hr><pre>commit fc9c9ab22d5650977c417ef2032d02f455011b23
Author: Bharath Ramesh &lt;bramesh@vt.edu&gt;
Date:   Sat Apr 16 15:25:41 2005 -0700

    [PATCH] AYSNC IO using singals other than SIGIO
    
    A question on sigwaitinfo based IO mechanism in multithreaded applications.
    
    I am trying to use RT signals to notify me of IO events using RT signals
    instead of SIGIO in a multithreaded applications.  I noticed that there was
    some discussion on lkml during november 1999 with the subject of the
    discussion as "Signal driven IO".  In the thread I noticed that RT signals
    were being delivered to the worker thread.  I am running 2.6.10 kernel and
    I am trying to use the very same mechanism and I find that only SIGIO being
    propogated to the worker threads and RT signals only being propogated to
    the main thread and not the worker threads where I actually want them to be
    propogated too.  On further inspection I found that the following patch
    which I have attached solves the problem.
    
    I am not sure if this is a bug or feature in the kernel.
    
    
    Roland McGrath &lt;roland@redhat.com&gt; said:
    
    This relates only to fcntl F_SETSIG, which is a Linux extension.  So there is
    no POSIX issue.  When changing various things like the normal SIGIO signalling
    to do group signals, I was concerned strictly with the POSIX semantics and
    generally avoided touching things in the domain of Linux inventions.  That's
    why I didn't change this when I changed the call right next to it.  There is
    no reason I can see that F_SETSIG-requested signals shouldn't use a group
    signal like normal SIGIO does.  I'm happy to ACK this patch, there is nothing
    wrong with its change to the semantics in my book.  But neither POSIX nor I
    care a whit what F_SETSIG does.
    
    Signed-off-by: Andrew Morton &lt;akpm@osdl.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/fs/fcntl.c b/fs/fcntl.c
index c1708066bf55..3e7ab16ed154 100644
--- a/fs/fcntl.c
+++ b/fs/fcntl.c
@@ -437,7 +437,7 @@ static void send_sigio_to_task(struct task_struct *p,
 			else
 				si.si_band = band_table[reason - POLL_IN];
 			si.si_fd    = fd;
-			if (!send_sig_info(fown-&gt;signum, &amp;si, p))
+			if (!send_group_sig_info(fown-&gt;signum, &amp;si, p))
 				break;
 		/* fall-through: fall back on the old plain SIGIO signal */
 		case 0:</pre><hr><pre>commit c09b42404d29c8a9266f8186632330dc8474bf2e
Author: Matt Tolentino &lt;metolent@cs.vt.edu&gt;
Date:   Tue Jan 17 07:03:44 2006 +0100

    [PATCH] x86_64: add __meminit for memory hotplug
    
    Add __meminit to the __init lineup to ensure functions default
    to __init when memory hotplug is not enabled.  Replace __devinit
    with __meminit on functions that were changed when the memory
    hotplug code was introduced.
    
    Signed-off-by: Matt Tolentino &lt;matthew.e.tolentino@intel.com&gt;
    Signed-off-by: Andi Kleen &lt;ak@suse.de&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/arch/i386/mm/init.c b/arch/i386/mm/init.c
index 7df494b51a5b..2700f01994ba 100644
--- a/arch/i386/mm/init.c
+++ b/arch/i386/mm/init.c
@@ -268,7 +268,7 @@ static void __init permanent_kmaps_init(pgd_t *pgd_base)
 	pkmap_page_table = pte;	
 }
 
-static void __devinit free_new_highpage(struct page *page)
+static void __meminit free_new_highpage(struct page *page)
 {
 	set_page_count(page, 1);
 	__free_page(page);
diff --git a/include/linux/init.h b/include/linux/init.h
index 59008c3826cf..ff8d8b8632f4 100644
--- a/include/linux/init.h
+++ b/include/linux/init.h
@@ -241,6 +241,18 @@ void __init parse_early_param(void);
 #define __cpuexitdata	__exitdata
 #endif
 
+#ifdef CONFIG_MEMORY_HOTPLUG
+#define __meminit
+#define __meminitdata
+#define __memexit
+#define __memexitdata
+#else
+#define __meminit	__init
+#define __meminitdata __initdata
+#define __memexit __exit
+#define __memexitdata	__exitdata
+#endif
+
 /* Functions marked as __devexit may be discarded at kernel link time, depending
    on config options.  Newer versions of binutils detect references from
    retained sections to discarded sections and flag an error.  Pointers to
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 8c960b469593..c2e29743a8d1 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1735,7 +1735,7 @@ static void __init calculate_zone_totalpages(struct pglist_data *pgdat,
  * up by free_all_bootmem() once the early boot process is
  * done. Non-atomic initialization, single-pass.
  */
-void __devinit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
+void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
 		unsigned long start_pfn)
 {
 	struct page *page;
@@ -1788,7 +1788,7 @@ void zonetable_add(struct zone *zone, int nid, int zid, unsigned long pfn,
 	memmap_init_zone((size), (nid), (zone), (start_pfn))
 #endif
 
-static int __devinit zone_batchsize(struct zone *zone)
+static int __meminit zone_batchsize(struct zone *zone)
 {
 	int batch;
 
@@ -1882,7 +1882,7 @@ static struct per_cpu_pageset
  * Dynamically allocate memory for the
  * per cpu pageset array in struct zone.
  */
-static int __devinit process_zones(int cpu)
+static int __meminit process_zones(int cpu)
 {
 	struct zone *zone, *dzone;
 
@@ -1923,7 +1923,7 @@ static inline void free_zone_pagesets(int cpu)
 	}
 }
 
-static int __devinit pageset_cpuup_callback(struct notifier_block *nfb,
+static int __meminit pageset_cpuup_callback(struct notifier_block *nfb,
 		unsigned long action,
 		void *hcpu)
 {
@@ -1963,7 +1963,7 @@ void __init setup_per_cpu_pageset(void)
 
 #endif
 
-static __devinit
+static __meminit
 void zone_wait_table_init(struct zone *zone, unsigned long zone_size_pages)
 {
 	int i;
@@ -1983,7 +1983,7 @@ void zone_wait_table_init(struct zone *zone, unsigned long zone_size_pages)
 		init_waitqueue_head(zone-&gt;wait_table + i);
 }
 
-static __devinit void zone_pcp_init(struct zone *zone)
+static __meminit void zone_pcp_init(struct zone *zone)
 {
 	int cpu;
 	unsigned long batch = zone_batchsize(zone);
@@ -2001,7 +2001,7 @@ static __devinit void zone_pcp_init(struct zone *zone)
 		zone-&gt;name, zone-&gt;present_pages, batch);
 }
 
-static __devinit void init_currently_empty_zone(struct zone *zone,
+static __meminit void init_currently_empty_zone(struct zone *zone,
 		unsigned long zone_start_pfn, unsigned long size)
 {
 	struct pglist_data *pgdat = zone-&gt;zone_pgdat;</pre><hr><pre>commit 44df75e629106efcada087cead6c3f33ed6bcc60
Author: Matt Tolentino &lt;metolent@cs.vt.edu&gt;
Date:   Tue Jan 17 07:03:41 2006 +0100

    [PATCH] x86_64: add x86-64 support for memory hot-add
    
    Add x86-64 specific memory hot-add functions, Kconfig options,
    and runtime kernel page table update functions to make
    hot-add usable on x86-64 machines.  Also, fixup the nefarious
    conditional locking and exports pointed out by Andi.
    
    Tested on Intel and IBM x86-64 memory hot-add capable systems.
    
    Signed-off-by: Matt Tolentino &lt;matthew.e.tolentino@intel.com&gt;
    Signed-off-by: Andi Kleen &lt;ak@suse.de&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/arch/x86_64/Kconfig b/arch/x86_64/Kconfig
index 787feff0e8a5..2f9deca31cc9 100644
--- a/arch/x86_64/Kconfig
+++ b/arch/x86_64/Kconfig
@@ -305,7 +305,11 @@ config ARCH_DISCONTIGMEM_DEFAULT
 
 config ARCH_SPARSEMEM_ENABLE
 	def_bool y
-	depends on NUMA
+	depends on (NUMA || EXPERIMENTAL)
+
+config ARCH_MEMORY_PROBE
+	def_bool y
+	depends on MEMORY_HOTPLUG
 
 config ARCH_FLATMEM_ENABLE
 	def_bool y
@@ -315,6 +319,7 @@ source "mm/Kconfig"
 
 config HAVE_ARCH_EARLY_PFN_TO_NID
 	def_bool y
+	depends on NUMA
 
 config NR_CPUS
 	int "Maximum number of CPUs (2-256)"
diff --git a/arch/x86_64/mm/init.c b/arch/x86_64/mm/init.c
index 307d1ac0ef33..7af1742aa958 100644
--- a/arch/x86_64/mm/init.c
+++ b/arch/x86_64/mm/init.c
@@ -24,6 +24,8 @@
 #include &lt;linux/proc_fs.h&gt;
 #include &lt;linux/pci.h&gt;
 #include &lt;linux/dma-mapping.h&gt;
+#include &lt;linux/module.h&gt;
+#include &lt;linux/memory_hotplug.h&gt;
 
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/system.h&gt;
@@ -180,13 +182,19 @@ static  struct temp_map {
 	{}
 }; 
 
-static __init void *alloc_low_page(int *index, unsigned long *phys) 
+static __meminit void *alloc_low_page(int *index, unsigned long *phys)
 { 
 	struct temp_map *ti;
 	int i; 
 	unsigned long pfn = table_end++, paddr; 
 	void *adr;
 
+	if (after_bootmem) {
+		adr = (void *)get_zeroed_page(GFP_ATOMIC);
+		*phys = __pa(adr);
+		return adr;
+	}
+
 	if (pfn &gt;= end_pfn) 
 		panic("alloc_low_page: ran out of memory"); 
 	for (i = 0; temp_mappings[i].allocated; i++) {
@@ -199,55 +207,86 @@ static __init void *alloc_low_page(int *index, unsigned long *phys)
 	ti-&gt;allocated = 1; 
 	__flush_tlb(); 	       
 	adr = ti-&gt;address + ((pfn &lt;&lt; PAGE_SHIFT) &amp; ~PMD_MASK); 
+	memset(adr, 0, PAGE_SIZE);
 	*index = i; 
 	*phys  = pfn * PAGE_SIZE;  
 	return adr; 
 } 
 
-static __init void unmap_low_page(int i)
+static __meminit void unmap_low_page(int i)
 { 
-	struct temp_map *ti = &amp;temp_mappings[i];
+	struct temp_map *ti;
+
+	if (after_bootmem)
+		return;
+
+	ti = &amp;temp_mappings[i];
 	set_pmd(ti-&gt;pmd, __pmd(0));
 	ti-&gt;allocated = 0; 
 } 
 
-static void __init phys_pud_init(pud_t *pud, unsigned long address, unsigned long end)
+static void __meminit
+phys_pmd_init(pmd_t *pmd, unsigned long address, unsigned long end)
+{
+	int i;
+
+	for (i = 0; i &lt; PTRS_PER_PMD; pmd++, i++, address += PMD_SIZE) {
+		unsigned long entry;
+
+		if (address &gt; end) {
+			for (; i &lt; PTRS_PER_PMD; i++, pmd++)
+				set_pmd(pmd, __pmd(0));
+			break;
+		}
+		entry = _PAGE_NX|_PAGE_PSE|_KERNPG_TABLE|_PAGE_GLOBAL|address;
+		entry &amp;= __supported_pte_mask;
+		set_pmd(pmd, __pmd(entry));
+	}
+}
+
+static void __meminit
+phys_pmd_update(pud_t *pud, unsigned long address, unsigned long end)
+{
+	pmd_t *pmd = pmd_offset(pud, (unsigned long)__va(address));
+
+	if (pmd_none(*pmd)) {
+		spin_lock(&amp;init_mm.page_table_lock);
+		phys_pmd_init(pmd, address, end);
+		spin_unlock(&amp;init_mm.page_table_lock);
+		__flush_tlb_all();
+	}
+}
+
+static void __meminit phys_pud_init(pud_t *pud, unsigned long address, unsigned long end)
 { 
-	long i, j; 
+	long i = pud_index(address);
 
-	i = pud_index(address);
 	pud = pud + i;
+
+	if (after_bootmem &amp;&amp; pud_val(*pud)) {
+		phys_pmd_update(pud, address, end);
+		return;
+	}
+
 	for (; i &lt; PTRS_PER_PUD; pud++, i++) {
 		int map; 
 		unsigned long paddr, pmd_phys;
 		pmd_t *pmd;
 
-		paddr = address + i*PUD_SIZE;
-		if (paddr &gt;= end) { 
-			for (; i &lt; PTRS_PER_PUD; i++, pud++) 
-				set_pud(pud, __pud(0)); 
+		paddr = (address &amp; PGDIR_MASK) + i*PUD_SIZE;
+		if (paddr &gt;= end)
 			break;
-		} 
 
-		if (!e820_mapped(paddr, paddr+PUD_SIZE, 0)) { 
+		if (!after_bootmem &amp;&amp; !e820_mapped(paddr, paddr+PUD_SIZE, 0)) {
 			set_pud(pud, __pud(0)); 
 			continue;
 		} 
 
 		pmd = alloc_low_page(&amp;map, &amp;pmd_phys);
+		spin_lock(&amp;init_mm.page_table_lock);
 		set_pud(pud, __pud(pmd_phys | _KERNPG_TABLE));
-		for (j = 0; j &lt; PTRS_PER_PMD; pmd++, j++, paddr += PMD_SIZE) {
-			unsigned long pe;
-
-			if (paddr &gt;= end) { 
-				for (; j &lt; PTRS_PER_PMD; j++, pmd++)
-					set_pmd(pmd,  __pmd(0)); 
-				break;
-		}
-			pe = _PAGE_NX|_PAGE_PSE | _KERNPG_TABLE | _PAGE_GLOBAL | paddr;
-			pe &amp;= __supported_pte_mask;
-			set_pmd(pmd, __pmd(pe));
-		}
+		phys_pmd_init(pmd, paddr, end);
+		spin_unlock(&amp;init_mm.page_table_lock);
 		unmap_low_page(map);
 	}
 	__flush_tlb();
@@ -272,12 +311,15 @@ static void __init find_early_table_space(unsigned long end)
 
 	table_start &gt;&gt;= PAGE_SHIFT;
 	table_end = table_start;
+
+	early_printk("kernel direct mapping tables up to %lx @ %lx-%lx\n",
+		end, table_start &lt;&lt; PAGE_SHIFT, table_end &lt;&lt; PAGE_SHIFT);
 }
 
 /* Setup the direct mapping of the physical memory at PAGE_OFFSET.
    This runs before bootmem is initialized and gets pages directly from the 
    physical memory. To access them they are temporarily mapped. */
-void __init init_memory_mapping(unsigned long start, unsigned long end)
+void __meminit init_memory_mapping(unsigned long start, unsigned long end)
 { 
 	unsigned long next; 
 
@@ -289,7 +331,8 @@ void __init init_memory_mapping(unsigned long start, unsigned long end)
 	 * mapped.  Unfortunately this is done currently before the nodes are 
 	 * discovered.
 	 */
-	find_early_table_space(end);
+	if (!after_bootmem)
+		find_early_table_space(end);
 
 	start = (unsigned long)__va(start);
 	end = (unsigned long)__va(end);
@@ -297,20 +340,26 @@ void __init init_memory_mapping(unsigned long start, unsigned long end)
 	for (; start &lt; end; start = next) {
 		int map;
 		unsigned long pud_phys; 
-		pud_t *pud = alloc_low_page(&amp;map, &amp;pud_phys);
+		pgd_t *pgd = pgd_offset_k(start);
+		pud_t *pud;
+
+		if (after_bootmem)
+			pud = pud_offset_k(pgd, __PAGE_OFFSET);
+		else
+			pud = alloc_low_page(&amp;map, &amp;pud_phys);
+
 		next = start + PGDIR_SIZE;
 		if (next &gt; end) 
 			next = end; 
 		phys_pud_init(pud, __pa(start), __pa(next));
-		set_pgd(pgd_offset_k(start), mk_kernel_pgd(pud_phys));
+		if (!after_bootmem)
+			set_pgd(pgd_offset_k(start), mk_kernel_pgd(pud_phys));
 		unmap_low_page(map);   
 	} 
 
-	asm volatile("movq %%cr4,%0" : "=r" (mmu_cr4_features));
+	if (!after_bootmem)
+		asm volatile("movq %%cr4,%0" : "=r" (mmu_cr4_features));
 	__flush_tlb_all();
-	early_printk("kernel direct mapping tables upto %lx @ %lx-%lx\n", end, 
-	       table_start&lt;&lt;PAGE_SHIFT, 
-	       table_end&lt;&lt;PAGE_SHIFT);
 }
 
 void __cpuinit zap_low_mappings(int cpu)
@@ -385,6 +434,9 @@ size_zones(unsigned long *z, unsigned long *h,
 void __init paging_init(void)
 {
 	unsigned long zones[MAX_NR_ZONES], holes[MAX_NR_ZONES];
+
+	memory_present(0, 0, end_pfn);
+	sparse_init();
 	size_zones(zones, holes, 0, end_pfn);
 	free_area_init_node(0, NODE_DATA(0), zones,
 			    __pa(PAGE_OFFSET) &gt;&gt; PAGE_SHIFT, holes);
@@ -425,6 +477,50 @@ void __init clear_kernel_mapping(unsigned long address, unsigned long size)
 	__flush_tlb_all();
 } 
 
+/*
+ * Memory hotplug specific functions
+ * These are only for non-NUMA machines right now.
+ */
+#ifdef CONFIG_MEMORY_HOTPLUG
+
+void online_page(struct page *page)
+{
+	ClearPageReserved(page);
+	set_page_count(page, 1);
+	__free_page(page);
+	totalram_pages++;
+	num_physpages++;
+}
+
+int add_memory(u64 start, u64 size)
+{
+	struct pglist_data *pgdat = NODE_DATA(0);
+	struct zone *zone = pgdat-&gt;node_zones + MAX_NR_ZONES-2;
+	unsigned long start_pfn = start &gt;&gt; PAGE_SHIFT;
+	unsigned long nr_pages = size &gt;&gt; PAGE_SHIFT;
+	int ret;
+
+	ret = __add_pages(zone, start_pfn, nr_pages);
+	if (ret)
+		goto error;
+
+	init_memory_mapping(start, (start + size -1));
+
+	return ret;
+error:
+	printk("%s: Problem encountered in __add_pages!\n", __func__);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(add_memory);
+
+int remove_memory(u64 start, u64 size)
+{
+	return -EINVAL;
+}
+EXPORT_SYMBOL_GPL(remove_memory);
+
+#endif
+
 static struct kcore_list kcore_mem, kcore_vmalloc, kcore_kernel, kcore_modules,
 			 kcore_vsyscall;
 </pre>
    <div class="pagination">
        <a href='10_11.html'>&lt;&lt;Prev</a><a href='10.html'>1</a><a href='10_2.html'>2</a><a href='10_3.html'>3</a><a href='10_4.html'>4</a><a href='10_5.html'>5</a><a href='10_6.html'>6</a><a href='10_7.html'>7</a><a href='10_8.html'>8</a><a href='10_9.html'>9</a><a href='10_10.html'>10</a><a href='10_11.html'>11</a><span>[12]</span>
    <div>
</body>

<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_66.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><span>[67]</span><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_68.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 6f16b60690ba04cf476480a6f19b204e4b95b4a6
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 19:06:51 2011 -0400

    ext4: enable mounting bigalloc as read/write
    
    Now that we have implemented all of the changes needed for bigalloc,
    we can finally enable it!
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index a6307f7c9807..a5a7e369f719 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -1439,7 +1439,8 @@ static inline void ext4_clear_state_flags(struct ext4_inode_info *ei)
 					 EXT4_FEATURE_RO_COMPAT_DIR_NLINK | \
 					 EXT4_FEATURE_RO_COMPAT_EXTRA_ISIZE | \
 					 EXT4_FEATURE_RO_COMPAT_BTREE_DIR |\
-					 EXT4_FEATURE_RO_COMPAT_HUGE_FILE)
+					 EXT4_FEATURE_RO_COMPAT_HUGE_FILE |\
+					 EXT4_FEATURE_RO_COMPAT_BIGALLOC)
 
 /*
  * Default values for user and/or group using reserved blocks</pre><hr><pre>commit 27baebb849d46d901e756e6502b0a65a62e43771
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 19:02:51 2011 -0400

    ext4: tune mballoc's default group prealloc size for bigalloc file systems
    
    The default group preallocation size had been previously set to 512
    blocks/clusters, regardless of the block/cluster size.  This is
    probably too big for large cluster sizes.  So adjust the default so
    that it is 2 megabytes or 32 clusters, whichever is larger.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index f8e37cf2c2dd..63dd56703342 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -126,7 +126,8 @@
  * list. In case of inode preallocation we follow a list of heuristics
  * based on file size. This can be found in ext4_mb_normalize_request. If
  * we are doing a group prealloc we try to normalize the request to
- * sbi-&gt;s_mb_group_prealloc. Default value of s_mb_group_prealloc is
+ * sbi-&gt;s_mb_group_prealloc.  The default value of s_mb_group_prealloc is
+ * dependent on the cluster size; for non-bigalloc file systems, it is
  * 512 blocks. This can be tuned via
  * /sys/fs/ext4/&lt;partition&gt;/mb_group_prealloc. The value is represented in
  * terms of number of blocks. If we have mounted the file system with -O
@@ -2473,7 +2474,20 @@ int ext4_mb_init(struct super_block *sb, int needs_recovery)
 	sbi-&gt;s_mb_stats = MB_DEFAULT_STATS;
 	sbi-&gt;s_mb_stream_request = MB_DEFAULT_STREAM_THRESHOLD;
 	sbi-&gt;s_mb_order2_reqs = MB_DEFAULT_ORDER2_REQS;
-	sbi-&gt;s_mb_group_prealloc = MB_DEFAULT_GROUP_PREALLOC;
+	/*
+	 * The default group preallocation is 512, which for 4k block
+	 * sizes translates to 2 megabytes.  However for bigalloc file
+	 * systems, this is probably too big (i.e, if the cluster size
+	 * is 1 megabyte, then group preallocation size becomes half a
+	 * gigabyte!).  As a default, we will keep a two megabyte
+	 * group pralloc size for cluster sizes up to 64k, and after
+	 * that, we will force a minimum group preallocation size of
+	 * 32 clusters.  This translates to 8 megs when the cluster
+	 * size is 256k, and 32 megs when the cluster size is 1 meg,
+	 * which seems reasonable as a default.
+	 */
+	sbi-&gt;s_mb_group_prealloc = max(MB_DEFAULT_GROUP_PREALLOC &gt;&gt;
+				       sbi-&gt;s_cluster_bits, 32);
 	/*
 	 * If there is a s_stripe &gt; 1, then we set the s_mb_group_prealloc
 	 * to the lowest multiple of s_stripe which is bigger than</pre><hr><pre>commit f975d6bcc7a698a10cc755115e27d3612dcfe322
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 19:00:51 2011 -0400

    ext4: teach ext4_statfs() to deal with clusters if bigalloc is enabled
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index d7e0e045b11b..6810957e0ac7 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -4580,16 +4580,34 @@ static int ext4_remount(struct super_block *sb, int *flags, char *data)
 	return err;
 }
 
+/*
+ * Note: calculating the overhead so we can be compatible with
+ * historical BSD practice is quite difficult in the face of
+ * clusters/bigalloc.  This is because multiple metadata blocks from
+ * different block group can end up in the same allocation cluster.
+ * Calculating the exact overhead in the face of clustered allocation
+ * requires either O(all block bitmaps) in memory or O(number of block
+ * groups**2) in time.  We will still calculate the superblock for
+ * older file systems --- and if we come across with a bigalloc file
+ * system with zero in s_overhead_clusters the estimate will be close to
+ * correct especially for very large cluster sizes --- but for newer
+ * file systems, it's better to calculate this figure once at mkfs
+ * time, and store it in the superblock.  If the superblock value is
+ * present (even for non-bigalloc file systems), we will use it.
+ */
 static int ext4_statfs(struct dentry *dentry, struct kstatfs *buf)
 {
 	struct super_block *sb = dentry-&gt;d_sb;
 	struct ext4_sb_info *sbi = EXT4_SB(sb);
 	struct ext4_super_block *es = sbi-&gt;s_es;
+	struct ext4_group_desc *gdp;
 	u64 fsid;
 	s64 bfree;
 
 	if (test_opt(sb, MINIX_DF)) {
 		sbi-&gt;s_overhead_last = 0;
+	} else if (es-&gt;s_overhead_clusters) {
+		sbi-&gt;s_overhead_last = le32_to_cpu(es-&gt;s_overhead_clusters);
 	} else if (sbi-&gt;s_blocks_last != ext4_blocks_count(es)) {
 		ext4_group_t i, ngroups = ext4_get_groups_count(sb);
 		ext4_fsblk_t overhead = 0;
@@ -4604,24 +4622,16 @@ static int ext4_statfs(struct dentry *dentry, struct kstatfs *buf)
 		 * All of the blocks before first_data_block are
 		 * overhead
 		 */
-		overhead = le32_to_cpu(es-&gt;s_first_data_block);
+		overhead = EXT4_B2C(sbi, le32_to_cpu(es-&gt;s_first_data_block));
 
 		/*
-		 * Add the overhead attributed to the superblock and
-		 * block group descriptors.  If the sparse superblocks
-		 * feature is turned on, then not all groups have this.
+		 * Add the overhead found in each block group
 		 */
 		for (i = 0; i &lt; ngroups; i++) {
-			overhead += ext4_bg_has_super(sb, i) +
-				ext4_bg_num_gdb(sb, i);
+			gdp = ext4_get_group_desc(sb, i, NULL);
+			overhead += ext4_num_overhead_clusters(sb, i, gdp);
 			cond_resched();
 		}
-
-		/*
-		 * Every block group has an inode bitmap, a block
-		 * bitmap, and an inode table.
-		 */
-		overhead += ngroups * (2 + sbi-&gt;s_itb_per_group);
 		sbi-&gt;s_overhead_last = overhead;
 		smp_wmb();
 		sbi-&gt;s_blocks_last = ext4_blocks_count(es);
@@ -4629,7 +4639,8 @@ static int ext4_statfs(struct dentry *dentry, struct kstatfs *buf)
 
 	buf-&gt;f_type = EXT4_SUPER_MAGIC;
 	buf-&gt;f_bsize = sb-&gt;s_blocksize;
-	buf-&gt;f_blocks = ext4_blocks_count(es) - sbi-&gt;s_overhead_last;
+	buf-&gt;f_blocks = (ext4_blocks_count(es) -
+			 EXT4_C2B(sbi, sbi-&gt;s_overhead_last));
 	bfree = percpu_counter_sum_positive(&amp;sbi-&gt;s_freeclusters_counter) -
 		percpu_counter_sum_positive(&amp;sbi-&gt;s_dirtyclusters_counter);
 	/* prevent underflow in case that few free space is available */</pre><hr><pre>commit 24aaa8ef4e2b5764ada1fc69787e2fbd4f6276e5
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 18:58:51 2011 -0400

    ext4: convert the free_blocks field in s_flex_groups to be free_clusters
    
    Convert the free_blocks to be free_clusters to make the final revised
    bigalloc changes easier to read/understand.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index c7588366471c..d2584224c89a 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -300,7 +300,7 @@ struct ext4_group_desc
 
 struct flex_groups {
 	atomic_t free_inodes;
-	atomic_t free_blocks;
+	atomic_t free_clusters;
 	atomic_t used_dirs;
 };
 
diff --git a/fs/ext4/ialloc.c b/fs/ext4/ialloc.c
index 58115bad163f..0be5862313f0 100644
--- a/fs/ext4/ialloc.c
+++ b/fs/ext4/ialloc.c
@@ -346,7 +346,7 @@ static int find_group_flex(struct super_block *sb, struct inode *parent,
 	int flex_size = ext4_flex_bg_size(sbi);
 	ext4_group_t best_flex = parent_fbg_group;
 	int blocks_per_flex = sbi-&gt;s_blocks_per_group * flex_size;
-	int flexbg_free_blocks;
+	int flexbg_free_clusters;
 	int flex_freeb_ratio;
 	ext4_group_t n_fbg_groups;
 	ext4_group_t i;
@@ -355,8 +355,9 @@ static int find_group_flex(struct super_block *sb, struct inode *parent,
 		sbi-&gt;s_log_groups_per_flex;
 
 find_close_to_parent:
-	flexbg_free_blocks = atomic_read(&amp;flex_group[best_flex].free_blocks);
-	flex_freeb_ratio = flexbg_free_blocks * 100 / blocks_per_flex;
+	flexbg_free_clusters = atomic_read(&amp;flex_group[best_flex].free_clusters);
+	flex_freeb_ratio = EXT4_C2B(sbi, flexbg_free_clusters) * 100 /
+		blocks_per_flex;
 	if (atomic_read(&amp;flex_group[best_flex].free_inodes) &amp;&amp;
 	    flex_freeb_ratio &gt; free_block_ratio)
 		goto found_flexbg;
@@ -370,8 +371,9 @@ static int find_group_flex(struct super_block *sb, struct inode *parent,
 		if (i == parent_fbg_group || i == parent_fbg_group - 1)
 			continue;
 
-		flexbg_free_blocks = atomic_read(&amp;flex_group[i].free_blocks);
-		flex_freeb_ratio = flexbg_free_blocks * 100 / blocks_per_flex;
+		flexbg_free_clusters = atomic_read(&amp;flex_group[i].free_clusters);
+		flex_freeb_ratio = EXT4_C2B(sbi, flexbg_free_clusters) * 100 /
+			blocks_per_flex;
 
 		if (flex_freeb_ratio &gt; free_block_ratio &amp;&amp;
 		    (atomic_read(&amp;flex_group[i].free_inodes))) {
@@ -380,14 +382,14 @@ static int find_group_flex(struct super_block *sb, struct inode *parent,
 		}
 
 		if ((atomic_read(&amp;flex_group[best_flex].free_inodes) == 0) ||
-		    ((atomic_read(&amp;flex_group[i].free_blocks) &gt;
-		      atomic_read(&amp;flex_group[best_flex].free_blocks)) &amp;&amp;
+		    ((atomic_read(&amp;flex_group[i].free_clusters) &gt;
+		      atomic_read(&amp;flex_group[best_flex].free_clusters)) &amp;&amp;
 		     atomic_read(&amp;flex_group[i].free_inodes)))
 			best_flex = i;
 	}
 
 	if (!atomic_read(&amp;flex_group[best_flex].free_inodes) ||
-	    !atomic_read(&amp;flex_group[best_flex].free_blocks))
+	    !atomic_read(&amp;flex_group[best_flex].free_clusters))
 		return -1;
 
 found_flexbg:
@@ -407,7 +409,7 @@ static int find_group_flex(struct super_block *sb, struct inode *parent,
 
 struct orlov_stats {
 	__u32 free_inodes;
-	__u32 free_blocks;
+	__u32 free_clusters;
 	__u32 used_dirs;
 };
 
@@ -424,7 +426,7 @@ static void get_orlov_stats(struct super_block *sb, ext4_group_t g,
 
 	if (flex_size &gt; 1) {
 		stats-&gt;free_inodes = atomic_read(&amp;flex_group[g].free_inodes);
-		stats-&gt;free_blocks = atomic_read(&amp;flex_group[g].free_blocks);
+		stats-&gt;free_clusters = atomic_read(&amp;flex_group[g].free_clusters);
 		stats-&gt;used_dirs = atomic_read(&amp;flex_group[g].used_dirs);
 		return;
 	}
@@ -432,11 +434,11 @@ static void get_orlov_stats(struct super_block *sb, ext4_group_t g,
 	desc = ext4_get_group_desc(sb, g, NULL);
 	if (desc) {
 		stats-&gt;free_inodes = ext4_free_inodes_count(sb, desc);
-		stats-&gt;free_blocks = ext4_free_blks_count(sb, desc);
+		stats-&gt;free_clusters = ext4_free_blks_count(sb, desc);
 		stats-&gt;used_dirs = ext4_used_dirs_count(sb, desc);
 	} else {
 		stats-&gt;free_inodes = 0;
-		stats-&gt;free_blocks = 0;
+		stats-&gt;free_clusters = 0;
 		stats-&gt;used_dirs = 0;
 	}
 }
@@ -471,10 +473,10 @@ static int find_group_orlov(struct super_block *sb, struct inode *parent,
 	ext4_group_t real_ngroups = ext4_get_groups_count(sb);
 	int inodes_per_group = EXT4_INODES_PER_GROUP(sb);
 	unsigned int freei, avefreei;
-	ext4_fsblk_t freeb, avefreeb;
+	ext4_fsblk_t freeb, avefreec;
 	unsigned int ndirs;
 	int max_dirs, min_inodes;
-	ext4_grpblk_t min_blocks;
+	ext4_grpblk_t min_clusters;
 	ext4_group_t i, grp, g, ngroups;
 	struct ext4_group_desc *desc;
 	struct orlov_stats stats;
@@ -492,8 +494,8 @@ static int find_group_orlov(struct super_block *sb, struct inode *parent,
 	avefreei = freei / ngroups;
 	freeb = EXT4_C2B(sbi,
 		percpu_counter_read_positive(&amp;sbi-&gt;s_freeclusters_counter));
-	avefreeb = freeb;
-	do_div(avefreeb, ngroups);
+	avefreec = freeb;
+	do_div(avefreec, ngroups);
 	ndirs = percpu_counter_read_positive(&amp;sbi-&gt;s_dirs_counter);
 
 	if (S_ISDIR(mode) &amp;&amp;
@@ -519,7 +521,7 @@ static int find_group_orlov(struct super_block *sb, struct inode *parent,
 				continue;
 			if (stats.free_inodes &lt; avefreei)
 				continue;
-			if (stats.free_blocks &lt; avefreeb)
+			if (stats.free_clusters &lt; avefreec)
 				continue;
 			grp = g;
 			ret = 0;
@@ -557,7 +559,7 @@ static int find_group_orlov(struct super_block *sb, struct inode *parent,
 	min_inodes = avefreei - inodes_per_group*flex_size / 4;
 	if (min_inodes &lt; 1)
 		min_inodes = 1;
-	min_blocks = avefreeb - EXT4_BLOCKS_PER_GROUP(sb)*flex_size / 4;
+	min_clusters = avefreec - EXT4_CLUSTERS_PER_GROUP(sb)*flex_size / 4;
 
 	/*
 	 * Start looking in the flex group where we last allocated an
@@ -576,7 +578,7 @@ static int find_group_orlov(struct super_block *sb, struct inode *parent,
 			continue;
 		if (stats.free_inodes &lt; min_inodes)
 			continue;
-		if (stats.free_blocks &lt; min_blocks)
+		if (stats.free_clusters &lt; min_clusters)
 			continue;
 		goto found_flex_bg;
 	}
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 4a38b65bd564..f8e37cf2c2dd 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2847,7 +2847,7 @@ ext4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,
 		ext4_group_t flex_group = ext4_flex_group(sbi,
 							  ac-&gt;ac_b_ex.fe_group);
 		atomic_sub(ac-&gt;ac_b_ex.fe_len,
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_blocks);
+			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 	}
 
 	err = ext4_handle_dirty_metadata(handle, NULL, bitmap_bh);
@@ -4696,7 +4696,8 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
-		atomic_add(count, &amp;sbi-&gt;s_flex_groups[flex_group].free_blocks);
+		atomic_add(count_clusters,
+			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 	}
 
 	ext4_mb_unload_buddy(&amp;e4b);
@@ -4839,8 +4840,8 @@ int ext4_group_add_blocks(handle_t *handle, struct super_block *sb,
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
-		atomic_add(blocks_freed,
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_blocks);
+		atomic_add(EXT4_B2C(sbi, blocks_freed),
+			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 	}
 
 	ext4_mb_unload_buddy(&amp;e4b);
diff --git a/fs/ext4/resize.c b/fs/ext4/resize.c
index a324a537f2dc..95a09ddca3b9 100644
--- a/fs/ext4/resize.c
+++ b/fs/ext4/resize.c
@@ -946,8 +946,8 @@ int ext4_group_add(struct super_block *sb, struct ext4_new_group_data *input)
 	    sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group;
 		flex_group = ext4_flex_group(sbi, input-&gt;group);
-		atomic_add(input-&gt;free_blocks_count,
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_blocks);
+		atomic_add(EXT4_B2C(sbi, input-&gt;free_blocks_count),
+			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 		atomic_add(EXT4_INODES_PER_GROUP(sb),
 			   &amp;sbi-&gt;s_flex_groups[flex_group].free_inodes);
 	}
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index f81e7e791655..d7e0e045b11b 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -2035,7 +2035,7 @@ static int ext4_fill_flex_info(struct super_block *sb)
 		atomic_add(ext4_free_inodes_count(sb, gdp),
 			   &amp;sbi-&gt;s_flex_groups[flex_group].free_inodes);
 		atomic_add(ext4_free_blks_count(sb, gdp),
-			   &amp;sbi-&gt;s_flex_groups[flex_group].free_blocks);
+			   &amp;sbi-&gt;s_flex_groups[flex_group].free_clusters);
 		atomic_add(ext4_used_dirs_count(sb, gdp),
 			   &amp;sbi-&gt;s_flex_groups[flex_group].used_dirs);
 	}</pre><hr><pre>commit 5704265188ffe4290ed73b3cb685206c3ed8209d
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 18:56:51 2011 -0400

    ext4: convert s_{dirty,free}blocks_counter to s_{dirty,free}clusters_counter
    
    Convert the percpu counters s_dirtyblocks_counter and
    s_freeblocks_counter in struct ext4_super_info to be
    s_dirtyclusters_counter and s_freeclusters_counter.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/balloc.c b/fs/ext4/balloc.c
index 89abf1f7b253..9080a857cda9 100644
--- a/fs/ext4/balloc.c
+++ b/fs/ext4/balloc.c
@@ -414,16 +414,16 @@ static int ext4_has_free_blocks(struct ext4_sb_info *sbi,
 				s64 nblocks, unsigned int flags)
 {
 	s64 free_blocks, dirty_blocks, root_blocks;
-	struct percpu_counter *fbc = &amp;sbi-&gt;s_freeblocks_counter;
-	struct percpu_counter *dbc = &amp;sbi-&gt;s_dirtyblocks_counter;
+	struct percpu_counter *fcc = &amp;sbi-&gt;s_freeclusters_counter;
+	struct percpu_counter *dbc = &amp;sbi-&gt;s_dirtyclusters_counter;
 
-	free_blocks  = percpu_counter_read_positive(fbc);
+	free_blocks  = percpu_counter_read_positive(fcc);
 	dirty_blocks = percpu_counter_read_positive(dbc);
 	root_blocks = ext4_r_blocks_count(sbi-&gt;s_es);
 
 	if (free_blocks - (nblocks + root_blocks + dirty_blocks) &lt;
 						EXT4_FREEBLOCKS_WATERMARK) {
-		free_blocks  = percpu_counter_sum_positive(fbc);
+		free_blocks  = EXT4_C2B(sbi, percpu_counter_sum_positive(fcc));
 		dirty_blocks = percpu_counter_sum_positive(dbc);
 	}
 	/* Check whether we have space after
@@ -449,7 +449,7 @@ int ext4_claim_free_blocks(struct ext4_sb_info *sbi,
 			   s64 nblocks, unsigned int flags)
 {
 	if (ext4_has_free_blocks(sbi, nblocks, flags)) {
-		percpu_counter_add(&amp;sbi-&gt;s_dirtyblocks_counter, nblocks);
+		percpu_counter_add(&amp;sbi-&gt;s_dirtyclusters_counter, nblocks);
 		return 0;
 	} else
 		return -ENOSPC;
diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 030bfc1cb59d..c7588366471c 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -855,6 +855,7 @@ struct ext4_inode_info {
 	ext4_group_t	i_last_alloc_group;
 
 	/* allocation reservation info for delalloc */
+	/* In case of bigalloc, these refer to clusters rather than blocks */
 	unsigned int i_reserved_data_blocks;
 	unsigned int i_reserved_meta_blocks;
 	unsigned int i_allocated_meta_blocks;
@@ -1144,10 +1145,10 @@ struct ext4_sb_info {
 	u32 s_hash_seed[4];
 	int s_def_hash_version;
 	int s_hash_unsigned;	/* 3 if hash should be signed, 0 if not */
-	struct percpu_counter s_freeblocks_counter;
+	struct percpu_counter s_freeclusters_counter;
 	struct percpu_counter s_freeinodes_counter;
 	struct percpu_counter s_dirs_counter;
-	struct percpu_counter s_dirtyblocks_counter;
+	struct percpu_counter s_dirtyclusters_counter;
 	struct blockgroup_lock *s_blockgroup_lock;
 	struct proc_dir_entry *s_proc;
 	struct kobject s_kobj;
diff --git a/fs/ext4/ialloc.c b/fs/ext4/ialloc.c
index b7a8130d0af4..58115bad163f 100644
--- a/fs/ext4/ialloc.c
+++ b/fs/ext4/ialloc.c
@@ -490,7 +490,8 @@ static int find_group_orlov(struct super_block *sb, struct inode *parent,
 
 	freei = percpu_counter_read_positive(&amp;sbi-&gt;s_freeinodes_counter);
 	avefreei = freei / ngroups;
-	freeb = percpu_counter_read_positive(&amp;sbi-&gt;s_freeblocks_counter);
+	freeb = EXT4_C2B(sbi,
+		percpu_counter_read_positive(&amp;sbi-&gt;s_freeclusters_counter));
 	avefreeb = freeb;
 	do_div(avefreeb, ngroups);
 	ndirs = percpu_counter_read_positive(&amp;sbi-&gt;s_dirs_counter);
diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 904a9a623dab..40f51aae42fe 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -281,7 +281,7 @@ void ext4_da_update_reserve_space(struct inode *inode,
 	/* Update per-inode reservations */
 	ei-&gt;i_reserved_data_blocks -= used;
 	ei-&gt;i_reserved_meta_blocks -= ei-&gt;i_allocated_meta_blocks;
-	percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter,
+	percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
 			   used + ei-&gt;i_allocated_meta_blocks);
 	ei-&gt;i_allocated_meta_blocks = 0;
 
@@ -291,7 +291,7 @@ void ext4_da_update_reserve_space(struct inode *inode,
 		 * only when we have written all of the delayed
 		 * allocation blocks.
 		 */
-		percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter,
+		percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
 				   ei-&gt;i_reserved_meta_blocks);
 		ei-&gt;i_reserved_meta_blocks = 0;
 		ei-&gt;i_da_metadata_calc_len = 0;
@@ -1119,14 +1119,14 @@ static void ext4_da_release_space(struct inode *inode, int to_free)
 		 * only when we have written all of the delayed
 		 * allocation blocks.
 		 */
-		percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter,
+		percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
 				   ei-&gt;i_reserved_meta_blocks);
 		ei-&gt;i_reserved_meta_blocks = 0;
 		ei-&gt;i_da_metadata_calc_len = 0;
 	}
 
 	/* update fs dirty data blocks counter */
-	percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, to_free);
+	percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter, to_free);
 
 	spin_unlock(&amp;EXT4_I(inode)-&gt;i_block_reservation_lock);
 
@@ -1349,9 +1349,10 @@ static void ext4_print_free_blocks(struct inode *inode)
 	       ext4_count_free_blocks(inode-&gt;i_sb));
 	printk(KERN_CRIT "Free/Dirty block details\n");
 	printk(KERN_CRIT "free_blocks=%lld\n",
-	       (long long) percpu_counter_sum(&amp;sbi-&gt;s_freeblocks_counter));
+	       (long long) EXT4_C2B(EXT4_SB(inode-&gt;i_sb),
+		percpu_counter_sum(&amp;sbi-&gt;s_freeclusters_counter)));
 	printk(KERN_CRIT "dirty_blocks=%lld\n",
-	       (long long) percpu_counter_sum(&amp;sbi-&gt;s_dirtyblocks_counter));
+	       (long long) percpu_counter_sum(&amp;sbi-&gt;s_dirtyclusters_counter));
 	printk(KERN_CRIT "Block reservation details\n");
 	printk(KERN_CRIT "i_reserved_data_blocks=%u\n",
 	       EXT4_I(inode)-&gt;i_reserved_data_blocks);
@@ -2226,8 +2227,9 @@ static int ext4_nonda_switch(struct super_block *sb)
 	 * Delalloc need an accurate free block accounting. So switch
 	 * to non delalloc when we are near to error range.
 	 */
-	free_blocks  = percpu_counter_read_positive(&amp;sbi-&gt;s_freeblocks_counter);
-	dirty_blocks = percpu_counter_read_positive(&amp;sbi-&gt;s_dirtyblocks_counter);
+	free_blocks  = EXT4_C2B(sbi,
+		percpu_counter_read_positive(&amp;sbi-&gt;s_freeclusters_counter));
+	dirty_blocks = percpu_counter_read_positive(&amp;sbi-&gt;s_dirtyclusters_counter);
 	if (2 * free_blocks &lt; 3 * dirty_blocks ||
 		free_blocks &lt; (dirty_blocks + EXT4_FREEBLOCKS_WATERMARK)) {
 		/*
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 57ce6960e940..4a38b65bd564 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2834,13 +2834,14 @@ ext4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,
 	gdp-&gt;bg_checksum = ext4_group_desc_csum(sbi, ac-&gt;ac_b_ex.fe_group, gdp);
 
 	ext4_unlock_group(sb, ac-&gt;ac_b_ex.fe_group);
-	percpu_counter_sub(&amp;sbi-&gt;s_freeblocks_counter, ac-&gt;ac_b_ex.fe_len);
+	percpu_counter_sub(&amp;sbi-&gt;s_freeclusters_counter, ac-&gt;ac_b_ex.fe_len);
 	/*
 	 * Now reduce the dirty block count also. Should not go negative
 	 */
 	if (!(ac-&gt;ac_flags &amp; EXT4_MB_DELALLOC_RESERVED))
 		/* release all the reserved blocks if non delalloc */
-		percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, reserv_clstrs);
+		percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
+				   reserv_clstrs);
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi,
@@ -4384,7 +4385,7 @@ ext4_fsblk_t ext4_mb_new_blocks(handle_t *handle,
 		if (!ext4_test_inode_state(ar-&gt;inode,
 					   EXT4_STATE_DELALLOC_RESERVED))
 			/* release all the reserved blocks if non delalloc */
-			percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter,
+			percpu_counter_sub(&amp;sbi-&gt;s_dirtyclusters_counter,
 						reserv_clstrs);
 	}
 
@@ -4691,7 +4692,7 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 	ext4_free_blks_set(sb, gdp, ret);
 	gdp-&gt;bg_checksum = ext4_group_desc_csum(sbi, block_group, gdp);
 	ext4_unlock_group(sb, block_group);
-	percpu_counter_add(&amp;sbi-&gt;s_freeblocks_counter, count);
+	percpu_counter_add(&amp;sbi-&gt;s_freeclusters_counter, count_clusters);
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
@@ -4833,7 +4834,8 @@ int ext4_group_add_blocks(handle_t *handle, struct super_block *sb,
 	ext4_free_blks_set(sb, desc, blk_free_count);
 	desc-&gt;bg_checksum = ext4_group_desc_csum(sbi, block_group, desc);
 	ext4_unlock_group(sb, block_group);
-	percpu_counter_add(&amp;sbi-&gt;s_freeblocks_counter, blocks_freed);
+	percpu_counter_add(&amp;sbi-&gt;s_freeclusters_counter,
+			   EXT4_B2C(sbi, blocks_freed));
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi, block_group);
diff --git a/fs/ext4/resize.c b/fs/ext4/resize.c
index 707d3f16f7ce..a324a537f2dc 100644
--- a/fs/ext4/resize.c
+++ b/fs/ext4/resize.c
@@ -937,8 +937,8 @@ int ext4_group_add(struct super_block *sb, struct ext4_new_group_data *input)
 		input-&gt;reserved_blocks);
 
 	/* Update the free space counts */
-	percpu_counter_add(&amp;sbi-&gt;s_freeblocks_counter,
-			   input-&gt;free_blocks_count);
+	percpu_counter_add(&amp;sbi-&gt;s_freeclusters_counter,
+			   EXT4_B2C(sbi, input-&gt;free_blocks_count));
 	percpu_counter_add(&amp;sbi-&gt;s_freeinodes_counter,
 			   EXT4_INODES_PER_GROUP(sb));
 
diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 25a4bfe3f39f..f81e7e791655 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -837,10 +837,10 @@ static void ext4_put_super(struct super_block *sb)
 		brelse(sbi-&gt;s_group_desc[i]);
 	ext4_kvfree(sbi-&gt;s_group_desc);
 	ext4_kvfree(sbi-&gt;s_flex_groups);
-	percpu_counter_destroy(&amp;sbi-&gt;s_freeblocks_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_freeclusters_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_freeinodes_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_dirs_counter);
-	percpu_counter_destroy(&amp;sbi-&gt;s_dirtyblocks_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_dirtyclusters_counter);
 	brelse(sbi-&gt;s_sbh);
 #ifdef CONFIG_QUOTA
 	for (i = 0; i &lt; MAXQUOTAS; i++)
@@ -2473,7 +2473,7 @@ static ssize_t delayed_allocation_blocks_show(struct ext4_attr *a,
 					      char *buf)
 {
 	return snprintf(buf, PAGE_SIZE, "%llu\n",
-			(s64) percpu_counter_sum(&amp;sbi-&gt;s_dirtyblocks_counter));
+		(s64) percpu_counter_sum(&amp;sbi-&gt;s_dirtyclusters_counter));
 }
 
 static ssize_t session_write_kbytes_show(struct ext4_attr *a,
@@ -3575,7 +3575,7 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	sbi-&gt;s_err_report.function = print_daily_error_info;
 	sbi-&gt;s_err_report.data = (unsigned long) sb;
 
-	err = percpu_counter_init(&amp;sbi-&gt;s_freeblocks_counter,
+	err = percpu_counter_init(&amp;sbi-&gt;s_freeclusters_counter,
 			ext4_count_free_blocks(sb));
 	if (!err) {
 		err = percpu_counter_init(&amp;sbi-&gt;s_freeinodes_counter,
@@ -3586,7 +3586,7 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 				ext4_count_dirs(sb));
 	}
 	if (!err) {
-		err = percpu_counter_init(&amp;sbi-&gt;s_dirtyblocks_counter, 0);
+		err = percpu_counter_init(&amp;sbi-&gt;s_dirtyclusters_counter, 0);
 	}
 	if (err) {
 		ext4_msg(sb, KERN_ERR, "insufficient memory");
@@ -3701,13 +3701,13 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	 * The journal may have updated the bg summary counts, so we
 	 * need to update the global counters.
 	 */
-	percpu_counter_set(&amp;sbi-&gt;s_freeblocks_counter,
+	percpu_counter_set(&amp;sbi-&gt;s_freeclusters_counter,
 			   ext4_count_free_blocks(sb));
 	percpu_counter_set(&amp;sbi-&gt;s_freeinodes_counter,
 			   ext4_count_free_inodes(sb));
 	percpu_counter_set(&amp;sbi-&gt;s_dirs_counter,
 			   ext4_count_dirs(sb));
-	percpu_counter_set(&amp;sbi-&gt;s_dirtyblocks_counter, 0);
+	percpu_counter_set(&amp;sbi-&gt;s_dirtyclusters_counter, 0);
 
 no_journal:
 	/*
@@ -3847,10 +3847,10 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 	del_timer(&amp;sbi-&gt;s_err_report);
 	if (sbi-&gt;s_flex_groups)
 		ext4_kvfree(sbi-&gt;s_flex_groups);
-	percpu_counter_destroy(&amp;sbi-&gt;s_freeblocks_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_freeclusters_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_freeinodes_counter);
 	percpu_counter_destroy(&amp;sbi-&gt;s_dirs_counter);
-	percpu_counter_destroy(&amp;sbi-&gt;s_dirtyblocks_counter);
+	percpu_counter_destroy(&amp;sbi-&gt;s_dirtyclusters_counter);
 	if (sbi-&gt;s_mmp_tsk)
 		kthread_stop(sbi-&gt;s_mmp_tsk);
 failed_mount2:
@@ -4173,8 +4173,9 @@ static int ext4_commit_super(struct super_block *sb, int sync)
 	else
 		es-&gt;s_kbytes_written =
 			cpu_to_le64(EXT4_SB(sb)-&gt;s_kbytes_written);
-	ext4_free_blocks_count_set(es, percpu_counter_sum_positive(
-					   &amp;EXT4_SB(sb)-&gt;s_freeblocks_counter));
+	ext4_free_blocks_count_set(es,
+			EXT4_C2B(EXT4_SB(sb), percpu_counter_sum_positive(
+				&amp;EXT4_SB(sb)-&gt;s_freeclusters_counter)));
 	es-&gt;s_free_inodes_count =
 		cpu_to_le32(percpu_counter_sum_positive(
 				&amp;EXT4_SB(sb)-&gt;s_freeinodes_counter));
@@ -4629,10 +4630,10 @@ static int ext4_statfs(struct dentry *dentry, struct kstatfs *buf)
 	buf-&gt;f_type = EXT4_SUPER_MAGIC;
 	buf-&gt;f_bsize = sb-&gt;s_blocksize;
 	buf-&gt;f_blocks = ext4_blocks_count(es) - sbi-&gt;s_overhead_last;
-	bfree = percpu_counter_sum_positive(&amp;sbi-&gt;s_freeblocks_counter) -
-		       percpu_counter_sum_positive(&amp;sbi-&gt;s_dirtyblocks_counter);
+	bfree = percpu_counter_sum_positive(&amp;sbi-&gt;s_freeclusters_counter) -
+		percpu_counter_sum_positive(&amp;sbi-&gt;s_dirtyclusters_counter);
 	/* prevent underflow in case that few free space is available */
-	buf-&gt;f_bfree = max_t(s64, bfree, 0);
+	buf-&gt;f_bfree = EXT4_C2B(sbi, max_t(s64, bfree, 0));
 	buf-&gt;f_bavail = buf-&gt;f_bfree - ext4_r_blocks_count(es);
 	if (buf-&gt;f_bfree &lt; ext4_r_blocks_count(es))
 		buf-&gt;f_bavail = 0;</pre><hr><pre>commit 0aa060000e83ca3d09ddc446a7174fb0820d99bc
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 18:54:51 2011 -0400

    ext4: teach ext4_ext_truncate() about the bigalloc feature
    
    When we are truncating (as opposed unlinking) a file, we need to worry
    about partial truncates of a file, especially in the light of sparse
    files.  The changes here make sure that arbitrary truncates of sparse
    files works correctly.  Yeah, it's messy.
    
    Note that these functions will need to be revisted when the punch
    ioctl is integrated --- in fact this commit will probably have merge
    conflicts with the punch changes which Allison Henders and the IBM LTC
    have been working on.  I will need to fix this up when either patch
    hits mainline.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index bd42ab29efec..cd4479c08031 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -2202,14 +2202,39 @@ int ext4_ext_index_trans_blocks(struct inode *inode, int nrblocks, int chunk)
 }
 
 static int ext4_remove_blocks(handle_t *handle, struct inode *inode,
-				struct ext4_extent *ex,
-				ext4_lblk_t from, ext4_lblk_t to)
+			      struct ext4_extent *ex,
+			      ext4_fsblk_t *partial_cluster,
+			      ext4_lblk_t from, ext4_lblk_t to)
 {
+	struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
 	unsigned short ee_len =  ext4_ext_get_actual_len(ex);
+	ext4_fsblk_t pblk;
 	int flags = EXT4_FREE_BLOCKS_FORGET;
 
 	if (S_ISDIR(inode-&gt;i_mode) || S_ISLNK(inode-&gt;i_mode))
 		flags |= EXT4_FREE_BLOCKS_METADATA;
+	/*
+	 * For bigalloc file systems, we never free a partial cluster
+	 * at the beginning of the extent.  Instead, we make a note
+	 * that we tried freeing the cluster, and check to see if we
+	 * need to free it on a subsequent call to ext4_remove_blocks,
+	 * or at the end of the ext4_truncate() operation.
+	 */
+	flags |= EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER;
+
+	/*
+	 * If we have a partial cluster, and it's different from the
+	 * cluster of the last block, we need to explicitly free the
+	 * partial cluster here.
+	 */
+	pblk = ext4_ext_pblock(ex) + ee_len - 1;
+	if (*partial_cluster &amp;&amp; (EXT4_B2C(sbi, pblk) != *partial_cluster)) {
+		ext4_free_blocks(handle, inode, NULL,
+				 EXT4_C2B(sbi, *partial_cluster),
+				 sbi-&gt;s_cluster_ratio, flags);
+		*partial_cluster = 0;
+	}
+
 #ifdef EXTENTS_STATS
 	{
 		struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
@@ -2229,12 +2254,24 @@ static int ext4_remove_blocks(handle_t *handle, struct inode *inode,
 	    &amp;&amp; to == le32_to_cpu(ex-&gt;ee_block) + ee_len - 1) {
 		/* tail removal */
 		ext4_lblk_t num;
-		ext4_fsblk_t start;
 
 		num = le32_to_cpu(ex-&gt;ee_block) + ee_len - from;
-		start = ext4_ext_pblock(ex) + ee_len - num;
-		ext_debug("free last %u blocks starting %llu\n", num, start);
-		ext4_free_blocks(handle, inode, NULL, start, num, flags);
+		pblk = ext4_ext_pblock(ex) + ee_len - num;
+		ext_debug("free last %u blocks starting %llu\n", num, pblk);
+		ext4_free_blocks(handle, inode, NULL, pblk, num, flags);
+		/*
+		 * If the block range to be freed didn't start at the
+		 * beginning of a cluster, and we removed the entire
+		 * extent, save the partial cluster here, since we
+		 * might need to delete if we determine that the
+		 * truncate operation has removed all of the blocks in
+		 * the cluster.
+		 */
+		if (pblk &amp; (sbi-&gt;s_cluster_ratio - 1) &amp;&amp;
+		    (ee_len == num))
+			*partial_cluster = EXT4_B2C(sbi, pblk);
+		else
+			*partial_cluster = 0;
 	} else if (from == le32_to_cpu(ex-&gt;ee_block)
 		   &amp;&amp; to &lt;= le32_to_cpu(ex-&gt;ee_block) + ee_len - 1) {
 		/* head removal */
@@ -2269,9 +2306,10 @@ static int ext4_remove_blocks(handle_t *handle, struct inode *inode,
  */
 static int
 ext4_ext_rm_leaf(handle_t *handle, struct inode *inode,
-		struct ext4_ext_path *path, ext4_lblk_t start,
-		ext4_lblk_t end)
+		 struct ext4_ext_path *path, ext4_fsblk_t *partial_cluster,
+		 ext4_lblk_t start, ext4_lblk_t end)
 {
+	struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
 	int err = 0, correct_index = 0;
 	int depth = ext_depth(inode), credits;
 	struct ext4_extent_header *eh;
@@ -2423,7 +2461,8 @@ ext4_ext_rm_leaf(handle_t *handle, struct inode *inode,
 		if (err)
 			goto out;
 
-		err = ext4_remove_blocks(handle, inode, ex, a, b);
+		err = ext4_remove_blocks(handle, inode, ex, partial_cluster,
+					 a, b);
 		if (err)
 			goto out;
 
@@ -2471,7 +2510,8 @@ ext4_ext_rm_leaf(handle_t *handle, struct inode *inode,
 					sizeof(struct ext4_extent));
 			}
 			le16_add_cpu(&amp;eh-&gt;eh_entries, -1);
-		}
+		} else
+			*partial_cluster = 0;
 
 		ext_debug("new extent: %u:%u:%llu\n", block, num,
 				ext4_ext_pblock(ex));
@@ -2483,6 +2523,25 @@ ext4_ext_rm_leaf(handle_t *handle, struct inode *inode,
 	if (correct_index &amp;&amp; eh-&gt;eh_entries)
 		err = ext4_ext_correct_indexes(handle, inode, path);
 
+	/*
+	 * If there is still a entry in the leaf node, check to see if
+	 * it references the partial cluster.  This is the only place
+	 * where it could; if it doesn't, we can free the cluster.
+	 */
+	if (*partial_cluster &amp;&amp; ex &gt;= EXT_FIRST_EXTENT(eh) &amp;&amp;
+	    (EXT4_B2C(sbi, ext4_ext_pblock(ex) + ex_ee_len - 1) !=
+	     *partial_cluster)) {
+		int flags = EXT4_FREE_BLOCKS_FORGET;
+
+		if (S_ISDIR(inode-&gt;i_mode) || S_ISLNK(inode-&gt;i_mode))
+			flags |= EXT4_FREE_BLOCKS_METADATA;
+
+		ext4_free_blocks(handle, inode, NULL,
+				 EXT4_C2B(sbi, *partial_cluster),
+				 sbi-&gt;s_cluster_ratio, flags);
+		*partial_cluster = 0;
+	}
+
 	/* if this leaf is free, then we should
 	 * remove it from index block above */
 	if (err == 0 &amp;&amp; eh-&gt;eh_entries == 0 &amp;&amp; path[depth].p_bh != NULL)
@@ -2518,6 +2577,7 @@ static int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start)
 	struct super_block *sb = inode-&gt;i_sb;
 	int depth = ext_depth(inode);
 	struct ext4_ext_path *path;
+	ext4_fsblk_t partial_cluster = 0;
 	handle_t *handle;
 	int i, err;
 
@@ -2553,7 +2613,8 @@ static int ext4_ext_remove_space(struct inode *inode, ext4_lblk_t start)
 		if (i == depth) {
 			/* this is leaf block */
 			err = ext4_ext_rm_leaf(handle, inode, path,
-					start, EXT_MAX_BLOCKS - 1);
+					       &amp;partial_cluster, start,
+					       EXT_MAX_BLOCKS - 1);
 			/* root level has p_bh == NULL, brelse() eats this */
 			brelse(path[i].p_bh);
 			path[i].p_bh = NULL;
@@ -3495,6 +3556,8 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 		ee_len = ext4_ext_get_actual_len(ex);
 		/* if found extent covers block, simply return it */
 		if (in_range(map-&gt;m_lblk, ee_block, ee_len)) {
+			ext4_fsblk_t partial_cluster = 0;
+
 			newblock = map-&gt;m_lblk - ee_block + ee_start;
 			/* number of remaining blocks in the extent */
 			allocated = ee_len - (map-&gt;m_lblk - ee_block);
@@ -3578,7 +3641,8 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 			ext4_ext_invalidate_cache(inode);
 
 			err = ext4_ext_rm_leaf(handle, inode, path,
-				map-&gt;m_lblk, map-&gt;m_lblk + punched_out);
+					       &amp;partial_cluster, map-&gt;m_lblk,
+					       map-&gt;m_lblk + punched_out);
 
 			if (!err &amp;&amp; path-&gt;p_hdr-&gt;eh_entries == 0) {
 				/*</pre><hr><pre>commit 4d33b1ef10995d7ba6191d67456202c697a92a32
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 18:52:51 2011 -0400

    ext4: teach ext4_ext_map_blocks() about the bigalloc feature
    
    If we need to allocate a new block in ext4_ext_map_blocks(), the
    function needs to see if the cluster has already been allocated.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index ba7bd5a176ce..bd42ab29efec 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -1270,7 +1270,8 @@ static int ext4_ext_search_left(struct inode *inode,
  */
 static int ext4_ext_search_right(struct inode *inode,
 				 struct ext4_ext_path *path,
-				 ext4_lblk_t *logical, ext4_fsblk_t *phys)
+				 ext4_lblk_t *logical, ext4_fsblk_t *phys,
+				 struct ext4_extent **ret_ex)
 {
 	struct buffer_head *bh = NULL;
 	struct ext4_extent_header *eh;
@@ -1312,9 +1313,7 @@ static int ext4_ext_search_right(struct inode *inode,
 				return -EIO;
 			}
 		}
-		*logical = le32_to_cpu(ex-&gt;ee_block);
-		*phys = ext4_ext_pblock(ex);
-		return 0;
+		goto found_extent;
 	}
 
 	if (unlikely(*logical &lt; (le32_to_cpu(ex-&gt;ee_block) + ee_len))) {
@@ -1327,9 +1326,7 @@ static int ext4_ext_search_right(struct inode *inode,
 	if (ex != EXT_LAST_EXTENT(path[depth].p_hdr)) {
 		/* next allocated block in this leaf */
 		ex++;
-		*logical = le32_to_cpu(ex-&gt;ee_block);
-		*phys = ext4_ext_pblock(ex);
-		return 0;
+		goto found_extent;
 	}
 
 	/* go up and search for index to the right */
@@ -1372,9 +1369,12 @@ static int ext4_ext_search_right(struct inode *inode,
 		return -EIO;
 	}
 	ex = EXT_FIRST_EXTENT(eh);
+found_extent:
 	*logical = le32_to_cpu(ex-&gt;ee_block);
 	*phys = ext4_ext_pblock(ex);
-	put_bh(bh);
+	*ret_ex = ex;
+	if (bh)
+		put_bh(bh);
 	return 0;
 }
 
@@ -1627,7 +1627,8 @@ static int ext4_ext_try_to_merge(struct inode *inode,
  * such that there will be no overlap, and then returns 1.
  * If there is no overlap found, it returns 0.
  */
-static unsigned int ext4_ext_check_overlap(struct inode *inode,
+static unsigned int ext4_ext_check_overlap(struct ext4_sb_info *sbi,
+					   struct inode *inode,
 					   struct ext4_extent *newext,
 					   struct ext4_ext_path *path)
 {
@@ -1641,6 +1642,7 @@ static unsigned int ext4_ext_check_overlap(struct inode *inode,
 	if (!path[depth].p_ext)
 		goto out;
 	b2 = le32_to_cpu(path[depth].p_ext-&gt;ee_block);
+	b2 &amp;= ~(sbi-&gt;s_cluster_ratio - 1);
 
 	/*
 	 * get the next allocated block if the extent in the path
@@ -1650,6 +1652,7 @@ static unsigned int ext4_ext_check_overlap(struct inode *inode,
 		b2 = ext4_ext_next_allocated_block(path);
 		if (b2 == EXT_MAX_BLOCKS)
 			goto out;
+		b2 &amp;= ~(sbi-&gt;s_cluster_ratio - 1);
 	}
 
 	/* check for wrap through zero on extent logical start block*/
@@ -3293,6 +3296,106 @@ ext4_ext_handle_uninitialized_extents(handle_t *handle, struct inode *inode,
 	return err ? err : allocated;
 }
 
+/*
+ * get_implied_cluster_alloc - check to see if the requested
+ * allocation (in the map structure) overlaps with a cluster already
+ * allocated in an extent.
+ *	@sbi	The ext4-specific superblock structure
+ *	@map	The requested lblk-&gt;pblk mapping
+ *	@ex	The extent structure which might contain an implied
+ *			cluster allocation
+ *
+ * This function is called by ext4_ext_map_blocks() after we failed to
+ * find blocks that were already in the inode's extent tree.  Hence,
+ * we know that the beginning of the requested region cannot overlap
+ * the extent from the inode's extent tree.  There are three cases we
+ * want to catch.  The first is this case:
+ *
+ *		 |--- cluster # N--|
+ *    |--- extent ---|	|---- requested region ---|
+ *			|==========|
+ *
+ * The second case that we need to test for is this one:
+ *
+ *   |--------- cluster # N ----------------|
+ *	   |--- requested region --|   |------- extent ----|
+ *	   |=======================|
+ *
+ * The third case is when the requested region lies between two extents
+ * within the same cluster:
+ *          |------------- cluster # N-------------|
+ * |----- ex -----|                  |---- ex_right ----|
+ *                  |------ requested region ------|
+ *                  |================|
+ *
+ * In each of the above cases, we need to set the map-&gt;m_pblk and
+ * map-&gt;m_len so it corresponds to the return the extent labelled as
+ * "|====|" from cluster #N, since it is already in use for data in
+ * cluster EXT4_B2C(sbi, map-&gt;m_lblk).	We will then return 1 to
+ * signal to ext4_ext_map_blocks() that map-&gt;m_pblk should be treated
+ * as a new "allocated" block region.  Otherwise, we will return 0 and
+ * ext4_ext_map_blocks() will then allocate one or more new clusters
+ * by calling ext4_mb_new_blocks().
+ */
+static int get_implied_cluster_alloc(struct ext4_sb_info *sbi,
+				     struct ext4_map_blocks *map,
+				     struct ext4_extent *ex,
+				     struct ext4_ext_path *path)
+{
+	ext4_lblk_t c_offset = map-&gt;m_lblk &amp; (sbi-&gt;s_cluster_ratio-1);
+	ext4_lblk_t ex_cluster_start, ex_cluster_end;
+	ext4_lblk_t rr_cluster_start, rr_cluster_end;
+	ext4_lblk_t ee_block = le32_to_cpu(ex-&gt;ee_block);
+	ext4_fsblk_t ee_start = ext4_ext_pblock(ex);
+	unsigned short ee_len = ext4_ext_get_actual_len(ex);
+
+	/* The extent passed in that we are trying to match */
+	ex_cluster_start = EXT4_B2C(sbi, ee_block);
+	ex_cluster_end = EXT4_B2C(sbi, ee_block + ee_len - 1);
+
+	/* The requested region passed into ext4_map_blocks() */
+	rr_cluster_start = EXT4_B2C(sbi, map-&gt;m_lblk);
+	rr_cluster_end = EXT4_B2C(sbi, map-&gt;m_lblk + map-&gt;m_len - 1);
+
+	if ((rr_cluster_start == ex_cluster_end) ||
+	    (rr_cluster_start == ex_cluster_start)) {
+		if (rr_cluster_start == ex_cluster_end)
+			ee_start += ee_len - 1;
+		map-&gt;m_pblk = (ee_start &amp; ~(sbi-&gt;s_cluster_ratio - 1)) +
+			c_offset;
+		map-&gt;m_len = min(map-&gt;m_len,
+				 (unsigned) sbi-&gt;s_cluster_ratio - c_offset);
+		/*
+		 * Check for and handle this case:
+		 *
+		 *   |--------- cluster # N-------------|
+		 *		       |------- extent ----|
+		 *	   |--- requested region ---|
+		 *	   |===========|
+		 */
+
+		if (map-&gt;m_lblk &lt; ee_block)
+			map-&gt;m_len = min(map-&gt;m_len, ee_block - map-&gt;m_lblk);
+
+		/*
+		 * Check for the case where there is already another allocated
+		 * block to the right of 'ex' but before the end of the cluster.
+		 *
+		 *          |------------- cluster # N-------------|
+		 * |----- ex -----|                  |---- ex_right ----|
+		 *                  |------ requested region ------|
+		 *                  |================|
+		 */
+		if (map-&gt;m_lblk &gt; ee_block) {
+			ext4_lblk_t next = ext4_ext_next_allocated_block(path);
+			map-&gt;m_len = min(map-&gt;m_len, next - map-&gt;m_lblk);
+		}
+		return 1;
+	}
+	return 0;
+}
+
+
 /*
  * Block allocation/map/preallocation routine for extents based files
  *
@@ -3315,14 +3418,16 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 			struct ext4_map_blocks *map, int flags)
 {
 	struct ext4_ext_path *path = NULL;
-	struct ext4_extent newex, *ex;
+	struct ext4_extent newex, *ex, *ex2;
+	struct ext4_sb_info *sbi = EXT4_SB(inode-&gt;i_sb);
 	ext4_fsblk_t newblock = 0;
-	int err = 0, depth, ret;
-	unsigned int allocated = 0;
+	int free_on_err = 0, err = 0, depth, ret;
+	unsigned int allocated = 0, offset = 0;
 	unsigned int punched_out = 0;
 	unsigned int result = 0;
 	struct ext4_allocation_request ar;
 	ext4_io_end_t *io = EXT4_I(inode)-&gt;cur_aio_dio;
+	ext4_lblk_t cluster_offset;
 	struct ext4_map_blocks punch_map;
 
 	ext_debug("blocks %u/%u requested for inode %lu\n",
@@ -3508,9 +3613,23 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 		ext4_ext_put_gap_in_cache(inode, path, map-&gt;m_lblk);
 		goto out2;
 	}
+
 	/*
 	 * Okay, we need to do block allocation.
 	 */
+	newex.ee_block = cpu_to_le32(map-&gt;m_lblk);
+	cluster_offset = map-&gt;m_lblk &amp; (sbi-&gt;s_cluster_ratio-1);
+
+	/*
+	 * If we are doing bigalloc, check to see if the extent returned
+	 * by ext4_ext_find_extent() implies a cluster we can use.
+	 */
+	if (cluster_offset &amp;&amp; ex &amp;&amp;
+	    get_implied_cluster_alloc(sbi, map, ex, path)) {
+		ar.len = allocated = map-&gt;m_len;
+		newblock = map-&gt;m_pblk;
+		goto got_allocated_blocks;
+	}
 
 	/* find neighbour allocated blocks */
 	ar.lleft = map-&gt;m_lblk;
@@ -3518,10 +3637,20 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 	if (err)
 		goto out2;
 	ar.lright = map-&gt;m_lblk;
-	err = ext4_ext_search_right(inode, path, &amp;ar.lright, &amp;ar.pright);
+	ex2 = NULL;
+	err = ext4_ext_search_right(inode, path, &amp;ar.lright, &amp;ar.pright, &amp;ex2);
 	if (err)
 		goto out2;
 
+	/* Check if the extent after searching to the right implies a
+	 * cluster we can use. */
+	if ((sbi-&gt;s_cluster_ratio &gt; 1) &amp;&amp; ex2 &amp;&amp;
+	    get_implied_cluster_alloc(sbi, map, ex2, path)) {
+		ar.len = allocated = map-&gt;m_len;
+		newblock = map-&gt;m_pblk;
+		goto got_allocated_blocks;
+	}
+
 	/*
 	 * See if request is beyond maximum number of blocks we can have in
 	 * a single extent. For an initialized extent this limit is
@@ -3536,9 +3665,8 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 		map-&gt;m_len = EXT_UNINIT_MAX_LEN;
 
 	/* Check if we can really insert (m_lblk)::(m_lblk + m_len) extent */
-	newex.ee_block = cpu_to_le32(map-&gt;m_lblk);
 	newex.ee_len = cpu_to_le16(map-&gt;m_len);
-	err = ext4_ext_check_overlap(inode, &amp;newex, path);
+	err = ext4_ext_check_overlap(sbi, inode, &amp;newex, path);
 	if (err)
 		allocated = ext4_ext_get_actual_len(&amp;newex);
 	else
@@ -3548,7 +3676,18 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 	ar.inode = inode;
 	ar.goal = ext4_ext_find_goal(inode, path, map-&gt;m_lblk);
 	ar.logical = map-&gt;m_lblk;
-	ar.len = allocated;
+	/*
+	 * We calculate the offset from the beginning of the cluster
+	 * for the logical block number, since when we allocate a
+	 * physical cluster, the physical block should start at the
+	 * same offset from the beginning of the cluster.  This is
+	 * needed so that future calls to get_implied_cluster_alloc()
+	 * work correctly.
+	 */
+	offset = map-&gt;m_lblk &amp; (sbi-&gt;s_cluster_ratio - 1);
+	ar.len = EXT4_NUM_B2C(sbi, offset+allocated);
+	ar.goal -= offset;
+	ar.logical -= offset;
 	if (S_ISREG(inode-&gt;i_mode))
 		ar.flags = EXT4_MB_HINT_DATA;
 	else
@@ -3561,9 +3700,14 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 		goto out2;
 	ext_debug("allocate new block: goal %llu, found %llu/%u\n",
 		  ar.goal, newblock, allocated);
+	free_on_err = 1;
+	ar.len = EXT4_C2B(sbi, ar.len) - offset;
+	if (ar.len &gt; allocated)
+		ar.len = allocated;
 
+got_allocated_blocks:
 	/* try to insert new extent into found leaf and return */
-	ext4_ext_store_pblock(&amp;newex, newblock);
+	ext4_ext_store_pblock(&amp;newex, newblock + offset);
 	newex.ee_len = cpu_to_le16(ar.len);
 	/* Mark uninitialized */
 	if (flags &amp; EXT4_GET_BLOCKS_UNINIT_EXT){
@@ -3591,7 +3735,7 @@ int ext4_ext_map_blocks(handle_t *handle, struct inode *inode,
 	if (!err)
 		err = ext4_ext_insert_extent(handle, inode, path,
 					     &amp;newex, flags);
-	if (err) {
+	if (err &amp;&amp; free_on_err) {
 		int fb_flags = flags &amp; EXT4_GET_BLOCKS_DELALLOC_RESERVE ?
 			EXT4_FREE_BLOCKS_NO_QUOT_UPDATE : 0;
 		/* free data blocks we just allocated */
@@ -4115,7 +4259,6 @@ static int ext4_ext_fiemap_cb(struct inode *inode, ext4_lblk_t next,
 		return EXT_BREAK;
 	return EXT_CONTINUE;
 }
-
 /* fiemap flags we can handle specified here */
 #define EXT4_FIEMAP_FLAGS	(FIEMAP_FLAG_SYNC|FIEMAP_FLAG_XATTR)
 </pre><hr><pre>commit 84130193e0e6568dfdfb823f0e1e19aec80aff6e
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 18:50:51 2011 -0400

    ext4: teach ext4_free_blocks() about bigalloc and clusters
    
    The ext4_free_blocks() function now has two new flags that indicate
    whether a partial cluster at the beginning or the end of the block
    extents should be freed or not.  That will be up the caller (i.e.,
    truncate), who can figure out whether partial clusters at the
    beginning or the end of a block range can be freed.
    
    We also have to update the ext4_mb_free_metadata() and
    release_blocks_on_commit() machinery to be cluster-based, since it is
    used by ext4_free_blocks().
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4.h b/fs/ext4/ext4.h
index 803cfa42e1e8..030bfc1cb59d 100644
--- a/fs/ext4/ext4.h
+++ b/fs/ext4/ext4.h
@@ -539,6 +539,8 @@ struct ext4_new_group_data {
 #define EXT4_FREE_BLOCKS_FORGET		0x0002
 #define EXT4_FREE_BLOCKS_VALIDATED	0x0004
 #define EXT4_FREE_BLOCKS_NO_QUOT_UPDATE	0x0008
+#define EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER	0x0010
+#define EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER	0x0020
 
 /*
  * Flags used by ext4_discard_partial_page_buffers
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 8765f2512f13..57ce6960e940 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2602,11 +2602,13 @@ int ext4_mb_release(struct super_block *sb)
 }
 
 static inline int ext4_issue_discard(struct super_block *sb,
-		ext4_group_t block_group, ext4_grpblk_t block, int count)
+		ext4_group_t block_group, ext4_grpblk_t cluster, int count)
 {
 	ext4_fsblk_t discard_block;
 
-	discard_block = block + ext4_group_first_block_no(sb, block_group);
+	discard_block = (EXT4_C2B(EXT4_SB(sb), cluster) +
+			 ext4_group_first_block_no(sb, block_group));
+	count = EXT4_C2B(EXT4_SB(sb), count);
 	trace_ext4_discard_blocks(sb,
 			(unsigned long long) discard_block, count);
 	return sb_issue_discard(sb, discard_block, count, GFP_NOFS, 0);
@@ -2633,7 +2635,7 @@ static void release_blocks_on_commit(journal_t *journal, transaction_t *txn)
 
 		if (test_opt(sb, DISCARD))
 			ext4_issue_discard(sb, entry-&gt;group,
-					   entry-&gt;start_blk, entry-&gt;count);
+					   entry-&gt;start_cluster, entry-&gt;count);
 
 		err = ext4_mb_load_buddy(sb, entry-&gt;group, &amp;e4b);
 		/* we expect to find existing buddy because it's pinned */
@@ -2646,7 +2648,7 @@ static void release_blocks_on_commit(journal_t *journal, transaction_t *txn)
 		ext4_lock_group(sb, entry-&gt;group);
 		/* Take it out of per group rb tree */
 		rb_erase(&amp;entry-&gt;node, &amp;(db-&gt;bb_free_root));
-		mb_free_blocks(NULL, &amp;e4b, entry-&gt;start_blk, entry-&gt;count);
+		mb_free_blocks(NULL, &amp;e4b, entry-&gt;start_cluster, entry-&gt;count);
 
 		/*
 		 * Clear the trimmed flag for the group so that the next
@@ -3300,7 +3302,7 @@ static void ext4_mb_generate_from_freelist(struct super_block *sb, void *bitmap,
 
 	while (n) {
 		entry = rb_entry(n, struct ext4_free_data, node);
-		ext4_set_bits(bitmap, entry-&gt;start_blk, entry-&gt;count);
+		ext4_set_bits(bitmap, entry-&gt;start_cluster, entry-&gt;count);
 		n = rb_next(n);
 	}
 	return;
@@ -4401,7 +4403,7 @@ static int can_merge(struct ext4_free_data *entry1,
 {
 	if ((entry1-&gt;t_tid == entry2-&gt;t_tid) &amp;&amp;
 	    (entry1-&gt;group == entry2-&gt;group) &amp;&amp;
-	    ((entry1-&gt;start_blk + entry1-&gt;count) == entry2-&gt;start_blk))
+	    ((entry1-&gt;start_cluster + entry1-&gt;count) == entry2-&gt;start_cluster))
 		return 1;
 	return 0;
 }
@@ -4411,7 +4413,7 @@ ext4_mb_free_metadata(handle_t *handle, struct ext4_buddy *e4b,
 		      struct ext4_free_data *new_entry)
 {
 	ext4_group_t group = e4b-&gt;bd_group;
-	ext4_grpblk_t block;
+	ext4_grpblk_t cluster;
 	struct ext4_free_data *entry;
 	struct ext4_group_info *db = e4b-&gt;bd_info;
 	struct super_block *sb = e4b-&gt;bd_sb;
@@ -4424,7 +4426,7 @@ ext4_mb_free_metadata(handle_t *handle, struct ext4_buddy *e4b,
 	BUG_ON(e4b-&gt;bd_buddy_page == NULL);
 
 	new_node = &amp;new_entry-&gt;node;
-	block = new_entry-&gt;start_blk;
+	cluster = new_entry-&gt;start_cluster;
 
 	if (!*n) {
 		/* first free block exent. We need to
@@ -4438,13 +4440,14 @@ ext4_mb_free_metadata(handle_t *handle, struct ext4_buddy *e4b,
 	while (*n) {
 		parent = *n;
 		entry = rb_entry(parent, struct ext4_free_data, node);
-		if (block &lt; entry-&gt;start_blk)
+		if (cluster &lt; entry-&gt;start_cluster)
 			n = &amp;(*n)-&gt;rb_left;
-		else if (block &gt;= (entry-&gt;start_blk + entry-&gt;count))
+		else if (cluster &gt;= (entry-&gt;start_cluster + entry-&gt;count))
 			n = &amp;(*n)-&gt;rb_right;
 		else {
 			ext4_grp_locked_error(sb, group, 0,
-				ext4_group_first_block_no(sb, group) + block,
+				ext4_group_first_block_no(sb, group) +
+				EXT4_C2B(sbi, cluster),
 				"Block already on to-be-freed list");
 			return 0;
 		}
@@ -4458,7 +4461,7 @@ ext4_mb_free_metadata(handle_t *handle, struct ext4_buddy *e4b,
 	if (node) {
 		entry = rb_entry(node, struct ext4_free_data, node);
 		if (can_merge(entry, new_entry)) {
-			new_entry-&gt;start_blk = entry-&gt;start_blk;
+			new_entry-&gt;start_cluster = entry-&gt;start_cluster;
 			new_entry-&gt;count += entry-&gt;count;
 			rb_erase(node, &amp;(db-&gt;bb_free_root));
 			spin_lock(&amp;sbi-&gt;s_md_lock);
@@ -4509,6 +4512,7 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 	ext4_group_t block_group;
 	struct ext4_sb_info *sbi;
 	struct ext4_buddy e4b;
+	unsigned int count_clusters;
 	int err = 0;
 	int ret;
 
@@ -4557,6 +4561,38 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 	if (!ext4_should_writeback_data(inode))
 		flags |= EXT4_FREE_BLOCKS_METADATA;
 
+	/*
+	 * If the extent to be freed does not begin on a cluster
+	 * boundary, we need to deal with partial clusters at the
+	 * beginning and end of the extent.  Normally we will free
+	 * blocks at the beginning or the end unless we are explicitly
+	 * requested to avoid doing so.
+	 */
+	overflow = block &amp; (sbi-&gt;s_cluster_ratio - 1);
+	if (overflow) {
+		if (flags &amp; EXT4_FREE_BLOCKS_NOFREE_FIRST_CLUSTER) {
+			overflow = sbi-&gt;s_cluster_ratio - overflow;
+			block += overflow;
+			if (count &gt; overflow)
+				count -= overflow;
+			else
+				return;
+		} else {
+			block -= overflow;
+			count += overflow;
+		}
+	}
+	overflow = count &amp; (sbi-&gt;s_cluster_ratio - 1);
+	if (overflow) {
+		if (flags &amp; EXT4_FREE_BLOCKS_NOFREE_LAST_CLUSTER) {
+			if (count &gt; overflow)
+				count -= overflow;
+			else
+				return;
+		} else
+			count += sbi-&gt;s_cluster_ratio - overflow;
+	}
+
 do_more:
 	overflow = 0;
 	ext4_get_group_no_and_offset(sb, block, &amp;block_group, &amp;bit);
@@ -4565,10 +4601,12 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 	 * Check to see if we are freeing blocks across a group
 	 * boundary.
 	 */
-	if (bit + count &gt; EXT4_CLUSTERS_PER_GROUP(sb)) {
-		overflow = bit + count - EXT4_CLUSTERS_PER_GROUP(sb);
+	if (EXT4_C2B(sbi, bit) + count &gt; EXT4_BLOCKS_PER_GROUP(sb)) {
+		overflow = EXT4_C2B(sbi, bit) + count -
+			EXT4_BLOCKS_PER_GROUP(sb);
 		count -= overflow;
 	}
+	count_clusters = EXT4_B2C(sbi, count);
 	bitmap_bh = ext4_read_block_bitmap(sb, block_group);
 	if (!bitmap_bh) {
 		err = -EIO;
@@ -4583,9 +4621,9 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 	if (in_range(ext4_block_bitmap(sb, gdp), block, count) ||
 	    in_range(ext4_inode_bitmap(sb, gdp), block, count) ||
 	    in_range(block, ext4_inode_table(sb, gdp),
-		      EXT4_SB(sb)-&gt;s_itb_per_group) ||
+		     EXT4_SB(sb)-&gt;s_itb_per_group) ||
 	    in_range(block + count - 1, ext4_inode_table(sb, gdp),
-		      EXT4_SB(sb)-&gt;s_itb_per_group)) {
+		     EXT4_SB(sb)-&gt;s_itb_per_group)) {
 
 		ext4_error(sb, "Freeing blocks in system zone - "
 			   "Block = %llu, count = %lu", block, count);
@@ -4610,11 +4648,11 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 #ifdef AGGRESSIVE_CHECK
 	{
 		int i;
-		for (i = 0; i &lt; count; i++)
+		for (i = 0; i &lt; count_clusters; i++)
 			BUG_ON(!mb_test_bit(bit + i, bitmap_bh-&gt;b_data));
 	}
 #endif
-	trace_ext4_mballoc_free(sb, inode, block_group, bit, count);
+	trace_ext4_mballoc_free(sb, inode, block_group, bit, count_clusters);
 
 	err = ext4_mb_load_buddy(sb, block_group, &amp;e4b);
 	if (err)
@@ -4631,13 +4669,13 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 			err = -ENOMEM;
 			goto error_return;
 		}
-		new_entry-&gt;start_blk = bit;
+		new_entry-&gt;start_cluster = bit;
 		new_entry-&gt;group  = block_group;
-		new_entry-&gt;count = count;
+		new_entry-&gt;count = count_clusters;
 		new_entry-&gt;t_tid = handle-&gt;h_transaction-&gt;t_tid;
 
 		ext4_lock_group(sb, block_group);
-		mb_clear_bits(bitmap_bh-&gt;b_data, bit, count);
+		mb_clear_bits(bitmap_bh-&gt;b_data, bit, count_clusters);
 		ext4_mb_free_metadata(handle, &amp;e4b, new_entry);
 	} else {
 		/* need to update group_info-&gt;bb_free and bitmap
@@ -4645,11 +4683,11 @@ void ext4_free_blocks(handle_t *handle, struct inode *inode,
 		 * them with group lock_held
 		 */
 		ext4_lock_group(sb, block_group);
-		mb_clear_bits(bitmap_bh-&gt;b_data, bit, count);
-		mb_free_blocks(inode, &amp;e4b, bit, count);
+		mb_clear_bits(bitmap_bh-&gt;b_data, bit, count_clusters);
+		mb_free_blocks(inode, &amp;e4b, bit, count_clusters);
 	}
 
-	ret = ext4_free_blks_count(sb, gdp) + count;
+	ret = ext4_free_blks_count(sb, gdp) + count_clusters;
 	ext4_free_blks_set(sb, gdp, ret);
 	gdp-&gt;bg_checksum = ext4_group_desc_csum(sbi, block_group, gdp);
 	ext4_unlock_group(sb, block_group);
diff --git a/fs/ext4/mballoc.h b/fs/ext4/mballoc.h
index 1641f4b57439..dc99930d4cb5 100644
--- a/fs/ext4/mballoc.h
+++ b/fs/ext4/mballoc.h
@@ -106,7 +106,7 @@ struct ext4_free_data {
 	ext4_group_t group;
 
 	/* free block extent */
-	ext4_grpblk_t start_blk;
+	ext4_grpblk_t start_cluster;
 	ext4_grpblk_t count;
 
 	/* transaction which freed this extent */</pre><hr><pre>commit 53accfa9f819c80056db6f03f9c5cfa4bcba1ed8
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 18:48:51 2011 -0400

    ext4: teach mballoc preallocation code about bigalloc clusters
    
    In most of mballoc.c, we do everything in units of clusters, since the
    block allocation bitmaps and buddy bitmaps are all denominated in
    clusters.  The one place where we do deal with absolute block numbers
    is in the code that handles the preallocation regions, since in the
    case of inode-based preallocation regions, the start of the
    preallocation region can't be relative to the beginning of the group.
    
    So this adds a bit of complexity, where pa_pstart and pa_lstart are
    block numbers, while pa_free, pa_len, and fe_len are denominated in
    units of clusters.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 81e28657a3c2..8765f2512f13 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -70,8 +70,8 @@
  *
  * pa_lstart -&gt; the logical start block for this prealloc space
  * pa_pstart -&gt; the physical start block for this prealloc space
- * pa_len    -&gt; length for this prealloc space
- * pa_free   -&gt;  free space available in this prealloc space
+ * pa_len    -&gt; length for this prealloc space (in clusters)
+ * pa_free   -&gt;  free space available in this prealloc space (in clusters)
  *
  * The inode preallocation space is used looking at the _logical_ start
  * block. If only the logical file block falls within the range of prealloc
@@ -459,7 +459,7 @@ static void mb_free_blocks_double(struct inode *inode, struct ext4_buddy *e4b,
 			ext4_fsblk_t blocknr;
 
 			blocknr = ext4_group_first_block_no(sb, e4b-&gt;bd_group);
-			blocknr += first + i;
+			blocknr += EXT4_C2B(EXT4_SB(sb), first + i);
 			ext4_grp_locked_error(sb, e4b-&gt;bd_group,
 					      inode ? inode-&gt;i_ino : 0,
 					      blocknr,
@@ -734,7 +734,7 @@ void ext4_mb_generate_buddy(struct super_block *sb,
 
 	if (free != grp-&gt;bb_free) {
 		ext4_grp_locked_error(sb, group, 0, 0,
-				      "%u blocks in bitmap, %u in gd",
+				      "%u clusters in bitmap, %u in gd",
 				      free, grp-&gt;bb_free);
 		/*
 		 * If we intent to continue, we consider group descritor
@@ -1339,7 +1339,7 @@ static void mb_free_blocks(struct inode *inode, struct ext4_buddy *e4b,
 			ext4_fsblk_t blocknr;
 
 			blocknr = ext4_group_first_block_no(sb, e4b-&gt;bd_group);
-			blocknr += block;
+			blocknr += EXT4_C2B(EXT4_SB(sb), block);
 			ext4_grp_locked_error(sb, e4b-&gt;bd_group,
 					      inode ? inode-&gt;i_ino : 0,
 					      blocknr,
@@ -1831,7 +1831,7 @@ void ext4_mb_complex_scan_group(struct ext4_allocation_context *ac,
 			 * we have free blocks
 			 */
 			ext4_grp_locked_error(sb, e4b-&gt;bd_group, 0, 0,
-					"%d free blocks as per "
+					"%d free clusters as per "
 					"group info. But bitmap says 0",
 					free);
 			break;
@@ -1841,7 +1841,7 @@ void ext4_mb_complex_scan_group(struct ext4_allocation_context *ac,
 		BUG_ON(ex.fe_len &lt;= 0);
 		if (free &lt; ex.fe_len) {
 			ext4_grp_locked_error(sb, e4b-&gt;bd_group, 0, 0,
-					"%d free blocks as per "
+					"%d free clusters as per "
 					"group info. But got %d blocks",
 					free, ex.fe_len);
 			/*
@@ -2752,7 +2752,7 @@ void ext4_exit_mballoc(void)
  */
 static noinline_for_stack int
 ext4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,
-				handle_t *handle, unsigned int reserv_blks)
+				handle_t *handle, unsigned int reserv_clstrs)
 {
 	struct buffer_head *bitmap_bh = NULL;
 	struct ext4_group_desc *gdp;
@@ -2791,7 +2791,7 @@ ext4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,
 
 	block = ext4_grp_offs_to_block(sb, &amp;ac-&gt;ac_b_ex);
 
-	len = ac-&gt;ac_b_ex.fe_len;
+	len = EXT4_C2B(sbi, ac-&gt;ac_b_ex.fe_len);
 	if (!ext4_data_block_valid(sbi, block, len)) {
 		ext4_error(sb, "Allocating blocks %llu-%llu which overlap "
 			   "fs metadata\n", block, block+len);
@@ -2838,7 +2838,7 @@ ext4_mb_mark_diskspace_used(struct ext4_allocation_context *ac,
 	 */
 	if (!(ac-&gt;ac_flags &amp; EXT4_MB_DELALLOC_RESERVED))
 		/* release all the reserved blocks if non delalloc */
-		percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, reserv_blks);
+		percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter, reserv_clstrs);
 
 	if (sbi-&gt;s_log_groups_per_flex) {
 		ext4_group_t flex_group = ext4_flex_group(sbi,
@@ -2886,6 +2886,7 @@ static noinline_for_stack void
 ext4_mb_normalize_request(struct ext4_allocation_context *ac,
 				struct ext4_allocation_request *ar)
 {
+	struct ext4_sb_info *sbi = EXT4_SB(ac-&gt;ac_sb);
 	int bsbits, max;
 	ext4_lblk_t end;
 	loff_t size, orig_size, start_off;
@@ -2916,7 +2917,7 @@ ext4_mb_normalize_request(struct ext4_allocation_context *ac,
 
 	/* first, let's learn actual file size
 	 * given current request is allocated */
-	size = ac-&gt;ac_o_ex.fe_logical + ac-&gt;ac_o_ex.fe_len;
+	size = ac-&gt;ac_o_ex.fe_logical + EXT4_C2B(sbi, ac-&gt;ac_o_ex.fe_len);
 	size = size &lt;&lt; bsbits;
 	if (size &lt; i_size_read(ac-&gt;ac_inode))
 		size = i_size_read(ac-&gt;ac_inode);
@@ -2988,7 +2989,8 @@ ext4_mb_normalize_request(struct ext4_allocation_context *ac,
 			continue;
 		}
 
-		pa_end = pa-&gt;pa_lstart + pa-&gt;pa_len;
+		pa_end = pa-&gt;pa_lstart + EXT4_C2B(EXT4_SB(ac-&gt;ac_sb),
+						  pa-&gt;pa_len);
 
 		/* PA must not overlap original request */
 		BUG_ON(!(ac-&gt;ac_o_ex.fe_logical &gt;= pa_end ||
@@ -3018,9 +3020,11 @@ ext4_mb_normalize_request(struct ext4_allocation_context *ac,
 	rcu_read_lock();
 	list_for_each_entry_rcu(pa, &amp;ei-&gt;i_prealloc_list, pa_inode_list) {
 		ext4_lblk_t pa_end;
+
 		spin_lock(&amp;pa-&gt;pa_lock);
 		if (pa-&gt;pa_deleted == 0) {
-			pa_end = pa-&gt;pa_lstart + pa-&gt;pa_len;
+			pa_end = pa-&gt;pa_lstart + EXT4_C2B(EXT4_SB(ac-&gt;ac_sb),
+							  pa-&gt;pa_len);
 			BUG_ON(!(start &gt;= pa_end || end &lt;= pa-&gt;pa_lstart));
 		}
 		spin_unlock(&amp;pa-&gt;pa_lock);
@@ -3043,7 +3047,7 @@ ext4_mb_normalize_request(struct ext4_allocation_context *ac,
 	/* XXX: is it better to align blocks WRT to logical
 	 * placement or satisfy big request as is */
 	ac-&gt;ac_g_ex.fe_logical = start;
-	ac-&gt;ac_g_ex.fe_len = size;
+	ac-&gt;ac_g_ex.fe_len = EXT4_NUM_B2C(sbi, size);
 
 	/* define goal start in order to merge */
 	if (ar-&gt;pright &amp;&amp; (ar-&gt;lright == (start + size))) {
@@ -3112,14 +3116,16 @@ static void ext4_discard_allocated_blocks(struct ext4_allocation_context *ac)
 static void ext4_mb_use_inode_pa(struct ext4_allocation_context *ac,
 				struct ext4_prealloc_space *pa)
 {
+	struct ext4_sb_info *sbi = EXT4_SB(ac-&gt;ac_sb);
 	ext4_fsblk_t start;
 	ext4_fsblk_t end;
 	int len;
 
 	/* found preallocated blocks, use them */
 	start = pa-&gt;pa_pstart + (ac-&gt;ac_o_ex.fe_logical - pa-&gt;pa_lstart);
-	end = min(pa-&gt;pa_pstart + pa-&gt;pa_len, start + ac-&gt;ac_o_ex.fe_len);
-	len = end - start;
+	end = min(pa-&gt;pa_pstart + EXT4_C2B(sbi, pa-&gt;pa_len),
+		  start + EXT4_C2B(sbi, ac-&gt;ac_o_ex.fe_len));
+	len = EXT4_NUM_B2C(sbi, end - start);
 	ext4_get_group_no_and_offset(ac-&gt;ac_sb, start, &amp;ac-&gt;ac_b_ex.fe_group,
 					&amp;ac-&gt;ac_b_ex.fe_start);
 	ac-&gt;ac_b_ex.fe_len = len;
@@ -3127,7 +3133,7 @@ static void ext4_mb_use_inode_pa(struct ext4_allocation_context *ac,
 	ac-&gt;ac_pa = pa;
 
 	BUG_ON(start &lt; pa-&gt;pa_pstart);
-	BUG_ON(start + len &gt; pa-&gt;pa_pstart + pa-&gt;pa_len);
+	BUG_ON(end &gt; pa-&gt;pa_pstart + EXT4_C2B(sbi, pa-&gt;pa_len));
 	BUG_ON(pa-&gt;pa_free &lt; len);
 	pa-&gt;pa_free -= len;
 
@@ -3193,6 +3199,7 @@ ext4_mb_check_group_pa(ext4_fsblk_t goal_block,
 static noinline_for_stack int
 ext4_mb_use_preallocated(struct ext4_allocation_context *ac)
 {
+	struct ext4_sb_info *sbi = EXT4_SB(ac-&gt;ac_sb);
 	int order, i;
 	struct ext4_inode_info *ei = EXT4_I(ac-&gt;ac_inode);
 	struct ext4_locality_group *lg;
@@ -3210,12 +3217,14 @@ ext4_mb_use_preallocated(struct ext4_allocation_context *ac)
 		/* all fields in this condition don't change,
 		 * so we can skip locking for them */
 		if (ac-&gt;ac_o_ex.fe_logical &lt; pa-&gt;pa_lstart ||
-			ac-&gt;ac_o_ex.fe_logical &gt;= pa-&gt;pa_lstart + pa-&gt;pa_len)
+		    ac-&gt;ac_o_ex.fe_logical &gt;= (pa-&gt;pa_lstart +
+					       EXT4_C2B(sbi, pa-&gt;pa_len)))
 			continue;
 
 		/* non-extent files can't have physical blocks past 2^32 */
 		if (!(ext4_test_inode_flag(ac-&gt;ac_inode, EXT4_INODE_EXTENTS)) &amp;&amp;
-			pa-&gt;pa_pstart + pa-&gt;pa_len &gt; EXT4_MAX_BLOCK_FILE_PHYS)
+		    (pa-&gt;pa_pstart + EXT4_C2B(sbi, pa-&gt;pa_len) &gt;
+		     EXT4_MAX_BLOCK_FILE_PHYS))
 			continue;
 
 		/* found preallocated blocks, use them */
@@ -3412,6 +3421,7 @@ static noinline_for_stack int
 ext4_mb_new_inode_pa(struct ext4_allocation_context *ac)
 {
 	struct super_block *sb = ac-&gt;ac_sb;
+	struct ext4_sb_info *sbi = EXT4_SB(sb);
 	struct ext4_prealloc_space *pa;
 	struct ext4_group_info *grp;
 	struct ext4_inode_info *ei;
@@ -3443,16 +3453,18 @@ ext4_mb_new_inode_pa(struct ext4_allocation_context *ac)
 		winl = ac-&gt;ac_o_ex.fe_logical - ac-&gt;ac_g_ex.fe_logical;
 
 		/* also, we should cover whole original request */
-		wins = ac-&gt;ac_b_ex.fe_len - ac-&gt;ac_o_ex.fe_len;
+		wins = EXT4_C2B(sbi, ac-&gt;ac_b_ex.fe_len - ac-&gt;ac_o_ex.fe_len);
 
 		/* the smallest one defines real window */
 		win = min(winl, wins);
 
-		offs = ac-&gt;ac_o_ex.fe_logical % ac-&gt;ac_b_ex.fe_len;
+		offs = ac-&gt;ac_o_ex.fe_logical %
+			EXT4_C2B(sbi, ac-&gt;ac_b_ex.fe_len);
 		if (offs &amp;&amp; offs &lt; win)
 			win = offs;
 
-		ac-&gt;ac_b_ex.fe_logical = ac-&gt;ac_o_ex.fe_logical - win;
+		ac-&gt;ac_b_ex.fe_logical = ac-&gt;ac_o_ex.fe_logical -
+			EXT4_B2C(sbi, win);
 		BUG_ON(ac-&gt;ac_o_ex.fe_logical &lt; ac-&gt;ac_b_ex.fe_logical);
 		BUG_ON(ac-&gt;ac_o_ex.fe_len &gt; ac-&gt;ac_b_ex.fe_len);
 	}
@@ -3477,7 +3489,7 @@ ext4_mb_new_inode_pa(struct ext4_allocation_context *ac)
 	trace_ext4_mb_new_inode_pa(ac, pa);
 
 	ext4_mb_use_inode_pa(ac, pa);
-	atomic_add(pa-&gt;pa_free, &amp;EXT4_SB(sb)-&gt;s_mb_preallocated);
+	atomic_add(pa-&gt;pa_free, &amp;sbi-&gt;s_mb_preallocated);
 
 	ei = EXT4_I(ac-&gt;ac_inode);
 	grp = ext4_get_group_info(sb, ac-&gt;ac_b_ex.fe_group);
@@ -3592,7 +3604,7 @@ ext4_mb_release_inode_pa(struct ext4_buddy *e4b, struct buffer_head *bitmap_bh,
 
 	BUG_ON(pa-&gt;pa_deleted == 0);
 	ext4_get_group_no_and_offset(sb, pa-&gt;pa_pstart, &amp;group, &amp;bit);
-	grp_blk_start = pa-&gt;pa_pstart - bit;
+	grp_blk_start = pa-&gt;pa_pstart - EXT4_C2B(sbi, bit);
 	BUG_ON(group != e4b-&gt;bd_group &amp;&amp; pa-&gt;pa_len != 0);
 	end = bit + pa-&gt;pa_len;
 
@@ -3607,7 +3619,8 @@ ext4_mb_release_inode_pa(struct ext4_buddy *e4b, struct buffer_head *bitmap_bh,
 		free += next - bit;
 
 		trace_ext4_mballoc_discard(sb, NULL, group, bit, next - bit);
-		trace_ext4_mb_release_inode_pa(pa, grp_blk_start + bit,
+		trace_ext4_mb_release_inode_pa(pa, (grp_blk_start +
+						    EXT4_C2B(sbi, bit)),
 					       next - bit);
 		mb_free_blocks(pa-&gt;pa_inode, e4b, bit, next - bit);
 		bit = next + 1;
@@ -3958,7 +3971,7 @@ static void ext4_mb_group_or_file(struct ext4_allocation_context *ac)
 	if (unlikely(ac-&gt;ac_flags &amp; EXT4_MB_HINT_GOAL_ONLY))
 		return;
 
-	size = ac-&gt;ac_o_ex.fe_logical + ac-&gt;ac_o_ex.fe_len;
+	size = ac-&gt;ac_o_ex.fe_logical + EXT4_C2B(sbi, ac-&gt;ac_o_ex.fe_len);
 	isize = (i_size_read(ac-&gt;ac_inode) + ac-&gt;ac_sb-&gt;s_blocksize - 1)
 		&gt;&gt; bsbits;
 
@@ -4019,18 +4032,15 @@ ext4_mb_initialize_context(struct ext4_allocation_context *ac,
 
 	/* set up allocation goals */
 	memset(ac, 0, sizeof(struct ext4_allocation_context));
-	ac-&gt;ac_b_ex.fe_logical = ar-&gt;logical;
+	ac-&gt;ac_b_ex.fe_logical = ar-&gt;logical &amp; ~(sbi-&gt;s_cluster_ratio - 1);
 	ac-&gt;ac_status = AC_STATUS_CONTINUE;
 	ac-&gt;ac_sb = sb;
 	ac-&gt;ac_inode = ar-&gt;inode;
-	ac-&gt;ac_o_ex.fe_logical = ar-&gt;logical;
+	ac-&gt;ac_o_ex.fe_logical = ac-&gt;ac_b_ex.fe_logical;
 	ac-&gt;ac_o_ex.fe_group = group;
 	ac-&gt;ac_o_ex.fe_start = block;
 	ac-&gt;ac_o_ex.fe_len = len;
-	ac-&gt;ac_g_ex.fe_logical = ar-&gt;logical;
-	ac-&gt;ac_g_ex.fe_group = group;
-	ac-&gt;ac_g_ex.fe_start = block;
-	ac-&gt;ac_g_ex.fe_len = len;
+	ac-&gt;ac_g_ex = ac-&gt;ac_o_ex;
 	ac-&gt;ac_flags = ar-&gt;flags;
 
 	/* we have to define context: we'll we work with a file or
@@ -4182,13 +4192,14 @@ static void ext4_mb_add_n_trim(struct ext4_allocation_context *ac)
  */
 static int ext4_mb_release_context(struct ext4_allocation_context *ac)
 {
+	struct ext4_sb_info *sbi = EXT4_SB(ac-&gt;ac_sb);
 	struct ext4_prealloc_space *pa = ac-&gt;ac_pa;
 	if (pa) {
 		if (pa-&gt;pa_type == MB_GROUP_PA) {
 			/* see comment in ext4_mb_use_group_pa() */
 			spin_lock(&amp;pa-&gt;pa_lock);
-			pa-&gt;pa_pstart += ac-&gt;ac_b_ex.fe_len;
-			pa-&gt;pa_lstart += ac-&gt;ac_b_ex.fe_len;
+			pa-&gt;pa_pstart += EXT4_C2B(sbi, ac-&gt;ac_b_ex.fe_len);
+			pa-&gt;pa_lstart += EXT4_C2B(sbi, ac-&gt;ac_b_ex.fe_len);
 			pa-&gt;pa_free -= ac-&gt;ac_b_ex.fe_len;
 			pa-&gt;pa_len -= ac-&gt;ac_b_ex.fe_len;
 			spin_unlock(&amp;pa-&gt;pa_lock);
@@ -4249,7 +4260,7 @@ ext4_fsblk_t ext4_mb_new_blocks(handle_t *handle,
 	struct super_block *sb;
 	ext4_fsblk_t block = 0;
 	unsigned int inquota = 0;
-	unsigned int reserv_blks = 0;
+	unsigned int reserv_clstrs = 0;
 
 	sb = ar-&gt;inode-&gt;i_sb;
 	sbi = EXT4_SB(sb);
@@ -4279,12 +4290,14 @@ ext4_fsblk_t ext4_mb_new_blocks(handle_t *handle,
 			*errp = -ENOSPC;
 			return 0;
 		}
-		reserv_blks = ar-&gt;len;
+		reserv_clstrs = ar-&gt;len;
 		if (ar-&gt;flags &amp; EXT4_MB_USE_ROOT_BLOCKS) {
-			dquot_alloc_block_nofail(ar-&gt;inode, ar-&gt;len);
+			dquot_alloc_block_nofail(ar-&gt;inode,
+						 EXT4_C2B(sbi, ar-&gt;len));
 		} else {
 			while (ar-&gt;len &amp;&amp;
-				dquot_alloc_block(ar-&gt;inode, ar-&gt;len)) {
+				dquot_alloc_block(ar-&gt;inode,
+						  EXT4_C2B(sbi, ar-&gt;len))) {
 
 				ar-&gt;flags |= EXT4_MB_HINT_NOPREALLOC;
 				ar-&gt;len--;
@@ -4328,7 +4341,7 @@ ext4_fsblk_t ext4_mb_new_blocks(handle_t *handle,
 			ext4_mb_new_preallocation(ac);
 	}
 	if (likely(ac-&gt;ac_status == AC_STATUS_FOUND)) {
-		*errp = ext4_mb_mark_diskspace_used(ac, handle, reserv_blks);
+		*errp = ext4_mb_mark_diskspace_used(ac, handle, reserv_clstrs);
 		if (*errp == -EAGAIN) {
 			/*
 			 * drop the reference that we took
@@ -4364,13 +4377,13 @@ ext4_fsblk_t ext4_mb_new_blocks(handle_t *handle,
 	if (ac)
 		kmem_cache_free(ext4_ac_cachep, ac);
 	if (inquota &amp;&amp; ar-&gt;len &lt; inquota)
-		dquot_free_block(ar-&gt;inode, inquota - ar-&gt;len);
+		dquot_free_block(ar-&gt;inode, EXT4_C2B(sbi, inquota - ar-&gt;len));
 	if (!ar-&gt;len) {
 		if (!ext4_test_inode_state(ar-&gt;inode,
 					   EXT4_STATE_DELALLOC_RESERVED))
 			/* release all the reserved blocks if non delalloc */
 			percpu_counter_sub(&amp;sbi-&gt;s_dirtyblocks_counter,
-						reserv_blks);
+						reserv_clstrs);
 	}
 
 	trace_ext4_allocate_blocks(ar, (unsigned long long)block);
diff --git a/fs/ext4/mballoc.h b/fs/ext4/mballoc.h
index 3cdb8aa9f6b7..1641f4b57439 100644
--- a/fs/ext4/mballoc.h
+++ b/fs/ext4/mballoc.h
@@ -139,9 +139,9 @@ enum {
 
 struct ext4_free_extent {
 	ext4_lblk_t fe_logical;
-	ext4_grpblk_t fe_start;
+	ext4_grpblk_t fe_start;	/* In cluster units */
 	ext4_group_t fe_group;
-	ext4_grpblk_t fe_len;
+	ext4_grpblk_t fe_len;	/* In cluster units */
 };
 
 /*</pre><hr><pre>commit 3212a80a58062056bb922811071062be58d8fee1
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Sep 9 18:46:51 2011 -0400

    ext4: convert block group-relative offsets to use clusters
    
    Certain parts of the ext4 code base, primarily in mballoc.c, use a
    block group number and offset from the beginning of the block group.
    This offset is invariably used to index into the allocation bitmap, so
    change the offset to be denominated in units of clusters.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/balloc.c b/fs/ext4/balloc.c
index 1c6d777b35a2..89abf1f7b253 100644
--- a/fs/ext4/balloc.c
+++ b/fs/ext4/balloc.c
@@ -28,7 +28,8 @@
  */
 
 /*
- * Calculate the block group number and offset, given a block number
+ * Calculate the block group number and offset into the block/cluster
+ * allocation bitmap, given a block number
  */
 void ext4_get_group_no_and_offset(struct super_block *sb, ext4_fsblk_t blocknr,
 		ext4_group_t *blockgrpp, ext4_grpblk_t *offsetp)
@@ -37,7 +38,8 @@ void ext4_get_group_no_and_offset(struct super_block *sb, ext4_fsblk_t blocknr,
 	ext4_grpblk_t offset;
 
 	blocknr = blocknr - le32_to_cpu(es-&gt;s_first_data_block);
-	offset = do_div(blocknr, EXT4_BLOCKS_PER_GROUP(sb));
+	offset = do_div(blocknr, EXT4_BLOCKS_PER_GROUP(sb)) &gt;&gt;
+		EXT4_SB(sb)-&gt;s_cluster_bits;
 	if (offsetp)
 		*offsetp = offset;
 	if (blockgrpp)
diff --git a/fs/ext4/mballoc.h b/fs/ext4/mballoc.h
index 9d4a636b546c..3cdb8aa9f6b7 100644
--- a/fs/ext4/mballoc.h
+++ b/fs/ext4/mballoc.h
@@ -216,6 +216,7 @@ struct ext4_buddy {
 static inline ext4_fsblk_t ext4_grp_offs_to_block(struct super_block *sb,
 					struct ext4_free_extent *fex)
 {
-	return ext4_group_first_block_no(sb, fex-&gt;fe_group) + fex-&gt;fe_start;
+	return ext4_group_first_block_no(sb, fex-&gt;fe_group) +
+		(fex-&gt;fe_start &lt;&lt; EXT4_SB(sb)-&gt;s_cluster_bits);
 }
 #endif</pre>
    <div class="pagination">
        <a href='1_66.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><span>[67]</span><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_68.html'>Next&gt;&gt;</a>
    <div>
</body>

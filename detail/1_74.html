<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_73.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><span>[74]</span><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_75.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit b616844310a6c8a4ab405d3436bbb6e53cfd852f
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Mon Feb 28 13:12:38 2011 -0500

    ext4: optimize ext4_bio_write_page() when no extent conversion is needed
    
    If no extent conversion is required, wake up any processes waiting for
    the page's writeback to be complete and free the ext4_io_end structure
    directly in ext4_end_bio() instead of dropping it on the linked list
    (which requires taking a spinlock to queue and dequeue the io_end
    structure), and waiting for the workqueue to do this work.
    
    This removes an extra scheduling delay before process waiting for an
    fsync() to complete gets woken up, and it also reduces the CPU
    overhead for a random write workload.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/page-io.c b/fs/ext4/page-io.c
index d5c391ffad7a..0cfd03e19d7d 100644
--- a/fs/ext4/page-io.c
+++ b/fs/ext4/page-io.c
@@ -259,6 +259,11 @@ static void ext4_end_bio(struct bio *bio, int error)
 			     bi_sector &gt;&gt; (inode-&gt;i_blkbits - 9));
 	}
 
+	if (!(io_end-&gt;flag &amp; EXT4_IO_END_UNWRITTEN)) {
+		ext4_free_io_end(io_end);
+		return;
+	}
+
 	/* Add the io_end to per-inode completed io list*/
 	spin_lock_irqsave(&amp;EXT4_I(inode)-&gt;i_completed_io_lock, flags);
 	list_add_tail(&amp;io_end-&gt;list, &amp;EXT4_I(inode)-&gt;i_completed_io_list);</pre><hr><pre>commit 4dd89fc6251a6bda2c18e71e7d266e983806579d
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun Feb 27 17:23:47 2011 -0500

    ext4: suppress verbose debugging information if malloc-debug is off
    
    If CONFIG_EXT4_DEBUG is enabled, then if a block allocation fails due
    to disk being full, a verbose debugging message is printed, even if
    the malloc-debug switch has not been enabled.  Suppress the debugging
    message so that nothing is printed unless malloc-debug has been turned
    on.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 66bee7274d6a..2f6f0dd08fca 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -3912,7 +3912,8 @@ static void ext4_mb_show_ac(struct ext4_allocation_context *ac)
 	struct super_block *sb = ac-&gt;ac_sb;
 	ext4_group_t ngroups, i;
 
-	if (EXT4_SB(sb)-&gt;s_mount_flags &amp; EXT4_MF_FS_ABORTED)
+	if (!mb_enable_debug ||
+	    (EXT4_SB(sb)-&gt;s_mount_flags &amp; EXT4_MF_FS_ABORTED))
 		return;
 
 	printk(KERN_ERR "EXT4-fs: Can't allocate:"</pre><hr><pre>commit a54aa76108619e5d8290b49081c2aaaeff5be9a2
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sun Feb 27 16:43:24 2011 -0500

    ext4: don't leave PageWriteback set after memory failure
    
    In ext4_bio_write_page(), if the memory allocation for the struct
    ext4_io_page fails, it returns with the page's PageWriteback flag set.
    This will end up causing the page not to skip writeback in
    WB_SYNC_NONE mode, and in WB_SYNC_ALL mode (i.e., on a sync, fsync, or
    umount) the writeback daemon will get stuck forever on the
    wait_on_page_writeback() function in write_cache_pages_da().
    
    Or, if journalling is enabled and the file gets deleted, it the
    journal thread can get stuck in journal_finish_inode_data_buffers()
    call to filemap_fdatawait().
    
    Another place where things can get hung up is in
    truncate_inode_pages(), called out of ext4_evict_inode().
    
    Fix this by not setting PageWriteback until after we have successfully
    allocated the struct ext4_io_page.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/page-io.c b/fs/ext4/page-io.c
index 68d92a8f71d7..d5c391ffad7a 100644
--- a/fs/ext4/page-io.c
+++ b/fs/ext4/page-io.c
@@ -381,8 +381,6 @@ int ext4_bio_write_page(struct ext4_io_submit *io,
 
 	BUG_ON(!PageLocked(page));
 	BUG_ON(PageWriteback(page));
-	set_page_writeback(page);
-	ClearPageError(page);
 
 	io_page = kmem_cache_alloc(io_page_cachep, GFP_NOFS);
 	if (!io_page) {
@@ -393,6 +391,8 @@ int ext4_bio_write_page(struct ext4_io_submit *io,
 	io_page-&gt;p_page = page;
 	atomic_set(&amp;io_page-&gt;p_count, 1);
 	get_page(page);
+	set_page_writeback(page);
+	ClearPageError(page);
 
 	for (bh = head = page_buffers(page), block_start = 0;
 	     bh != head || !block_start;</pre><hr><pre>commit 168fc0223c0e944957b1f31d88c2334fc904baf1
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 26 14:09:20 2011 -0500

    ext4: move setup of the mpd structure to write_cache_pages_da()
    
    Move the initialization of all of the fields of the mpd structure to
    write_cache_pages_da().  This simplifies the code considerably.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index c2e6af338234..dcc2287433b6 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2714,7 +2714,8 @@ static int ext4_da_writepages_trans_blocks(struct inode *inode)
 /*
  * write_cache_pages_da - walk the list of dirty pages of the given
  * address space and accumulate pages that need writing, and call
- * mpage_da_map_and_submit to map the pages and then write them.
+ * mpage_da_map_and_submit to map a single contiguous memory region
+ * and then write them.
  */
 static int write_cache_pages_da(struct address_space *mapping,
 				struct writeback_control *wbc,
@@ -2722,7 +2723,7 @@ static int write_cache_pages_da(struct address_space *mapping,
 				pgoff_t *done_index)
 {
 	struct buffer_head	*bh, *head;
-	struct inode		*inode = mpd-&gt;inode;
+	struct inode		*inode = mapping-&gt;host;
 	struct pagevec		pvec;
 	unsigned int		nr_pages;
 	sector_t		logical;
@@ -2730,6 +2731,9 @@ static int write_cache_pages_da(struct address_space *mapping,
 	long			nr_to_write = wbc-&gt;nr_to_write;
 	int			i, tag, ret = 0;
 
+	memset(mpd, 0, sizeof(struct mpage_da_data));
+	mpd-&gt;wbc = wbc;
+	mpd-&gt;inode = inode;
 	pagevec_init(&amp;pvec, 0);
 	index = wbc-&gt;range_start &gt;&gt; PAGE_CACHE_SHIFT;
 	end = wbc-&gt;range_end &gt;&gt; PAGE_CACHE_SHIFT;
@@ -2794,16 +2798,8 @@ static int write_cache_pages_da(struct address_space *mapping,
 
 			BUG_ON(PageWriteback(page));
 
-			if (mpd-&gt;next_page != page-&gt;index) {
-				/*
-				 * Start next extent of pages and blocks
-				 */
+			if (mpd-&gt;next_page != page-&gt;index)
 				mpd-&gt;first_page = page-&gt;index;
-				mpd-&gt;b_size = 0;
-				mpd-&gt;b_state = 0;
-				mpd-&gt;b_blocknr = 0;
-			}
-
 			mpd-&gt;next_page = page-&gt;index + 1;
 			logical = (sector_t) page-&gt;index &lt;&lt;
 				(PAGE_CACHE_SHIFT - inode-&gt;i_blkbits);
@@ -2975,9 +2971,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 		wbc-&gt;nr_to_write = desired_nr_to_write;
 	}
 
-	mpd.wbc = wbc;
-	mpd.inode = mapping-&gt;host;
-
 retry:
 	if (wbc-&gt;sync_mode == WB_SYNC_ALL)
 		tag_pages_for_writeback(mapping, index, end);
@@ -3008,14 +3001,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 		 * contiguous region of logical blocks that need
 		 * blocks to be allocated by ext4 and submit them.
 		 */
-		mpd.b_size = 0;
-		mpd.b_state = 0;
-		mpd.b_blocknr = 0;
-		mpd.first_page = 0;
-		mpd.next_page = 0;
-		mpd.io_done = 0;
-		mpd.pages_written = 0;
-		mpd.retval = 0;
 		ret = write_cache_pages_da(mapping, wbc, &amp;mpd, &amp;done_index);
 		/*
 		 * If we have a contiguous extent of pages and we</pre><hr><pre>commit 78aaced3408141bb7c836f2db0ca435790399da5
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 26 14:09:14 2011 -0500

    ext4: don't lock the next page in write_cache_pages if not needed
    
    If we have accumulated a contiguous region of memory to be written
    out, and the next page can added to this region, don't bother locking
    (and then unlocking the page) before writing out the memory.  In the
    unlikely event that the next page was being written back by some other
    CPU, we can also skip waiting that page to finish writeback.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 617c9cbba182..c2e6af338234 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2761,6 +2761,16 @@ static int write_cache_pages_da(struct address_space *mapping,
 
 			*done_index = page-&gt;index + 1;
 
+			/*
+			 * If we can't merge this page, and we have
+			 * accumulated an contiguous region, write it
+			 */
+			if ((mpd-&gt;next_page != page-&gt;index) &amp;&amp;
+			    (mpd-&gt;next_page != mpd-&gt;first_page)) {
+				mpage_da_map_and_submit(mpd);
+				goto ret_extent_tail;
+			}
+
 			lock_page(page);
 
 			/*
@@ -2784,24 +2794,7 @@ static int write_cache_pages_da(struct address_space *mapping,
 
 			BUG_ON(PageWriteback(page));
 
-			/*
-			 * Can we merge this page to current extent?
-			 */
 			if (mpd-&gt;next_page != page-&gt;index) {
-				/*
-				 * Nope, we can't. So, we map
-				 * non-allocated blocks and start IO
-				 * on them
-				 */
-				if (mpd-&gt;next_page != mpd-&gt;first_page) {
-					mpage_da_map_and_submit(mpd);
-					/*
-					 * skip rest of the page in the page_vec
-					 */
-					unlock_page(page);
-					goto ret_extent_tail;
-				}
-
 				/*
 				 * Start next extent of pages and blocks
 				 */</pre><hr><pre>commit ee6ecbcc5d73672217fdea420d182ecb0cdf310c
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 26 14:08:11 2011 -0500

    ext4: remove page_skipped hackery in ext4_da_writepages()
    
    Because the ext4 page writeback codepath had been prematurely calling
    clear_page_dirty_for_io(), if it turned out that a particular page
    couldn't be written out during a particular pass of
    write_cache_pages_da(), the page would have to get redirtied by
    calling redirty_pages_for_writeback().  Not only was this wasted work,
    but redirty_page_for_writeback() would increment wbc-&gt;pages_skipped to
    signal to writeback_sb_inodes() that buffers were locked, and that it
    should skip this inode until later.
    
    Since this signal was incorrect in ext4's case --- which was caused by
    ext4's historically incorrect use of write_cache_pages() ---
    ext4_da_writepages() saved and restored wbc-&gt;skipped_pages to avoid
    confusing writeback_sb_inodes().
    
    Now that we've fixed ext4 to call clear_page_dirty_for_io() right
    before initiating the page I/O, we can nuke the page_skipped
    save/restore hackery, and breathe a sigh of relief.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index ae6e2f43d873..617c9cbba182 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2900,7 +2900,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 	struct mpage_da_data mpd;
 	struct inode *inode = mapping-&gt;host;
 	int pages_written = 0;
-	long pages_skipped;
 	unsigned int max_pages;
 	int range_cyclic, cycled = 1, io_done = 0;
 	int needed_blocks, ret = 0;
@@ -2986,8 +2985,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 	mpd.wbc = wbc;
 	mpd.inode = mapping-&gt;host;
 
-	pages_skipped = wbc-&gt;pages_skipped;
-
 retry:
 	if (wbc-&gt;sync_mode == WB_SYNC_ALL)
 		tag_pages_for_writeback(mapping, index, end);
@@ -3047,7 +3044,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 			 * and try again
 			 */
 			jbd2_journal_force_commit_nested(sbi-&gt;s_journal);
-			wbc-&gt;pages_skipped = pages_skipped;
 			ret = 0;
 		} else if (ret == MPAGE_DA_EXTENT_TAIL) {
 			/*
@@ -3055,7 +3051,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 			 * rest of the pages
 			 */
 			pages_written += mpd.pages_written;
-			wbc-&gt;pages_skipped = pages_skipped;
 			ret = 0;
 			io_done = 1;
 		} else if (wbc-&gt;nr_to_write)
@@ -3073,11 +3068,6 @@ static int ext4_da_writepages(struct address_space *mapping,
 		wbc-&gt;range_end  = mapping-&gt;writeback_index - 1;
 		goto retry;
 	}
-	if (pages_skipped != wbc-&gt;pages_skipped)
-		ext4_msg(inode-&gt;i_sb, KERN_CRIT,
-			 "This should not happen leaving %s "
-			 "with nr_to_write = %ld ret = %d",
-			 __func__, wbc-&gt;nr_to_write, ret);
 
 	/* Update index */
 	wbc-&gt;range_cyclic = range_cyclic;</pre><hr><pre>commit 9749895644a817cfd28a535bc3ae60e4267bdc50
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 26 14:08:01 2011 -0500

    ext4: clear the dirty bit for a page in writeback at the last minute
    
    Move when we call clear_page_dirty_for_io() to just before we actually
    write the page.  This simplifies the code somewhat, and avoids marking
    pages as clean and then needing to remark them as dirty later.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 1e718e87f466..ae6e2f43d873 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2060,7 +2060,7 @@ static int mpage_da_submit_io(struct mpage_da_data *mpd,
 		if (nr_pages == 0)
 			break;
 		for (i = 0; i &lt; nr_pages; i++) {
-			int commit_write = 0, redirty_page = 0;
+			int commit_write = 0, skip_page = 0;
 			struct page *page = pvec.pages[i];
 
 			index = page-&gt;index;
@@ -2086,14 +2086,12 @@ static int mpage_da_submit_io(struct mpage_da_data *mpd,
 			 * If the page does not have buffers (for
 			 * whatever reason), try to create them using
 			 * __block_write_begin.  If this fails,
-			 * redirty the page and move on.
+			 * skip the page and move on.
 			 */
 			if (!page_has_buffers(page)) {
 				if (__block_write_begin(page, 0, len,
 						noalloc_get_block_write)) {
-				redirty_page:
-					redirty_page_for_writepage(mpd-&gt;wbc,
-								   page);
+				skip_page:
 					unlock_page(page);
 					continue;
 				}
@@ -2104,7 +2102,7 @@ static int mpage_da_submit_io(struct mpage_da_data *mpd,
 			block_start = 0;
 			do {
 				if (!bh)
-					goto redirty_page;
+					goto skip_page;
 				if (map &amp;&amp; (cur_logical &gt;= map-&gt;m_lblk) &amp;&amp;
 				    (cur_logical &lt;= (map-&gt;m_lblk +
 						     (map-&gt;m_len - 1)))) {
@@ -2120,22 +2118,23 @@ static int mpage_da_submit_io(struct mpage_da_data *mpd,
 					clear_buffer_unwritten(bh);
 				}
 
-				/* redirty page if block allocation undone */
+				/* skip page if block allocation undone */
 				if (buffer_delay(bh) || buffer_unwritten(bh))
-					redirty_page = 1;
+					skip_page = 1;
 				bh = bh-&gt;b_this_page;
 				block_start += bh-&gt;b_size;
 				cur_logical++;
 				pblock++;
 			} while (bh != page_bufs);
 
-			if (redirty_page)
-				goto redirty_page;
+			if (skip_page)
+				goto skip_page;
 
 			if (commit_write)
 				/* mark the buffer_heads as dirty &amp; uptodate */
 				block_commit_write(page, 0, len);
 
+			clear_page_dirty_for_io(page);
 			/*
 			 * Delalloc doesn't support data journalling,
 			 * but eventually maybe we'll lift this
@@ -2277,9 +2276,8 @@ static void mpage_da_map_and_submit(struct mpage_da_data *mpd)
 		err = blks;
 		/*
 		 * If get block returns EAGAIN or ENOSPC and there
-		 * appears to be free blocks we will call
-		 * ext4_writepage() for all of the pages which will
-		 * just redirty the pages.
+		 * appears to be free blocks we will just let
+		 * mpage_da_submit_io() unlock all of the pages.
 		 */
 		if (err == -EAGAIN)
 			goto submit_io;
@@ -2777,7 +2775,6 @@ static int write_cache_pages_da(struct address_space *mapping,
 			    (PageWriteback(page) &amp;&amp;
 			     (wbc-&gt;sync_mode == WB_SYNC_NONE)) ||
 			    unlikely(page-&gt;mapping != mapping)) {
-			continue_unlock:
 				unlock_page(page);
 				continue;
 			}
@@ -2786,8 +2783,6 @@ static int write_cache_pages_da(struct address_space *mapping,
 				wait_on_page_writeback(page);
 
 			BUG_ON(PageWriteback(page));
-			if (!clear_page_dirty_for_io(page))
-				goto continue_unlock;
 
 			/*
 			 * Can we merge this page to current extent?
@@ -2803,7 +2798,6 @@ static int write_cache_pages_da(struct address_space *mapping,
 					/*
 					 * skip rest of the page in the page_vec
 					 */
-					redirty_page_for_writepage(wbc, page);
 					unlock_page(page);
 					goto ret_extent_tail;
 				}</pre><hr><pre>commit 4f01b02c8c4e4111bd1adbcafb5741e8e991f5fd
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 26 14:07:37 2011 -0500

    ext4: simple cleanups to write_cache_pages_da()
    
    Eliminate duplicate code, unneeded variables, etc., to make it easier
    to understand the code.  No behavioral changes were made in this patch.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index fcd08ca0643b..1e718e87f466 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2723,17 +2723,14 @@ static int write_cache_pages_da(struct address_space *mapping,
 				struct mpage_da_data *mpd,
 				pgoff_t *done_index)
 {
-	struct inode *inode = mpd-&gt;inode;
-	struct buffer_head *bh, *head;
-	sector_t logical;
-	int ret = 0;
-	int done = 0;
-	struct pagevec pvec;
-	unsigned nr_pages;
-	pgoff_t index;
-	pgoff_t end;		/* Inclusive */
-	long nr_to_write = wbc-&gt;nr_to_write;
-	int tag;
+	struct buffer_head	*bh, *head;
+	struct inode		*inode = mpd-&gt;inode;
+	struct pagevec		pvec;
+	unsigned int		nr_pages;
+	sector_t		logical;
+	pgoff_t			index, end;
+	long			nr_to_write = wbc-&gt;nr_to_write;
+	int			i, tag, ret = 0;
 
 	pagevec_init(&amp;pvec, 0);
 	index = wbc-&gt;range_start &gt;&gt; PAGE_CACHE_SHIFT;
@@ -2745,13 +2742,11 @@ static int write_cache_pages_da(struct address_space *mapping,
 		tag = PAGECACHE_TAG_DIRTY;
 
 	*done_index = index;
-	while (!done &amp;&amp; (index &lt;= end)) {
-		int i;
-
+	while (index &lt;= end) {
 		nr_pages = pagevec_lookup_tag(&amp;pvec, mapping, &amp;index, tag,
 			      min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1);
 		if (nr_pages == 0)
-			break;
+			return 0;
 
 		for (i = 0; i &lt; nr_pages; i++) {
 			struct page *page = pvec.pages[i];
@@ -2763,47 +2758,37 @@ static int write_cache_pages_da(struct address_space *mapping,
 			 * mapping. However, page-&gt;index will not change
 			 * because we have a reference on the page.
 			 */
-			if (page-&gt;index &gt; end) {
-				done = 1;
-				break;
-			}
+			if (page-&gt;index &gt; end)
+				goto out;
 
 			*done_index = page-&gt;index + 1;
 
 			lock_page(page);
 
 			/*
-			 * Page truncated or invalidated. We can freely skip it
-			 * then, even for data integrity operations: the page
-			 * has disappeared concurrently, so there could be no
-			 * real expectation of this data interity operation
-			 * even if there is now a new, dirty page at the same
-			 * pagecache address.
+			 * If the page is no longer dirty, or its
+			 * mapping no longer corresponds to inode we
+			 * are writing (which means it has been
+			 * truncated or invalidated), or the page is
+			 * already under writeback and we are not
+			 * doing a data integrity writeback, skip the page
 			 */
-			if (unlikely(page-&gt;mapping != mapping)) {
-continue_unlock:
+			if (!PageDirty(page) ||
+			    (PageWriteback(page) &amp;&amp;
+			     (wbc-&gt;sync_mode == WB_SYNC_NONE)) ||
+			    unlikely(page-&gt;mapping != mapping)) {
+			continue_unlock:
 				unlock_page(page);
 				continue;
 			}
 
-			if (!PageDirty(page)) {
-				/* someone wrote it for us */
-				goto continue_unlock;
-			}
-
-			if (PageWriteback(page)) {
-				if (wbc-&gt;sync_mode != WB_SYNC_NONE)
-					wait_on_page_writeback(page);
-				else
-					goto continue_unlock;
-			}
+			if (PageWriteback(page))
+				wait_on_page_writeback(page);
 
 			BUG_ON(PageWriteback(page));
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
-			/* BEGIN __mpage_da_writepage */
-
 			/*
 			 * Can we merge this page to current extent?
 			 */
@@ -2820,8 +2805,7 @@ static int write_cache_pages_da(struct address_space *mapping,
 					 */
 					redirty_page_for_writepage(wbc, page);
 					unlock_page(page);
-					ret = MPAGE_DA_EXTENT_TAIL;
-					goto out;
+					goto ret_extent_tail;
 				}
 
 				/*
@@ -2838,15 +2822,15 @@ static int write_cache_pages_da(struct address_space *mapping,
 				(PAGE_CACHE_SHIFT - inode-&gt;i_blkbits);
 
 			if (!page_has_buffers(page)) {
-				mpage_add_bh_to_extent(mpd, logical, PAGE_CACHE_SIZE,
+				mpage_add_bh_to_extent(mpd, logical,
+						       PAGE_CACHE_SIZE,
 						       (1 &lt;&lt; BH_Dirty) | (1 &lt;&lt; BH_Uptodate));
-				if (mpd-&gt;io_done) {
-					ret = MPAGE_DA_EXTENT_TAIL;
-					goto out;
-				}
+				if (mpd-&gt;io_done)
+					goto ret_extent_tail;
 			} else {
 				/*
-				 * Page with regular buffer heads, just add all dirty ones
+				 * Page with regular buffer heads,
+				 * just add all dirty ones
 				 */
 				head = page_buffers(page);
 				bh = head;
@@ -2862,18 +2846,19 @@ static int write_cache_pages_da(struct address_space *mapping,
 						mpage_add_bh_to_extent(mpd, logical,
 								       bh-&gt;b_size,
 								       bh-&gt;b_state);
-						if (mpd-&gt;io_done) {
-							ret = MPAGE_DA_EXTENT_TAIL;
-							goto out;
-						}
+						if (mpd-&gt;io_done)
+							goto ret_extent_tail;
 					} else if (buffer_dirty(bh) &amp;&amp; (buffer_mapped(bh))) {
 						/*
-						 * mapped dirty buffer. We need to update
-						 * the b_state because we look at
-						 * b_state in mpage_da_map_blocks. We don't
-						 * update b_size because if we find an
-						 * unmapped buffer_head later we need to
-						 * use the b_state flag of that buffer_head.
+						 * mapped dirty buffer. We need
+						 * to update the b_state
+						 * because we look at b_state
+						 * in mpage_da_map_blocks.  We
+						 * don't update b_size because
+						 * if we find an unmapped
+						 * buffer_head later we need to
+						 * use the b_state flag of that
+						 * buffer_head.
 						 */
 						if (mpd-&gt;b_size == 0)
 							mpd-&gt;b_state = bh-&gt;b_state &amp; BH_FLAGS;
@@ -2882,14 +2867,10 @@ static int write_cache_pages_da(struct address_space *mapping,
 				} while ((bh = bh-&gt;b_this_page) != head);
 			}
 
-			ret = 0;
-
-			/* END __mpage_da_writepage */
-
 			if (nr_to_write &gt; 0) {
 				nr_to_write--;
 				if (nr_to_write == 0 &amp;&amp;
-				    wbc-&gt;sync_mode == WB_SYNC_NONE) {
+				    wbc-&gt;sync_mode == WB_SYNC_NONE)
 					/*
 					 * We stop writing back only if we are
 					 * not doing integrity sync. In case of
@@ -2900,15 +2881,15 @@ static int write_cache_pages_da(struct address_space *mapping,
 					 * pages, but have not synced all of the
 					 * old dirty pages.
 					 */
-					done = 1;
-					break;
-				}
+					goto out;
 			}
 		}
 		pagevec_release(&amp;pvec);
 		cond_resched();
 	}
-	return ret;
+	return 0;
+ret_extent_tail:
+	ret = MPAGE_DA_EXTENT_TAIL;
 out:
 	pagevec_release(&amp;pvec);
 	cond_resched();</pre><hr><pre>commit 8eb9e5ce211de1b98bc84e93258b7db0860a103c
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 26 14:07:31 2011 -0500

    ext4: fold __mpage_da_writepage() into write_cache_pages_da()
    
    Fold the __mpage_da_writepage() function into write_cache_pages_da().
    This will give us opportunities to clean up and simplify the resulting
    code.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index e878c3a7aaf0..fcd08ca0643b 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -2437,102 +2437,6 @@ static int ext4_bh_delay_or_unwritten(handle_t *handle, struct buffer_head *bh)
 	return (buffer_delay(bh) || buffer_unwritten(bh)) &amp;&amp; buffer_dirty(bh);
 }
 
-/*
- * __mpage_da_writepage - finds extent of pages and blocks
- *
- * @page: page to consider
- * @wbc: not used, we just follow rules
- * @data: context
- *
- * The function finds extents of pages and scan them for all blocks.
- */
-static int __mpage_da_writepage(struct page *page,
-				struct writeback_control *wbc,
-				struct mpage_da_data *mpd)
-{
-	struct inode *inode = mpd-&gt;inode;
-	struct buffer_head *bh, *head;
-	sector_t logical;
-
-	/*
-	 * Can we merge this page to current extent?
-	 */
-	if (mpd-&gt;next_page != page-&gt;index) {
-		/*
-		 * Nope, we can't. So, we map non-allocated blocks
-		 * and start IO on them
-		 */
-		if (mpd-&gt;next_page != mpd-&gt;first_page) {
-			mpage_da_map_and_submit(mpd);
-			/*
-			 * skip rest of the page in the page_vec
-			 */
-			redirty_page_for_writepage(wbc, page);
-			unlock_page(page);
-			return MPAGE_DA_EXTENT_TAIL;
-		}
-
-		/*
-		 * Start next extent of pages ...
-		 */
-		mpd-&gt;first_page = page-&gt;index;
-
-		/*
-		 * ... and blocks
-		 */
-		mpd-&gt;b_size = 0;
-		mpd-&gt;b_state = 0;
-		mpd-&gt;b_blocknr = 0;
-	}
-
-	mpd-&gt;next_page = page-&gt;index + 1;
-	logical = (sector_t) page-&gt;index &lt;&lt;
-		  (PAGE_CACHE_SHIFT - inode-&gt;i_blkbits);
-
-	if (!page_has_buffers(page)) {
-		mpage_add_bh_to_extent(mpd, logical, PAGE_CACHE_SIZE,
-				       (1 &lt;&lt; BH_Dirty) | (1 &lt;&lt; BH_Uptodate));
-		if (mpd-&gt;io_done)
-			return MPAGE_DA_EXTENT_TAIL;
-	} else {
-		/*
-		 * Page with regular buffer heads, just add all dirty ones
-		 */
-		head = page_buffers(page);
-		bh = head;
-		do {
-			BUG_ON(buffer_locked(bh));
-			/*
-			 * We need to try to allocate
-			 * unmapped blocks in the same page.
-			 * Otherwise we won't make progress
-			 * with the page in ext4_writepage
-			 */
-			if (ext4_bh_delay_or_unwritten(NULL, bh)) {
-				mpage_add_bh_to_extent(mpd, logical,
-						       bh-&gt;b_size,
-						       bh-&gt;b_state);
-				if (mpd-&gt;io_done)
-					return MPAGE_DA_EXTENT_TAIL;
-			} else if (buffer_dirty(bh) &amp;&amp; (buffer_mapped(bh))) {
-				/*
-				 * mapped dirty buffer. We need to update
-				 * the b_state because we look at
-				 * b_state in mpage_da_map_blocks. We don't
-				 * update b_size because if we find an
-				 * unmapped buffer_head later we need to
-				 * use the b_state flag of that buffer_head.
-				 */
-				if (mpd-&gt;b_size == 0)
-					mpd-&gt;b_state = bh-&gt;b_state &amp; BH_FLAGS;
-			}
-			logical++;
-		} while ((bh = bh-&gt;b_this_page) != head);
-	}
-
-	return 0;
-}
-
 /*
  * This is a special get_blocks_t callback which is used by
  * ext4_da_write_begin().  It will either return mapped block or
@@ -2811,18 +2715,17 @@ static int ext4_da_writepages_trans_blocks(struct inode *inode)
 
 /*
  * write_cache_pages_da - walk the list of dirty pages of the given
- * address space and call the callback function (which usually writes
- * the pages).
- *
- * This is a forked version of write_cache_pages().  Differences:
- *	Range cyclic is ignored.
- *	no_nrwrite_index_update is always presumed true
+ * address space and accumulate pages that need writing, and call
+ * mpage_da_map_and_submit to map the pages and then write them.
  */
 static int write_cache_pages_da(struct address_space *mapping,
 				struct writeback_control *wbc,
 				struct mpage_da_data *mpd,
 				pgoff_t *done_index)
 {
+	struct inode *inode = mpd-&gt;inode;
+	struct buffer_head *bh, *head;
+	sector_t logical;
 	int ret = 0;
 	int done = 0;
 	struct pagevec pvec;
@@ -2899,17 +2802,90 @@ static int write_cache_pages_da(struct address_space *mapping,
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
-			ret = __mpage_da_writepage(page, wbc, mpd);
-			if (unlikely(ret)) {
-				if (ret == AOP_WRITEPAGE_ACTIVATE) {
+			/* BEGIN __mpage_da_writepage */
+
+			/*
+			 * Can we merge this page to current extent?
+			 */
+			if (mpd-&gt;next_page != page-&gt;index) {
+				/*
+				 * Nope, we can't. So, we map
+				 * non-allocated blocks and start IO
+				 * on them
+				 */
+				if (mpd-&gt;next_page != mpd-&gt;first_page) {
+					mpage_da_map_and_submit(mpd);
+					/*
+					 * skip rest of the page in the page_vec
+					 */
+					redirty_page_for_writepage(wbc, page);
 					unlock_page(page);
-					ret = 0;
-				} else {
-					done = 1;
-					break;
+					ret = MPAGE_DA_EXTENT_TAIL;
+					goto out;
 				}
+
+				/*
+				 * Start next extent of pages and blocks
+				 */
+				mpd-&gt;first_page = page-&gt;index;
+				mpd-&gt;b_size = 0;
+				mpd-&gt;b_state = 0;
+				mpd-&gt;b_blocknr = 0;
+			}
+
+			mpd-&gt;next_page = page-&gt;index + 1;
+			logical = (sector_t) page-&gt;index &lt;&lt;
+				(PAGE_CACHE_SHIFT - inode-&gt;i_blkbits);
+
+			if (!page_has_buffers(page)) {
+				mpage_add_bh_to_extent(mpd, logical, PAGE_CACHE_SIZE,
+						       (1 &lt;&lt; BH_Dirty) | (1 &lt;&lt; BH_Uptodate));
+				if (mpd-&gt;io_done) {
+					ret = MPAGE_DA_EXTENT_TAIL;
+					goto out;
+				}
+			} else {
+				/*
+				 * Page with regular buffer heads, just add all dirty ones
+				 */
+				head = page_buffers(page);
+				bh = head;
+				do {
+					BUG_ON(buffer_locked(bh));
+					/*
+					 * We need to try to allocate
+					 * unmapped blocks in the same page.
+					 * Otherwise we won't make progress
+					 * with the page in ext4_writepage
+					 */
+					if (ext4_bh_delay_or_unwritten(NULL, bh)) {
+						mpage_add_bh_to_extent(mpd, logical,
+								       bh-&gt;b_size,
+								       bh-&gt;b_state);
+						if (mpd-&gt;io_done) {
+							ret = MPAGE_DA_EXTENT_TAIL;
+							goto out;
+						}
+					} else if (buffer_dirty(bh) &amp;&amp; (buffer_mapped(bh))) {
+						/*
+						 * mapped dirty buffer. We need to update
+						 * the b_state because we look at
+						 * b_state in mpage_da_map_blocks. We don't
+						 * update b_size because if we find an
+						 * unmapped buffer_head later we need to
+						 * use the b_state flag of that buffer_head.
+						 */
+						if (mpd-&gt;b_size == 0)
+							mpd-&gt;b_state = bh-&gt;b_state &amp; BH_FLAGS;
+					}
+					logical++;
+				} while ((bh = bh-&gt;b_this_page) != head);
 			}
 
+			ret = 0;
+
+			/* END __mpage_da_writepage */
+
 			if (nr_to_write &gt; 0) {
 				nr_to_write--;
 				if (nr_to_write == 0 &amp;&amp;
@@ -2933,6 +2909,10 @@ static int write_cache_pages_da(struct address_space *mapping,
 		cond_resched();
 	}
 	return ret;
+out:
+	pagevec_release(&amp;pvec);
+	cond_resched();
+	return ret;
 }
 
 
@@ -3059,13 +3039,9 @@ static int ext4_da_writepages(struct address_space *mapping,
 		}
 
 		/*
-		 * Now call __mpage_da_writepage to find the next
+		 * Now call write_cache_pages_da() to find the next
 		 * contiguous region of logical blocks that need
-		 * blocks to be allocated by ext4.  We don't actually
-		 * submit the blocks for I/O here, even though
-		 * write_cache_pages thinks it will, and will set the
-		 * pages as clean for write before calling
-		 * __mpage_da_writepage().
+		 * blocks to be allocated by ext4 and submit them.
 		 */
 		mpd.b_size = 0;
 		mpd.b_state = 0;</pre><hr><pre>commit 6fd7a46781999c32f423025767e43b349b967d57
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Feb 26 13:53:09 2011 -0500

    ext4: enable mblk_io_submit by default
    
    Now that we've fixed the file corruption bug in commit d50bdd5aa55,
    it's time to enable mblk_io_submit by default.
    
    Signed-off-by: "Theodore Ts'o" &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index a665d2fb70c1..33c398785e53 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -1038,8 +1038,8 @@ static int ext4_show_options(struct seq_file *seq, struct vfsmount *vfs)
 	    !(def_mount_opts &amp; EXT4_DEFM_NODELALLOC))
 		seq_puts(seq, ",nodelalloc");
 
-	if (test_opt(sb, MBLK_IO_SUBMIT))
-		seq_puts(seq, ",mblk_io_submit");
+	if (!test_opt(sb, MBLK_IO_SUBMIT))
+		seq_puts(seq, ",nomblk_io_submit");
 	if (sbi-&gt;s_stripe)
 		seq_printf(seq, ",stripe=%lu", sbi-&gt;s_stripe);
 	/*
@@ -3099,6 +3099,7 @@ static int ext4_fill_super(struct super_block *sb, void *data, int silent)
 #ifdef CONFIG_EXT4_FS_POSIX_ACL
 	set_opt(sb, POSIX_ACL);
 #endif
+	set_opt(sb, MBLK_IO_SUBMIT);
 	if ((def_mount_opts &amp; EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_DATA)
 		set_opt(sb, JOURNAL_DATA);
 	else if ((def_mount_opts &amp; EXT4_DEFM_JMODE) == EXT4_DEFM_JMODE_ORDERED)</pre>
    <div class="pagination">
        <a href='1_73.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><span>[74]</span><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_75.html'>Next&gt;&gt;</a>
    <div>
</body>

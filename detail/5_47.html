<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by University of South Carolina</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by University of South Carolina</h1>
    <div class="pagination">
        <a href='5_46.html'>&lt;&lt;Prev</a><a href='5.html'>1</a><a href='5_2.html'>2</a><a href='5_3.html'>3</a><a href='5_4.html'>4</a><a href='5_5.html'>5</a><a href='5_6.html'>6</a><a href='5_7.html'>7</a><a href='5_8.html'>8</a><a href='5_9.html'>9</a><a href='5_10.html'>10</a><a href='5_11.html'>11</a><a href='5_12.html'>12</a><a href='5_13.html'>13</a><a href='5_14.html'>14</a><a href='5_15.html'>15</a><a href='5_16.html'>16</a><a href='5_17.html'>17</a><a href='5_18.html'>18</a><a href='5_19.html'>19</a><a href='5_20.html'>20</a><a href='5_21.html'>21</a><a href='5_22.html'>22</a><a href='5_23.html'>23</a><a href='5_24.html'>24</a><a href='5_25.html'>25</a><a href='5_26.html'>26</a><a href='5_27.html'>27</a><a href='5_28.html'>28</a><a href='5_29.html'>29</a><a href='5_30.html'>30</a><a href='5_31.html'>31</a><a href='5_32.html'>32</a><a href='5_33.html'>33</a><a href='5_34.html'>34</a><a href='5_35.html'>35</a><a href='5_36.html'>36</a><a href='5_37.html'>37</a><a href='5_38.html'>38</a><a href='5_39.html'>39</a><a href='5_40.html'>40</a><a href='5_41.html'>41</a><a href='5_42.html'>42</a><a href='5_43.html'>43</a><a href='5_44.html'>44</a><a href='5_45.html'>45</a><a href='5_46.html'>46</a><span>[47]</span><a href='5_48.html'>48</a><a href='5_48.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 53cdcc04c1e85d4e423b2822b66149b6f2e52c2c
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Fri Mar 16 15:04:03 2007 -0700

    [TCP]: Fix tcp_mem[] initialization.
    
    Change tcp_mem initialization function.  The fraction of total memory
    is now a continuous function of memory size, and independent of page
    size.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 74c4d103ebc2..3834b10b5115 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -2458,11 +2458,18 @@ void __init tcp_init(void)
 		sysctl_max_syn_backlog = 128;
 	}
 
-	/* Allow no more than 3/4 kernel memory (usually less) allocated to TCP */
-	sysctl_tcp_mem[0] = (1536 / sizeof (struct inet_bind_hashbucket)) &lt;&lt; order;
-	sysctl_tcp_mem[1] = sysctl_tcp_mem[0] * 4 / 3;
+	/* Set the pressure threshold to be a fraction of global memory that
+	 * is up to 1/2 at 256 MB, decreasing toward zero with the amount of
+	 * memory, with a floor of 128 pages.
+	 */
+	limit = min(nr_all_pages, 1UL&lt;&lt;(28-PAGE_SHIFT)) &gt;&gt; (20-PAGE_SHIFT);
+	limit = (limit * (nr_all_pages &gt;&gt; (20-PAGE_SHIFT))) &gt;&gt; (PAGE_SHIFT-11);
+	limit = max(limit, 128UL);
+	sysctl_tcp_mem[0] = limit / 4 * 3;
+	sysctl_tcp_mem[1] = limit;
 	sysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;
 
+	/* Set per-socket limits to no more than 1/128 the pressure threshold */
 	limit = ((unsigned long)sysctl_tcp_mem[1]) &lt;&lt; (PAGE_SHIFT - 7);
 	max_share = min(4UL*1024*1024, limit);
 </pre><hr><pre>commit 71599cd1c381d1b5f58c35653ac1d3627c6276db
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Tue Feb 27 10:03:56 2007 -0800

    [TCP]: Document several sysctls.
    
    This adds documentation for tcp_moderate_rcvbuf, tcp_no_metrics_save,
    tcp_base_mss, and tcp_mtu_probing.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/Documentation/networking/ip-sysctl.txt b/Documentation/networking/ip-sysctl.txt
index a0f6842368c3..d3aae1f9b4c1 100644
--- a/Documentation/networking/ip-sysctl.txt
+++ b/Documentation/networking/ip-sysctl.txt
@@ -147,6 +147,11 @@ tcp_available_congestion_control - STRING
 	More congestion control algorithms may be available as modules,
 	but not loaded.
 
+tcp_base_mss - INTEGER
+	The initial value of search_low to be used by Packetization Layer
+	Path MTU Discovery (MTU probing).  If MTU probing is enabled,
+	this is the inital MSS used by the connection.
+
 tcp_congestion_control - STRING
 	Set the congestion control algorithm to be used for new
 	connections. The algorithm "reno" is always available, but
@@ -243,6 +248,27 @@ tcp_mem - vector of 3 INTEGERs: min, pressure, max
 	Defaults are calculated at boot time from amount of available
 	memory.
 
+tcp_moderate_rcvbuf - BOOLEAN
+	If set, TCP performs receive buffer autotuning, attempting to
+	automatically size the buffer (no greater than tcp_rmem[2]) to
+	match the size required by the path for full throughput.  Enabled by
+	default.
+
+tcp_mtu_probing - INTEGER
+	Controls TCP Packetization-Layer Path MTU Discovery.  Takes three
+	values:
+	  0 - Disabled
+	  1 - Disabled by default, enabled when an ICMP black hole detected
+	  2 - Always enabled, use initial MSS of tcp_base_mss.
+
+tcp_no_metrics_save - BOOLEAN
+	By default, TCP saves various connection metrics in the route cache
+	when the connection closes, so that connections established in the
+	near future can use these to set initial conditions.  Usually, this
+	increases overall performance, but may sometimes cause performance
+	degredation.  If set, TCP will not cache metrics on closing
+	connections.
+
 tcp_orphan_retries - INTEGER
 	How may times to retry before killing TCP connection, closed
 	by our side. Default value 7 corresponds to ~50sec-16min</pre><hr><pre>commit 104439a8876a98eac1b6593907a3c7bc51e362fe
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Mon Feb 5 17:53:11 2007 -0800

    [TCP]: Don't apply FIN exception to full TSO segments.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index 975f4472af29..58b7111523f4 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -965,7 +965,8 @@ static inline unsigned int tcp_cwnd_test(struct tcp_sock *tp, struct sk_buff *sk
 	u32 in_flight, cwnd;
 
 	/* Don't be strict about the congestion window for the final FIN.  */
-	if (TCP_SKB_CB(skb)-&gt;flags &amp; TCPCB_FLAG_FIN)
+	if ((TCP_SKB_CB(skb)-&gt;flags &amp; TCPCB_FLAG_FIN) &amp;&amp;
+	    tcp_skb_pcount(skb) == 1)
 		return 1;
 
 	in_flight = tcp_packets_in_flight(tp);</pre><hr><pre>commit 52bf376c63eebe72e862a1a6e713976b038c3f50
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Tue Nov 14 20:25:17 2006 -0800

    [TCP]: Fix up sysctl_tcp_mem initialization.
    
    Fix up tcp_mem initial settings to take into account the size of the
    hash entries (different on SMP and non-SMP systems).
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 4322318ab332..c05e8edaf544 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -2316,9 +2316,10 @@ void __init tcp_init(void)
 		sysctl_max_syn_backlog = 128;
 	}
 
-	sysctl_tcp_mem[0] =  768 &lt;&lt; order;
-	sysctl_tcp_mem[1] = 1024 &lt;&lt; order;
-	sysctl_tcp_mem[2] = 1536 &lt;&lt; order;
+	/* Allow no more than 3/4 kernel memory (usually less) allocated to TCP */
+	sysctl_tcp_mem[0] = (1536 / sizeof (struct inet_bind_hashbucket)) &lt;&lt; order;
+	sysctl_tcp_mem[1] = sysctl_tcp_mem[0] * 4 / 3;
+	sysctl_tcp_mem[2] = sysctl_tcp_mem[0] * 2;
 
 	limit = ((unsigned long)sysctl_tcp_mem[1]) &lt;&lt; (PAGE_SHIFT - 7);
 	max_share = min(4UL*1024*1024, limit);</pre><hr><pre>commit 9e950efa20dc8037c27509666cba6999da9368e8
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Mon Nov 6 23:10:51 2006 -0800

    [TCP]: Don't use highmem in tcp hash size calculation.
    
    This patch removes consideration of high memory when determining TCP
    hash table sizes.  Taking into account high memory results in tcp_mem
    values that are too large.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 66e9a729f6df..4322318ab332 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -2270,7 +2270,7 @@ void __init tcp_init(void)
 					thash_entries,
 					(num_physpages &gt;= 128 * 1024) ?
 					13 : 15,
-					HASH_HIGHMEM,
+					0,
 					&amp;tcp_hashinfo.ehash_size,
 					NULL,
 					0);
@@ -2286,7 +2286,7 @@ void __init tcp_init(void)
 					tcp_hashinfo.ehash_size,
 					(num_physpages &gt;= 128 * 1024) ?
 					13 : 15,
-					HASH_HIGHMEM,
+					0,
 					&amp;tcp_hashinfo.bhash_size,
 					NULL,
 					64 * 1024);</pre><hr><pre>commit ae8064ac32d07f609114d73928cdef803be87134
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Wed Oct 18 20:36:48 2006 -0700

    [TCP]: Bound TSO defer time
    
    This patch limits the amount of time you will defer sending a TSO segment
    to less than two clock ticks, or the time between two acks, whichever is
    longer.
    
    On slow links, deferring causes significant bursts.  See attached plots,
    which show RTT through a 1 Mbps link with a 100 ms RTT and ~100 ms queue
    for (a) non-TSO, (b) currnet TSO, and (c) patched TSO.  This burstiness
    causes significant jitter, tends to overflow queues early (bad for short
    queues), and makes delay-based congestion control more difficult.
    
    Deferring by a couple clock ticks I believe will have a relatively small
    impact on performance.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index 0e058a2d1c6d..2d36f6db3706 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -342,6 +342,8 @@ struct tcp_sock {
 
 	unsigned long last_synq_overflow; 
 
+	__u32	tso_deferred;
+
 /* Receiver side RTT estimation */
 	struct {
 		__u32	rtt;
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index f22536e32cb1..ca406157724c 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -1096,10 +1096,14 @@ static int tcp_tso_should_defer(struct sock *sk, struct tcp_sock *tp, struct sk_
 	u32 send_win, cong_win, limit, in_flight;
 
 	if (TCP_SKB_CB(skb)-&gt;flags &amp; TCPCB_FLAG_FIN)
-		return 0;
+		goto send_now;
 
 	if (icsk-&gt;icsk_ca_state != TCP_CA_Open)
-		return 0;
+		goto send_now;
+
+	/* Defer for less than two clock ticks. */
+	if (!tp-&gt;tso_deferred &amp;&amp; ((jiffies&lt;&lt;1)&gt;&gt;1) - (tp-&gt;tso_deferred&gt;&gt;1) &gt; 1)
+		goto send_now;
 
 	in_flight = tcp_packets_in_flight(tp);
 
@@ -1115,7 +1119,7 @@ static int tcp_tso_should_defer(struct sock *sk, struct tcp_sock *tp, struct sk_
 
 	/* If a full-sized TSO skb can be sent, do it. */
 	if (limit &gt;= 65536)
-		return 0;
+		goto send_now;
 
 	if (sysctl_tcp_tso_win_divisor) {
 		u32 chunk = min(tp-&gt;snd_wnd, tp-&gt;snd_cwnd * tp-&gt;mss_cache);
@@ -1125,7 +1129,7 @@ static int tcp_tso_should_defer(struct sock *sk, struct tcp_sock *tp, struct sk_
 		 */
 		chunk /= sysctl_tcp_tso_win_divisor;
 		if (limit &gt;= chunk)
-			return 0;
+			goto send_now;
 	} else {
 		/* Different approach, try not to defer past a single
 		 * ACK.  Receiver should ACK every other full sized
@@ -1133,11 +1137,17 @@ static int tcp_tso_should_defer(struct sock *sk, struct tcp_sock *tp, struct sk_
 		 * then send now.
 		 */
 		if (limit &gt; tcp_max_burst(tp) * tp-&gt;mss_cache)
-			return 0;
+			goto send_now;
 	}
 
 	/* Ok, it looks like it is advisable to defer.  */
+	tp-&gt;tso_deferred = 1 | (jiffies&lt;&lt;1);
+
 	return 1;
+
+send_now:
+	tp-&gt;tso_deferred = 0;
+	return 0;
 }
 
 /* Create a new MTU probe if we are ready.</pre><hr><pre>commit 8ea333eb5da3e3219f570220c56bca09f6f4d25a
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Thu Sep 28 14:47:38 2006 -0700

    [TCP]: Fix and simplify microsecond rtt sampling
    
    This changes the microsecond RTT sampling so that samples are taken in
    the same way that RTT samples are taken for the RTO calculator: on the
    last segment acknowledged, and only when the segment hasn't been
    retransmitted.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Acked-by: Stephen Hemminger &lt;shemminger@osdl.org&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index 85c966316668..3f884cea14ff 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -2239,13 +2239,12 @@ static int tcp_tso_acked(struct sock *sk, struct sk_buff *skb,
 	return acked;
 }
 
-static u32 tcp_usrtt(const struct sk_buff *skb)
+static u32 tcp_usrtt(struct timeval *tv)
 {
-	struct timeval tv, now;
+	struct timeval now;
 
 	do_gettimeofday(&amp;now);
-	skb_get_timestamp(skb, &amp;tv);
-	return (now.tv_sec - tv.tv_sec) * 1000000 + (now.tv_usec - tv.tv_usec);
+	return (now.tv_sec - tv-&gt;tv_sec) * 1000000 + (now.tv_usec - tv-&gt;tv_usec);
 }
 
 /* Remove acknowledged frames from the retransmission queue. */
@@ -2260,6 +2259,7 @@ static int tcp_clean_rtx_queue(struct sock *sk, __s32 *seq_rtt_p)
 	u32 pkts_acked = 0;
 	void (*rtt_sample)(struct sock *sk, u32 usrtt)
 		= icsk-&gt;icsk_ca_ops-&gt;rtt_sample;
+	struct timeval tv;
 
 	while ((skb = skb_peek(&amp;sk-&gt;sk_write_queue)) &amp;&amp;
 	       skb != sk-&gt;sk_send_head) {
@@ -2308,8 +2308,7 @@ static int tcp_clean_rtx_queue(struct sock *sk, __s32 *seq_rtt_p)
 				seq_rtt = -1;
 			} else if (seq_rtt &lt; 0) {
 				seq_rtt = now - scb-&gt;when;
-				if (rtt_sample)
-					(*rtt_sample)(sk, tcp_usrtt(skb));
+				skb_get_timestamp(skb, &amp;tv);
 			}
 			if (sacked &amp; TCPCB_SACKED_ACKED)
 				tp-&gt;sacked_out -= tcp_skb_pcount(skb);
@@ -2322,8 +2321,7 @@ static int tcp_clean_rtx_queue(struct sock *sk, __s32 *seq_rtt_p)
 			}
 		} else if (seq_rtt &lt; 0) {
 			seq_rtt = now - scb-&gt;when;
-			if (rtt_sample)
-				(*rtt_sample)(sk, tcp_usrtt(skb));
+			skb_get_timestamp(skb, &amp;tv);
 		}
 		tcp_dec_pcount_approx(&amp;tp-&gt;fackets_out, skb);
 		tcp_packets_out_dec(tp, skb);
@@ -2335,6 +2333,8 @@ static int tcp_clean_rtx_queue(struct sock *sk, __s32 *seq_rtt_p)
 	if (acked&amp;FLAG_ACKED) {
 		tcp_ack_update_rtt(sk, acked, seq_rtt);
 		tcp_ack_packets_out(sk, tp);
+		if (rtt_sample &amp;&amp; !(acked &amp; FLAG_RETRANS_DATA_ACKED))
+			(*rtt_sample)(sk, tcp_usrtt(&amp;tv));
 
 		if (icsk-&gt;icsk_ca_ops-&gt;pkts_acked)
 			icsk-&gt;icsk_ca_ops-&gt;pkts_acked(sk, pkts_acked);</pre><hr><pre>commit 5528e568a760442e0ec8fd2dea1f0791875a066b
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Fri May 5 17:41:44 2006 -0700

    [TCP]: Fix snd_cwnd adjustments in tcp_highspeed.c
    
    Xiaoliang (David) Wei wrote:
    &gt; Hi gurus,
    &gt;
    &gt;    I am reading the code of tcp_highspeed.c in the kernel and have a
    &gt; question on the hstcp_cong_avoid function, specifically the following
    &gt; AI part (line 136~143 in net/ipv4/tcp_highspeed.c ):
    &gt;
    &gt;                /* Do additive increase */
    &gt;                if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp) {
    &gt;                        tp-&gt;snd_cwnd_cnt += ca-&gt;ai;
    &gt;                        if (tp-&gt;snd_cwnd_cnt &gt;= tp-&gt;snd_cwnd) {
    &gt;                                tp-&gt;snd_cwnd++;
    &gt;                                tp-&gt;snd_cwnd_cnt -= tp-&gt;snd_cwnd;
    &gt;                        }
    &gt;                }
    &gt;
    &gt;    In this part, when (tp-&gt;snd_cwnd_cnt == tp-&gt;snd_cwnd),
    &gt; snd_cwnd_cnt will be -1... snd_cwnd_cnt is defined as u16, will this
    &gt; small chance of getting -1 becomes a problem?
    &gt; Shall we change it by reversing the order of the cwnd++ and cwnd_cnt -=
    &gt; cwnd?
    
    Absolutely correct.  Thanks.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/ipv4/tcp_highspeed.c b/net/ipv4/tcp_highspeed.c
index e0e9d1383c7c..b72fa55dfb84 100644
--- a/net/ipv4/tcp_highspeed.c
+++ b/net/ipv4/tcp_highspeed.c
@@ -137,8 +137,8 @@ static void hstcp_cong_avoid(struct sock *sk, u32 adk, u32 rtt,
 		if (tp-&gt;snd_cwnd &lt; tp-&gt;snd_cwnd_clamp) {
 			tp-&gt;snd_cwnd_cnt += ca-&gt;ai;
 			if (tp-&gt;snd_cwnd_cnt &gt;= tp-&gt;snd_cwnd) {
-				tp-&gt;snd_cwnd++;
 				tp-&gt;snd_cwnd_cnt -= tp-&gt;snd_cwnd;
+				tp-&gt;snd_cwnd++;
 			}
 		}
 	}</pre><hr><pre>commit 7b4f4b5ebceab67ce440a61081a69f0265e17c2a
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Sat Mar 25 01:34:07 2006 -0800

    [TCP]: Set default max buffers from memory pool size
    
    This patch sets the maximum TCP buffer sizes (available to automatic
    buffer tuning, not to setsockopt) based on the TCP memory pool size.
    The maximum sndbuf and rcvbuf each will be up to 4 MB, but no more
    than 1/128 of the memory pressure threshold.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 4b0272c92d66..591e96dffc28 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -276,8 +276,8 @@ atomic_t tcp_orphan_count = ATOMIC_INIT(0);
 EXPORT_SYMBOL_GPL(tcp_orphan_count);
 
 int sysctl_tcp_mem[3];
-int sysctl_tcp_wmem[3] = { 4 * 1024, 16 * 1024, 128 * 1024 };
-int sysctl_tcp_rmem[3] = { 4 * 1024, 87380, 87380 * 2 };
+int sysctl_tcp_wmem[3];
+int sysctl_tcp_rmem[3];
 
 EXPORT_SYMBOL(sysctl_tcp_mem);
 EXPORT_SYMBOL(sysctl_tcp_rmem);
@@ -2081,7 +2081,8 @@ __setup("thash_entries=", set_thash_entries);
 void __init tcp_init(void)
 {
 	struct sk_buff *skb = NULL;
-	int order, i;
+	unsigned long limit;
+	int order, i, max_share;
 
 	if (sizeof(struct tcp_skb_cb) &gt; sizeof(skb-&gt;cb))
 		__skb_cb_too_small_for_tcp(sizeof(struct tcp_skb_cb),
@@ -2155,12 +2156,16 @@ void __init tcp_init(void)
 	sysctl_tcp_mem[1] = 1024 &lt;&lt; order;
 	sysctl_tcp_mem[2] = 1536 &lt;&lt; order;
 
-	if (order &lt; 3) {
-		sysctl_tcp_wmem[2] = 64 * 1024;
-		sysctl_tcp_rmem[0] = PAGE_SIZE;
-		sysctl_tcp_rmem[1] = 43689;
-		sysctl_tcp_rmem[2] = 2 * 43689;
-	}
+	limit = ((unsigned long)sysctl_tcp_mem[1]) &lt;&lt; (PAGE_SHIFT - 7);
+	max_share = min(4UL*1024*1024, limit);
+
+	sysctl_tcp_wmem[0] = SK_STREAM_MEM_QUANTUM;
+	sysctl_tcp_wmem[1] = 16*1024;
+	sysctl_tcp_wmem[2] = max(64*1024, max_share);
+
+	sysctl_tcp_rmem[0] = SK_STREAM_MEM_QUANTUM;
+	sysctl_tcp_rmem[1] = 87380;
+	sysctl_tcp_rmem[2] = max(87380, max_share);
 
 	printk(KERN_INFO "TCP: Hash tables configured "
 	       "(established %d bind %d)\n",</pre><hr><pre>commit 0e7b13685f9a06949ea3070c97c0f0085a08cd37
Author: John Heffner &lt;jheffner@psc.edu&gt;
Date:   Mon Mar 20 21:32:58 2006 -0800

    [TCP] mtu probing: move tcp-specific data out of inet_connection_sock
    
    This moves some TCP-specific MTU probing state out of
    inet_connection_sock back to tcp_sock.
    
    Signed-off-by: John Heffner &lt;jheffner@psc.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/include/linux/tcp.h b/include/linux/tcp.h
index f2bb2396853f..542d39596bd8 100644
--- a/include/linux/tcp.h
+++ b/include/linux/tcp.h
@@ -343,6 +343,12 @@ struct tcp_sock {
 		__u32	seq;
 		__u32	time;
 	} rcvq_space;
+
+/* TCP-specific MTU probe information. */
+	struct {
+		__u32		  probe_seq_start;
+		__u32		  probe_seq_end;
+	} mtu_probe;
 };
 
 static inline struct tcp_sock *tcp_sk(const struct sock *sk)
diff --git a/include/net/inet_connection_sock.h b/include/net/inet_connection_sock.h
index b3abe33f4e5f..4e5a9ff99fc3 100644
--- a/include/net/inet_connection_sock.h
+++ b/include/net/inet_connection_sock.h
@@ -114,8 +114,6 @@ struct inet_connection_sock {
 
 		/* Information on the current probe. */
 		int		  probe_size;
-		__u32		  probe_seq_start;
-		__u32		  probe_seq_end;
 	} icsk_mtup;
 	u32			  icsk_ca_priv[16];
 #define ICSK_CA_PRIV_SIZE	(16 * sizeof(u32))
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index 0ac388e3d01d..195d83584558 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -2054,7 +2054,7 @@ tcp_fastretrans_alert(struct sock *sk, u32 prior_snd_una,
 		/* MTU probe failure: don't reduce cwnd */
 		if (icsk-&gt;icsk_ca_state &lt; TCP_CA_CWR &amp;&amp;
 		    icsk-&gt;icsk_mtup.probe_size &amp;&amp;
-		    tp-&gt;snd_una == icsk-&gt;icsk_mtup.probe_seq_start) {
+		    tp-&gt;snd_una == tp-&gt;mtu_probe.probe_seq_start) {
 			tcp_mtup_probe_failed(sk);
 			/* Restores the reduction we did in tcp_mtup_probe() */
 			tp-&gt;snd_cwnd++;
@@ -2284,7 +2284,7 @@ static int tcp_clean_rtx_queue(struct sock *sk, __s32 *seq_rtt_p)
 
 		/* MTU probing checks */
 		if (icsk-&gt;icsk_mtup.probe_size) {
-			if (!after(icsk-&gt;icsk_mtup.probe_seq_end, TCP_SKB_CB(skb)-&gt;end_seq)) {
+			if (!after(tp-&gt;mtu_probe.probe_seq_end, TCP_SKB_CB(skb)-&gt;end_seq)) {
 				tcp_mtup_probe_success(sk, skb);
 			}
 		}
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index 8197b5e12f1f..518e568b53f3 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -1238,8 +1238,8 @@ static int tcp_mtu_probe(struct sock *sk)
 		update_send_head(sk, tp, nskb);
 
 		icsk-&gt;icsk_mtup.probe_size = tcp_mss_to_mtu(sk, nskb-&gt;len);
-		icsk-&gt;icsk_mtup.probe_seq_start = TCP_SKB_CB(nskb)-&gt;seq;
-		icsk-&gt;icsk_mtup.probe_seq_end = TCP_SKB_CB(nskb)-&gt;end_seq;
+		tp-&gt;mtu_probe.probe_seq_start = TCP_SKB_CB(nskb)-&gt;seq;
+		tp-&gt;mtu_probe.probe_seq_end = TCP_SKB_CB(nskb)-&gt;end_seq;
 
 		return 1;
 	}</pre>
    <div class="pagination">
        <a href='5_46.html'>&lt;&lt;Prev</a><a href='5.html'>1</a><a href='5_2.html'>2</a><a href='5_3.html'>3</a><a href='5_4.html'>4</a><a href='5_5.html'>5</a><a href='5_6.html'>6</a><a href='5_7.html'>7</a><a href='5_8.html'>8</a><a href='5_9.html'>9</a><a href='5_10.html'>10</a><a href='5_11.html'>11</a><a href='5_12.html'>12</a><a href='5_13.html'>13</a><a href='5_14.html'>14</a><a href='5_15.html'>15</a><a href='5_16.html'>16</a><a href='5_17.html'>17</a><a href='5_18.html'>18</a><a href='5_19.html'>19</a><a href='5_20.html'>20</a><a href='5_21.html'>21</a><a href='5_22.html'>22</a><a href='5_23.html'>23</a><a href='5_24.html'>24</a><a href='5_25.html'>25</a><a href='5_26.html'>26</a><a href='5_27.html'>27</a><a href='5_28.html'>28</a><a href='5_29.html'>29</a><a href='5_30.html'>30</a><a href='5_31.html'>31</a><a href='5_32.html'>32</a><a href='5_33.html'>33</a><a href='5_34.html'>34</a><a href='5_35.html'>35</a><a href='5_36.html'>36</a><a href='5_37.html'>37</a><a href='5_38.html'>38</a><a href='5_39.html'>39</a><a href='5_40.html'>40</a><a href='5_41.html'>41</a><a href='5_42.html'>42</a><a href='5_43.html'>43</a><a href='5_44.html'>44</a><a href='5_45.html'>45</a><a href='5_46.html'>46</a><span>[47]</span><a href='5_48.html'>48</a><a href='5_48.html'>Next&gt;&gt;</a>
    <div>
</body>

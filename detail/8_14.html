<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by University of New South Wales</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by University of New South Wales</h1>
    <div class="pagination">
        <a href='8_13.html'>&lt;&lt;Prev</a><a href='8.html'>1</a><a href='8_2.html'>2</a><a href='8_3.html'>3</a><a href='8_4.html'>4</a><a href='8_5.html'>5</a><a href='8_6.html'>6</a><a href='8_7.html'>7</a><a href='8_8.html'>8</a><a href='8_9.html'>9</a><a href='8_10.html'>10</a><a href='8_11.html'>11</a><a href='8_12.html'>12</a><a href='8_13.html'>13</a><span>[14]</span><a href='8_15.html'>15</a><a href='8_16.html'>16</a><a href='8_17.html'>17</a><a href='8_18.html'>18</a><a href='8_15.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit a757e64cfa400391041ed7953f0290c34a820c93
Author: NeilBrown &lt;neilb@cse.unsw.edu.au&gt;
Date:   Sat Apr 16 15:26:42 2005 -0700

    [PATCH] md: remove a number of misleading calls to MD_BUG
    
    The conditions that cause these calls to MD_BUG are not kernel bugs, just
    oddities in what userspace is asking for.
    
    Also convert analyze_sbs to return void, and the value it returned was
    always 0.
    
    Signed-off-by: Neil Brown &lt;neilb@cse.unsw.edu.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@osdl.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/drivers/md/md.c b/drivers/md/md.c
index 44a164965546..97af857d8a88 100644
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@ -1387,7 +1387,7 @@ static mdk_rdev_t *md_import_device(dev_t newdev, int super_format, int super_mi
  */
 
 
-static int analyze_sbs(mddev_t * mddev)
+static void analyze_sbs(mddev_t * mddev)
 {
 	int i;
 	struct list_head *tmp;
@@ -1441,7 +1441,6 @@ static int analyze_sbs(mddev_t * mddev)
 		       " -- starting background reconstruction\n",
 		       mdname(mddev));
 
-	return 0;
 }
 
 int mdp_major = 0;
@@ -1508,10 +1507,9 @@ static int do_md_run(mddev_t * mddev)
 	struct gendisk *disk;
 	char b[BDEVNAME_SIZE];
 
-	if (list_empty(&amp;mddev-&gt;disks)) {
-		MD_BUG();
+	if (list_empty(&amp;mddev-&gt;disks))
+		/* cannot run an array with no devices.. */
 		return -EINVAL;
-	}
 
 	if (mddev-&gt;pers)
 		return -EBUSY;
@@ -1519,10 +1517,8 @@ static int do_md_run(mddev_t * mddev)
 	/*
 	 * Analyze all RAID superblock(s)
 	 */
-	if (!mddev-&gt;raid_disks &amp;&amp; analyze_sbs(mddev)) {
-		MD_BUG();
-		return -EINVAL;
-	}
+	if (!mddev-&gt;raid_disks)
+		analyze_sbs(mddev);
 
 	chunk_size = mddev-&gt;chunk_size;
 	pnum = level_to_pers(mddev-&gt;level);
@@ -1548,7 +1544,7 @@ static int do_md_run(mddev_t * mddev)
 		 * chunk-size has to be a power of 2 and multiples of PAGE_SIZE
 		 */
 		if ( (1 &lt;&lt; ffz(~chunk_size)) != chunk_size) {
-			MD_BUG();
+			printk(KERN_ERR "chunk_size of %d not valid\n", chunk_size);
 			return -EINVAL;
 		}
 		if (chunk_size &lt; PAGE_SIZE) {
@@ -1573,11 +1569,6 @@ static int do_md_run(mddev_t * mddev)
 		}
 	}
 
-	if (pnum &gt;= MAX_PERSONALITY) {
-		MD_BUG();
-		return -EINVAL;
-	}
-
 #ifdef CONFIG_KMOD
 	if (!pers[pnum])
 	{
@@ -1762,10 +1753,8 @@ static void autorun_array(mddev_t *mddev)
 	struct list_head *tmp;
 	int err;
 
-	if (list_empty(&amp;mddev-&gt;disks)) {
-		MD_BUG();
+	if (list_empty(&amp;mddev-&gt;disks))
 		return;
-	}
 
 	printk(KERN_INFO "md: running: ");
 
@@ -3128,7 +3117,6 @@ int register_md_personality(int pnum, mdk_personality_t *p)
 	spin_lock(&amp;pers_lock);
 	if (pers[pnum]) {
 		spin_unlock(&amp;pers_lock);
-		MD_BUG();
 		return -EBUSY;
 	}
 
@@ -3140,10 +3128,8 @@ int register_md_personality(int pnum, mdk_personality_t *p)
 
 int unregister_md_personality(int pnum)
 {
-	if (pnum &gt;= MAX_PERSONALITY) {
-		MD_BUG();
+	if (pnum &gt;= MAX_PERSONALITY)
 		return -EINVAL;
-	}
 
 	printk(KERN_INFO "md: %s personality unregistered\n", pers[pnum]-&gt;name);
 	spin_lock(&amp;pers_lock);</pre><hr><pre>commit d28446fe2d87ea344c14741c39962dcc7aee5c78
Author: NeilBrown &lt;neilb@cse.unsw.edu.au&gt;
Date:   Sat Apr 16 15:26:41 2005 -0700

    [PATCH] md: close a small race in md thread deregistration
    
    There is a tiny race when de-registering an MD thread, in that the thread
    could disappear before it is set a SIGKILL, causing send_sig to have
    problems.
    
    This is most easily closed by holding tasklist_lock between enabling the
    thread to exit (setting -&gt;run to NULL) and telling it to exit.
    
    (akpm: ick.  Needs to use kthread API and stop using signals)
    
    Signed-off-by: Neil Brown &lt;neilb@cse.unsw.edu.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@osdl.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/drivers/md/md.c b/drivers/md/md.c
index aa72c88a024f..44a164965546 100644
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@ -2840,16 +2840,6 @@ mdk_thread_t *md_register_thread(void (*run) (mddev_t *), mddev_t *mddev,
 	return thread;
 }
 
-static void md_interrupt_thread(mdk_thread_t *thread)
-{
-	if (!thread-&gt;tsk) {
-		MD_BUG();
-		return;
-	}
-	dprintk("interrupting MD-thread pid %d\n", thread-&gt;tsk-&gt;pid);
-	send_sig(SIGKILL, thread-&gt;tsk, 1);
-}
-
 void md_unregister_thread(mdk_thread_t *thread)
 {
 	struct completion event;
@@ -2857,9 +2847,15 @@ void md_unregister_thread(mdk_thread_t *thread)
 	init_completion(&amp;event);
 
 	thread-&gt;event = &amp;event;
+
+	/* As soon as -&gt;run is set to NULL, the task could disappear,
+	 * so we need to hold tasklist_lock until we have sent the signal
+	 */
+	dprintk("interrupting MD-thread pid %d\n", thread-&gt;tsk-&gt;pid);
+	read_lock(&amp;tasklist_lock);
 	thread-&gt;run = NULL;
-	thread-&gt;name = NULL;
-	md_interrupt_thread(thread);
+	send_sig(SIGKILL, thread-&gt;tsk, 1);
+	read_unlock(&amp;tasklist_lock);
 	wait_for_completion(&amp;event);
 	kfree(thread);
 }</pre><hr><pre>commit c907132d534c10b4f34a60383c8384403cb424a4
Author: NeilBrown &lt;neilb@cse.unsw.edu.au&gt;
Date:   Sat Apr 16 15:26:38 2005 -0700

    [PATCH] nfsd4: fix struct file leak
    
    We were failing to close on an error path, resulting in a leak of struct files
    which could take a v4 server down fairly quickly....  So call
    nfs4_close_delegation instead of just open-coding parts of it.
    
    Simplify the cleanup on delegation failure while we're at it.
    
    Signed-off-by: J. Bruce Fields &lt;bfields@citi.umich.edu&gt;
    Signed-off-by: Neil Brown &lt;neilb@cse.unsw.edu.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@osdl.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/fs/nfsd/nfs4state.c b/fs/nfsd/nfs4state.c
index 579f7fea7968..75e8b137580c 100644
--- a/fs/nfsd/nfs4state.c
+++ b/fs/nfsd/nfs4state.c
@@ -190,7 +190,8 @@ nfs4_close_delegation(struct nfs4_delegation *dp)
 	dp-&gt;dl_vfs_file = NULL;
 	/* The following nfsd_close may not actually close the file,
 	 * but we want to remove the lease in any case. */
-	setlease(filp, F_UNLCK, &amp;dp-&gt;dl_flock);
+	if (dp-&gt;dl_flock)
+		setlease(filp, F_UNLCK, &amp;dp-&gt;dl_flock);
 	nfsd_close(filp);
 	vfsclose++;
 }
@@ -1673,10 +1674,7 @@ nfs4_open_delegation(struct svc_fh *fh, struct nfsd4_open *open, struct nfs4_sta
 	if ((status = setlease(stp-&gt;st_vfs_file,
 		flag == NFS4_OPEN_DELEGATE_READ? F_RDLCK: F_WRLCK, &amp;flp))) {
 		dprintk("NFSD: setlease failed [%d], no delegation\n", status);
-		list_del(&amp;dp-&gt;dl_del_perfile);
-		list_del(&amp;dp-&gt;dl_del_perclnt);
-		nfs4_put_delegation(dp);
-		free_delegation++;
+		unhash_delegation(dp);
 		flag = NFS4_OPEN_DELEGATE_NONE;
 		goto out;
 	}</pre><hr><pre>commit f1ee4f22f21d74bc3ca63b95ca5b63d3a8620527
Author: NeilBrown &lt;neilb@cse.unsw.edu.au&gt;
Date:   Sat Apr 16 15:26:38 2005 -0700

    [PATCH] nfsd4: callback create rpc client returns
    
    rpc_create_clnt and friends return errors, not NULL, on failure.
    
    Signed-off-by: J. Bruce Fields &lt;bfields@citi.umich.edu&gt;
    Signed-off-by: Neil Brown &lt;neilb@cse.unsw.edu.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@osdl.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/fs/nfsd/nfs4callback.c b/fs/nfsd/nfs4callback.c
index c70de9c2af74..1a55dfcb74bc 100644
--- a/fs/nfsd/nfs4callback.c
+++ b/fs/nfsd/nfs4callback.c
@@ -405,7 +405,8 @@ nfsd4_probe_callback(struct nfs4_client *clp)
 	timeparms.to_exponential = 1;
 
 	/* Create RPC transport */
-	if (!(xprt = xprt_create_proto(IPPROTO_TCP, &amp;addr, &amp;timeparms))) {
+	xprt = xprt_create_proto(IPPROTO_TCP, &amp;addr, &amp;timeparms);
+	if (IS_ERR(xprt)) {
 		dprintk("NFSD: couldn't create callback transport!\n");
 		goto out_err;
 	}
@@ -426,7 +427,8 @@ nfsd4_probe_callback(struct nfs4_client *clp)
 	 * XXX AUTH_UNIX only - need AUTH_GSS....
 	 */
 	sprintf(hostname, "%u.%u.%u.%u", NIPQUAD(addr.sin_addr.s_addr));
-	if (!(clnt = rpc_create_client(xprt, hostname, program, 1, RPC_AUTH_UNIX))) {
+	clnt = rpc_create_client(xprt, hostname, program, 1, RPC_AUTH_UNIX);
+	if (IS_ERR(clnt)) {
 		dprintk("NFSD: couldn't create callback client\n");
 		goto out_xprt;
 	}</pre><hr><pre>commit 9e416052f1462801ca857c7536288bac0621615e
Author: NeilBrown &lt;neilb@cse.unsw.edu.au&gt;
Date:   Sat Apr 16 15:26:37 2005 -0700

    [PATCH] nfsd: clear signals before exiting the nfsd() thread
    
    Fixes the error "RPC: failed to contact portmap (errno -512)." when the server
    later tries to unregister from the portmapper.
    
    Signed-off-by: Trond Myklebust &lt;Trond.Myklebust@netapp.com&gt;
    Signed-off-by: J. Bruce Fields &lt;bfields@citi.umich.edu&gt;
    Signed-off-by: Neil Brown &lt;neilb@cse.unsw.edu.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@osdl.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/fs/nfsd/nfssvc.c b/fs/nfsd/nfssvc.c
index 39551657e656..02ded7cfbdcf 100644
--- a/fs/nfsd/nfssvc.c
+++ b/fs/nfsd/nfssvc.c
@@ -258,6 +258,8 @@ nfsd(struct svc_rqst *rqstp)
 				break;
 		err = signo;
 	}
+	/* Clear signals before calling lockd_down() and svc_exit_thread() */
+	flush_signals(current);
 
 	lock_kernel();
 </pre><hr><pre>commit baaa2c512dc1c47e3afeb9d558c5323c9240bd21
Author: Neil Brown &lt;neilb@cse.unsw.edu.au&gt;
Date:   Sat Apr 16 15:23:54 2005 -0700

    [PATCH] Avoid deadlock in sync_page_io by using GFP_NOIO
    
    ..as sync_page_io can be called on the write-out path.
    
    Signed-off-by: Neil Brown &lt;neilb@cse.unsw.edu.au&gt;
    Signed-off-by: Andrew Morton &lt;akpm@osdl.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@osdl.org&gt;

diff --git a/drivers/md/md.c b/drivers/md/md.c
index 04562add1920..aa72c88a024f 100644
--- a/drivers/md/md.c
+++ b/drivers/md/md.c
@@ -332,7 +332,7 @@ static int bi_complete(struct bio *bio, unsigned int bytes_done, int error)
 static int sync_page_io(struct block_device *bdev, sector_t sector, int size,
 		   struct page *page, int rw)
 {
-	struct bio *bio = bio_alloc(GFP_KERNEL, 1);
+	struct bio *bio = bio_alloc(GFP_NOIO, 1);
 	struct completion event;
 	int ret;
 </pre><hr><pre>commit e354597cce8d219d135d65e585dc4f30323486b9
Author: Peter Chubb &lt;peterc@gelato.unsw.edu.au&gt;
Date:   Mon Oct 13 11:49:04 2008 +1100

    PCI: fix 64-vbit prefetchable memory resource BARs
    
    Since patch 6ac665c63dcac8fcec534a1d224ecbb8b867ad59 my infiniband
    controller hasn't worked.  This is because it has 64-bit prefetchable
    memory, which was mistakenly being  taken to be 32-bit memory.  The
    resource flags in this case are PCI_BASE_ADDRESS_MEM_TYPE_64 |
    PCI_BASE_ADDRESS_MEM_PREFETCH.
    
    This patch checks only for the PCI_BASE_ADDRESS_MEM_TYPE_64 bit; thus
    whether the region is prefetchable or not is ignored.  This fixes my
    Infiniband.
    
    Reviewed-by: Matthew Wilcox &lt;matthew@wil.cx&gt;
    Signed-off-by: Peter Chubb &lt;peterc@gelato.unsw.edu.au&gt;
    Signed-off-by: Jesse Barnes &lt;jbarnes@virtuousgeek.org&gt;

diff --git a/drivers/pci/probe.c b/drivers/pci/probe.c
index f6754e87f046..49599ac49bda 100644
--- a/drivers/pci/probe.c
+++ b/drivers/pci/probe.c
@@ -217,7 +217,7 @@ static inline enum pci_bar_type decode_bar(struct resource *res, u32 bar)
 
 	res-&gt;flags = bar &amp; ~PCI_BASE_ADDRESS_MEM_MASK;
 
-	if (res-&gt;flags == PCI_BASE_ADDRESS_MEM_TYPE_64)
+	if (res-&gt;flags &amp; PCI_BASE_ADDRESS_MEM_TYPE_64)
 		return pci_bar_mem64;
 	return pci_bar_mem32;
 }</pre><hr><pre>commit 45333d5a31296d0af886d94f1d08f128231cab8e
Author: Aaron Carroll &lt;aaronc@gelato.unsw.edu.au&gt;
Date:   Tue Aug 26 15:52:36 2008 +0200

    cfq-iosched: fix queue depth detection
    
    CFQ's detection of queueing devices assumes a non-queuing device and detects
    if the queue depth reaches a certain threshold.  Under some workloads (e.g.
    synchronous reads), CFQ effectively forces a unit queue depth, thus defeating
    the detection logic.  This leads to poor performance on queuing hardware,
    since the idle window remains enabled.
    
    This patch inverts the sense of the logic: assume a queuing-capable device,
    and detect if the depth does not exceed the threshold.
    
    Signed-off-by: Aaron Carroll &lt;aaronc@gelato.unsw.edu.au&gt;
    Signed-off-by: Jens Axboe &lt;jens.axboe@oracle.com&gt;

diff --git a/block/cfq-iosched.c b/block/cfq-iosched.c
index 5f6fd287c185..494b6fdcb183 100644
--- a/block/cfq-iosched.c
+++ b/block/cfq-iosched.c
@@ -39,6 +39,7 @@ static int cfq_slice_idle = HZ / 125;
 #define CFQ_MIN_TT		(2)
 
 #define CFQ_SLICE_SCALE		(5)
+#define CFQ_HW_QUEUE_MIN	(5)
 
 #define RQ_CIC(rq)		\
 	((struct cfq_io_context *) (rq)-&gt;elevator_private)
@@ -86,7 +87,14 @@ struct cfq_data {
 
 	int rq_in_driver;
 	int sync_flight;
+
+	/*
+	 * queue-depth detection
+	 */
+	int rq_queued;
 	int hw_tag;
+	int hw_tag_samples;
+	int rq_in_driver_peak;
 
 	/*
 	 * idle window management
@@ -654,15 +662,6 @@ static void cfq_activate_request(struct request_queue *q, struct request *rq)
 	cfq_log_cfqq(cfqd, RQ_CFQQ(rq), "activate rq, drv=%d",
 						cfqd-&gt;rq_in_driver);
 
-	/*
-	 * If the depth is larger 1, it really could be queueing. But lets
-	 * make the mark a little higher - idling could still be good for
-	 * low queueing, and a low queueing number could also just indicate
-	 * a SCSI mid layer like behaviour where limit+1 is often seen.
-	 */
-	if (!cfqd-&gt;hw_tag &amp;&amp; cfqd-&gt;rq_in_driver &gt; 4)
-		cfqd-&gt;hw_tag = 1;
-
 	cfqd-&gt;last_position = rq-&gt;hard_sector + rq-&gt;hard_nr_sectors;
 }
 
@@ -686,6 +685,7 @@ static void cfq_remove_request(struct request *rq)
 	list_del_init(&amp;rq-&gt;queuelist);
 	cfq_del_rq_rb(rq);
 
+	cfqq-&gt;cfqd-&gt;rq_queued--;
 	if (rq_is_meta(rq)) {
 		WARN_ON(!cfqq-&gt;meta_pending);
 		cfqq-&gt;meta_pending--;
@@ -1833,6 +1833,7 @@ cfq_rq_enqueued(struct cfq_data *cfqd, struct cfq_queue *cfqq,
 {
 	struct cfq_io_context *cic = RQ_CIC(rq);
 
+	cfqd-&gt;rq_queued++;
 	if (rq_is_meta(rq))
 		cfqq-&gt;meta_pending++;
 
@@ -1880,6 +1881,31 @@ static void cfq_insert_request(struct request_queue *q, struct request *rq)
 	cfq_rq_enqueued(cfqd, cfqq, rq);
 }
 
+/*
+ * Update hw_tag based on peak queue depth over 50 samples under
+ * sufficient load.
+ */
+static void cfq_update_hw_tag(struct cfq_data *cfqd)
+{
+	if (cfqd-&gt;rq_in_driver &gt; cfqd-&gt;rq_in_driver_peak)
+		cfqd-&gt;rq_in_driver_peak = cfqd-&gt;rq_in_driver;
+
+	if (cfqd-&gt;rq_queued &lt;= CFQ_HW_QUEUE_MIN &amp;&amp;
+	    cfqd-&gt;rq_in_driver &lt;= CFQ_HW_QUEUE_MIN)
+		return;
+
+	if (cfqd-&gt;hw_tag_samples++ &lt; 50)
+		return;
+
+	if (cfqd-&gt;rq_in_driver_peak &gt;= CFQ_HW_QUEUE_MIN)
+		cfqd-&gt;hw_tag = 1;
+	else
+		cfqd-&gt;hw_tag = 0;
+
+	cfqd-&gt;hw_tag_samples = 0;
+	cfqd-&gt;rq_in_driver_peak = 0;
+}
+
 static void cfq_completed_request(struct request_queue *q, struct request *rq)
 {
 	struct cfq_queue *cfqq = RQ_CFQQ(rq);
@@ -1890,6 +1916,8 @@ static void cfq_completed_request(struct request_queue *q, struct request *rq)
 	now = jiffies;
 	cfq_log_cfqq(cfqd, cfqq, "complete");
 
+	cfq_update_hw_tag(cfqd);
+
 	WARN_ON(!cfqd-&gt;rq_in_driver);
 	WARN_ON(!cfqq-&gt;dispatched);
 	cfqd-&gt;rq_in_driver--;
@@ -2200,6 +2228,7 @@ static void *cfq_init_queue(struct request_queue *q)
 	cfqd-&gt;cfq_slice[1] = cfq_slice_sync;
 	cfqd-&gt;cfq_slice_async_rq = cfq_slice_async_rq;
 	cfqd-&gt;cfq_slice_idle = cfq_slice_idle;
+	cfqd-&gt;hw_tag = 1;
 
 	return cfqd;
 }</pre><hr><pre>commit 6a421c1dc94b12923294a359822346f12492de5e
Author: Aaron Carroll &lt;aaronc@gelato.unsw.edu.au&gt;
Date:   Thu Aug 14 18:17:15 2008 +1000

    block: update documentation for deadline fifo_batch tunable
    
    Update the description of fifo_batch to match the current implementation,
    and include a description of how to tune it.
    
    Signed-off-by: Aaron Carroll &lt;aaronc@gelato.unsw.edu.au&gt;
    Signed-off-by: Jens Axboe &lt;jens.axboe@oracle.com&gt;

diff --git a/Documentation/block/deadline-iosched.txt b/Documentation/block/deadline-iosched.txt
index c23cab13c3d1..72576769e0f4 100644
--- a/Documentation/block/deadline-iosched.txt
+++ b/Documentation/block/deadline-iosched.txt
@@ -30,12 +30,18 @@ write_expire	(in ms)
 Similar to read_expire mentioned above, but for writes.
 
 
-fifo_batch
+fifo_batch	(number of requests)
 ----------
 
-When a read request expires its deadline, we must move some requests from
-the sorted io scheduler list to the block device dispatch queue. fifo_batch
-controls how many requests we move.
+Requests are grouped into ``batches'' of a particular data direction (read or
+write) which are serviced in increasing sector order.  To limit extra seeking,
+deadline expiries are only checked between batches.  fifo_batch controls the
+maximum number of requests per batch.
+
+This parameter tunes the balance between per-request latency and aggregate
+throughput.  When low latency is the primary concern, smaller is better (where
+a value of 1 yields first-come first-served behaviour).  Increasing fifo_batch
+generally improves throughput, at the cost of latency variation.
 
 
 writes_starved	(number of dispatches)</pre><hr><pre>commit 4fb72f7646e86874eb2798256eaa6bf3fbe4edcf
Author: Aaron Carroll &lt;aaronc@gelato.unsw.edu.au&gt;
Date:   Thu Aug 14 18:17:14 2008 +1000

    deadline-iosched: non-functional fixes
    
    * convert goto to simpler while loop;
     * use rq_end_sector() instead of computing manually;
     * fix false comments;
     * remove spurious whitespace;
     * convert rq_rb_root macro to an inline function.
    
    Signed-off-by: Aaron Carroll &lt;aaronc@gelato.unsw.edu.au&gt;
    Signed-off-by: Jens Axboe &lt;jens.axboe@oracle.com&gt;

diff --git a/block/deadline-iosched.c b/block/deadline-iosched.c
index 07b80e4642f9..fd311179f44c 100644
--- a/block/deadline-iosched.c
+++ b/block/deadline-iosched.c
@@ -33,7 +33,7 @@ struct deadline_data {
 	 */
 	struct rb_root sort_list[2];	
 	struct list_head fifo_list[2];
-	
+
 	/*
 	 * next in sort order. read, write or both are NULL
 	 */
@@ -53,7 +53,11 @@ struct deadline_data {
 
 static void deadline_move_request(struct deadline_data *, struct request *);
 
-#define RQ_RB_ROOT(dd, rq)	(&amp;(dd)-&gt;sort_list[rq_data_dir((rq))])
+static inline struct rb_root *
+deadline_rb_root(struct deadline_data *dd, struct request *rq)
+{
+	return &amp;dd-&gt;sort_list[rq_data_dir(rq)];
+}
 
 /*
  * get the request after `rq' in sector-sorted order
@@ -72,15 +76,11 @@ deadline_latter_request(struct request *rq)
 static void
 deadline_add_rq_rb(struct deadline_data *dd, struct request *rq)
 {
-	struct rb_root *root = RQ_RB_ROOT(dd, rq);
+	struct rb_root *root = deadline_rb_root(dd, rq);
 	struct request *__alias;
 
-retry:
-	__alias = elv_rb_add(root, rq);
-	if (unlikely(__alias)) {
+	while (unlikely(__alias = elv_rb_add(root, rq)))
 		deadline_move_request(dd, __alias);
-		goto retry;
-	}
 }
 
 static inline void
@@ -91,7 +91,7 @@ deadline_del_rq_rb(struct deadline_data *dd, struct request *rq)
 	if (dd-&gt;next_rq[data_dir] == rq)
 		dd-&gt;next_rq[data_dir] = deadline_latter_request(rq);
 
-	elv_rb_del(RQ_RB_ROOT(dd, rq), rq);
+	elv_rb_del(deadline_rb_root(dd, rq), rq);
 }
 
 /*
@@ -106,7 +106,7 @@ deadline_add_request(struct request_queue *q, struct request *rq)
 	deadline_add_rq_rb(dd, rq);
 
 	/*
-	 * set expire time (only used for reads) and add to fifo list
+	 * set expire time and add to fifo list
 	 */
 	rq_set_fifo_time(rq, jiffies + dd-&gt;fifo_expire[data_dir]);
 	list_add_tail(&amp;rq-&gt;queuelist, &amp;dd-&gt;fifo_list[data_dir]);
@@ -162,7 +162,7 @@ static void deadline_merged_request(struct request_queue *q,
 	 * if the merge was a front merge, we need to reposition request
 	 */
 	if (type == ELEVATOR_FRONT_MERGE) {
-		elv_rb_del(RQ_RB_ROOT(dd, req), req);
+		elv_rb_del(deadline_rb_root(dd, req), req);
 		deadline_add_rq_rb(dd, req);
 	}
 }
@@ -212,7 +212,7 @@ deadline_move_request(struct deadline_data *dd, struct request *rq)
 	dd-&gt;next_rq[WRITE] = NULL;
 	dd-&gt;next_rq[data_dir] = deadline_latter_request(rq);
 
-	dd-&gt;last_sector = rq-&gt;sector + rq-&gt;nr_sectors;
+	dd-&gt;last_sector = rq_end_sector(rq);
 
 	/*
 	 * take it off the sort and fifo list, move
@@ -222,7 +222,7 @@ deadline_move_request(struct deadline_data *dd, struct request *rq)
 }
 
 /*
- * deadline_check_fifo returns 0 if there are no expired reads on the fifo,
+ * deadline_check_fifo returns 0 if there are no expired requests on the fifo,
  * 1 otherwise. Requires !list_empty(&amp;dd-&gt;fifo_list[data_dir])
  */
 static inline int deadline_check_fifo(struct deadline_data *dd, int ddir)</pre>
    <div class="pagination">
        <a href='8_13.html'>&lt;&lt;Prev</a><a href='8.html'>1</a><a href='8_2.html'>2</a><a href='8_3.html'>3</a><a href='8_4.html'>4</a><a href='8_5.html'>5</a><a href='8_6.html'>6</a><a href='8_7.html'>7</a><a href='8_8.html'>8</a><a href='8_9.html'>9</a><a href='8_10.html'>10</a><a href='8_11.html'>11</a><a href='8_12.html'>12</a><a href='8_13.html'>13</a><span>[14]</span><a href='8_15.html'>15</a><a href='8_16.html'>16</a><a href='8_17.html'>17</a><a href='8_18.html'>18</a><a href='8_15.html'>Next&gt;&gt;</a>
    <div>
</body>

<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Purdue University</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Purdue University</h1>
    <div class="pagination">
        <span>[1]</span><a href='40_2.html'>2</a><a href='40_2.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit bb5eb8f3b329789fbf22c85328dcf696a3e97ffb
Author: Congyu Liu &lt;liu3101@purdue.edu&gt;
Date:   Mon May 23 06:30:33 2022 +0000

    tracing: Disable kcov on trace_preemptirq.c
    
    Functions in trace_preemptirq.c could be invoked from early interrupt
    code that bypasses kcov trace function's in_task() check. Disable kcov
    on this file to reduce random code coverage.
    
    Link: https://lkml.kernel.org/r/20220523063033.1778974-1-liu3101@purdue.edu
    
    Acked-by: Dmitry Vyukov &lt;dvyukov@google.com&gt;
    Signed-off-by: Congyu Liu &lt;liu3101@purdue.edu&gt;
    Signed-off-by: Steven Rostedt (Google) &lt;rostedt@goodmis.org&gt;

diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index d77cd8032213..0d261774d6f3 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -31,6 +31,10 @@ ifdef CONFIG_GCOV_PROFILE_FTRACE
 GCOV_PROFILE := y
 endif
 
+# Functions in this file could be invoked from early interrupt
+# code and produce random code coverage.
+KCOV_INSTRUMENT_trace_preemptirq.o := n
+
 CFLAGS_bpf_trace.o := -I$(src)
 
 CFLAGS_trace_benchmark.o := -I$(src)</pre><hr><pre>commit 3159d79b56c15068aeb7e4630cd5f6dacd20fda4
Author: Congyu Liu &lt;liu3101@purdue.edu&gt;
Date:   Mon May 23 05:35:31 2022 +0000

    kcov: update pos before writing pc in trace function
    
    In __sanitizer_cov_trace_pc(), previously we write pc before updating pos.
    However, some early interrupt code could bypass check_kcov_mode() check
    and invoke __sanitizer_cov_trace_pc().  If such interrupt is raised
    between writing pc and updating pos, the pc could be overitten by the
    recursive __sanitizer_cov_trace_pc().
    
    As suggested by Dmitry, we cold update pos before writing pc to avoid such
    interleaving.
    
    Apply the same change to write_comp_data().
    
    Link: https://lkml.kernel.org/r/20220523053531.1572793-1-liu3101@purdue.edu
    Signed-off-by: Congyu Liu &lt;liu3101@purdue.edu&gt;
    Reviewed-by: Dmitry Vyukov &lt;dvyukov@google.com&gt;
    Cc: Andrey Konovalov &lt;andreyknvl@gmail.com&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;

diff --git a/kernel/kcov.c b/kernel/kcov.c
index b3732b210593..e19c84b02452 100644
--- a/kernel/kcov.c
+++ b/kernel/kcov.c
@@ -204,8 +204,16 @@ void notrace __sanitizer_cov_trace_pc(void)
 	/* The first 64-bit word is the number of subsequent PCs. */
 	pos = READ_ONCE(area[0]) + 1;
 	if (likely(pos &lt; t-&gt;kcov_size)) {
-		area[pos] = ip;
+		/* Previously we write pc before updating pos. However, some
+		 * early interrupt code could bypass check_kcov_mode() check
+		 * and invoke __sanitizer_cov_trace_pc(). If such interrupt is
+		 * raised between writing pc and updating pos, the pc could be
+		 * overitten by the recursive __sanitizer_cov_trace_pc().
+		 * Update pos before writing pc to avoid such interleaving.
+		 */
 		WRITE_ONCE(area[0], pos);
+		barrier();
+		area[pos] = ip;
 	}
 }
 EXPORT_SYMBOL(__sanitizer_cov_trace_pc);
@@ -236,11 +244,13 @@ static void notrace write_comp_data(u64 type, u64 arg1, u64 arg2, u64 ip)
 	start_index = 1 + count * KCOV_WORDS_PER_CMP;
 	end_pos = (start_index + KCOV_WORDS_PER_CMP) * sizeof(u64);
 	if (likely(end_pos &lt;= max_pos)) {
+		/* See comment in __sanitizer_cov_trace_pc(). */
+		WRITE_ONCE(area[0], count + 1);
+		barrier();
 		area[start_index] = type;
 		area[start_index + 1] = arg1;
 		area[start_index + 2] = arg2;
 		area[start_index + 3] = ip;
-		WRITE_ONCE(area[0], count + 1);
 	}
 }
 </pre><hr><pre>commit 47934e06b65637c88a762d9c98329ae6e3238888
Author: Congyu Liu &lt;liu3101@purdue.edu&gt;
Date:   Tue Jan 18 14:20:13 2022 -0500

    net: fix information leakage in /proc/net/ptype
    
    In one net namespace, after creating a packet socket without binding
    it to a device, users in other net namespaces can observe the new
    `packet_type` added by this packet socket by reading `/proc/net/ptype`
    file. This is minor information leakage as packet socket is
    namespace aware.
    
    Add a net pointer in `packet_type` to keep the net namespace of
    of corresponding packet socket. In `ptype_seq_show`, this net pointer
    must be checked when it is not NULL.
    
    Fixes: 2feb27dbe00c ("[NETNS]: Minor information leak via /proc/net/ptype file.")
    Signed-off-by: Congyu Liu &lt;liu3101@purdue.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 3213c7227b59..e490b84732d1 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -2548,6 +2548,7 @@ struct packet_type {
 					      struct net_device *);
 	bool			(*id_match)(struct packet_type *ptype,
 					    struct sock *sk);
+	struct net		*af_packet_net;
 	void			*af_packet_priv;
 	struct list_head	list;
 };
diff --git a/net/core/net-procfs.c b/net/core/net-procfs.c
index d8b9dbabd4a4..5b8016335aca 100644
--- a/net/core/net-procfs.c
+++ b/net/core/net-procfs.c
@@ -260,7 +260,8 @@ static int ptype_seq_show(struct seq_file *seq, void *v)
 
 	if (v == SEQ_START_TOKEN)
 		seq_puts(seq, "Type Device      Function\n");
-	else if (pt-&gt;dev == NULL || dev_net(pt-&gt;dev) == seq_file_net(seq)) {
+	else if ((!pt-&gt;af_packet_net || net_eq(pt-&gt;af_packet_net, seq_file_net(seq))) &amp;&amp;
+		 (!pt-&gt;dev || net_eq(dev_net(pt-&gt;dev), seq_file_net(seq)))) {
 		if (pt-&gt;type == htons(ETH_P_ALL))
 			seq_puts(seq, "ALL ");
 		else
diff --git a/net/packet/af_packet.c b/net/packet/af_packet.c
index 5bd409ab4cc2..85ea7ddb48db 100644
--- a/net/packet/af_packet.c
+++ b/net/packet/af_packet.c
@@ -1774,6 +1774,7 @@ static int fanout_add(struct sock *sk, struct fanout_args *args)
 		match-&gt;prot_hook.dev = po-&gt;prot_hook.dev;
 		match-&gt;prot_hook.func = packet_rcv_fanout;
 		match-&gt;prot_hook.af_packet_priv = match;
+		match-&gt;prot_hook.af_packet_net = read_pnet(&amp;match-&gt;net);
 		match-&gt;prot_hook.id_match = match_fanout_group;
 		match-&gt;max_num_members = args-&gt;max_num_members;
 		list_add(&amp;match-&gt;list, &amp;fanout_list);
@@ -3353,6 +3354,7 @@ static int packet_create(struct net *net, struct socket *sock, int protocol,
 		po-&gt;prot_hook.func = packet_rcv_spkt;
 
 	po-&gt;prot_hook.af_packet_priv = sk;
+	po-&gt;prot_hook.af_packet_net = sock_net(sk);
 
 	if (proto) {
 		po-&gt;prot_hook.type = proto;</pre><hr><pre>commit c42dd069be8dfc9b2239a5c89e73bbd08ab35de0
Author: Sishuai Gong &lt;sishuai@purdue.edu&gt;
Date:   Wed Aug 25 07:52:20 2021 +0200

    configfs: fix a race in configfs_lookup()
    
    When configfs_lookup() is executing list_for_each_entry(),
    it is possible that configfs_dir_lseek() is calling list_del().
    Some unfortunate interleavings of them can cause a kernel NULL
    pointer dereference error
    
    Thread 1                  Thread 2
    //configfs_dir_lseek()    //configfs_lookup()
    list_del(&amp;cursor-&gt;s_sibling);
                             list_for_each_entry(sd, ...)
    
    Fix this by grabbing configfs_dirent_lock in configfs_lookup()
    while iterating -&gt;s_children.
    
    Signed-off-by: Sishuai Gong &lt;sishuai@purdue.edu&gt;
    Signed-off-by: Christoph Hellwig &lt;hch@lst.de&gt;

diff --git a/fs/configfs/dir.c b/fs/configfs/dir.c
index fc20bd8a6337..1466b5d01cbb 100644
--- a/fs/configfs/dir.c
+++ b/fs/configfs/dir.c
@@ -439,13 +439,13 @@ static struct dentry * configfs_lookup(struct inode *dir,
 	if (!configfs_dirent_is_ready(parent_sd))
 		return ERR_PTR(-ENOENT);
 
+	spin_lock(&amp;configfs_dirent_lock);
 	list_for_each_entry(sd, &amp;parent_sd-&gt;s_children, s_sibling) {
 		if ((sd-&gt;s_type &amp; CONFIGFS_NOT_PINNED) &amp;&amp;
 		    !strcmp(configfs_get_name(sd), dentry-&gt;d_name.name)) {
 			struct configfs_attribute *attr = sd-&gt;s_element;
 			umode_t mode = (attr-&gt;ca_mode &amp; S_IALLUGO) | S_IFREG;
 
-			spin_lock(&amp;configfs_dirent_lock);
 			dentry-&gt;d_fsdata = configfs_get(sd);
 			sd-&gt;s_dentry = dentry;
 			spin_unlock(&amp;configfs_dirent_lock);
@@ -462,10 +462,11 @@ static struct dentry * configfs_lookup(struct inode *dir,
 				inode-&gt;i_size = PAGE_SIZE;
 				inode-&gt;i_fop = &amp;configfs_file_operations;
 			}
-			break;
+			goto done;
 		}
 	}
-
+	spin_unlock(&amp;configfs_dirent_lock);
+done:
 	d_add(dentry, inode);
 	return NULL;
 }</pre><hr><pre>commit 69e16d01d1de4f1249869de342915f608feb55d5
Author: Gong, Sishuai &lt;sishuai@purdue.edu&gt;
Date:   Tue Apr 27 15:04:24 2021 +0000

    net: fix a concurrency bug in l2tp_tunnel_register()
    
    l2tp_tunnel_register() registers a tunnel without fully
    initializing its attribute. This can allow another kernel thread
    running l2tp_xmit_core() to access the uninitialized data and
    then cause a kernel NULL pointer dereference error, as shown below.
    
    Thread 1    Thread 2
    //l2tp_tunnel_register()
    list_add_rcu(&amp;tunnel-&gt;list, &amp;pn-&gt;l2tp_tunnel_list);
               //pppol2tp_connect()
               tunnel = l2tp_tunnel_get(sock_net(sk), info.tunnel_id);
               // Fetch the new tunnel
               ...
               //l2tp_xmit_core()
               struct sock *sk = tunnel-&gt;sock;
               ...
               bh_lock_sock(sk);
               //Null pointer error happens
    tunnel-&gt;sock = sk;
    
    Fix this bug by initializing tunnel-&gt;sock before adding the
    tunnel into l2tp_tunnel_list.
    
    Reviewed-by: Cong Wang &lt;cong.wang@bytedance.com&gt;
    Signed-off-by: Sishuai Gong &lt;sishuai@purdue.edu&gt;
    Reported-by: Sishuai Gong &lt;sishuai@purdue.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/l2tp/l2tp_core.c b/net/l2tp/l2tp_core.c
index 2ee20743cb41..53486b162f01 100644
--- a/net/l2tp/l2tp_core.c
+++ b/net/l2tp/l2tp_core.c
@@ -1478,11 +1478,15 @@ int l2tp_tunnel_register(struct l2tp_tunnel *tunnel, struct net *net,
 	tunnel-&gt;l2tp_net = net;
 	pn = l2tp_pernet(net);
 
+	sk = sock-&gt;sk;
+	sock_hold(sk);
+	tunnel-&gt;sock = sk;
+
 	spin_lock_bh(&amp;pn-&gt;l2tp_tunnel_list_lock);
 	list_for_each_entry(tunnel_walk, &amp;pn-&gt;l2tp_tunnel_list, list) {
 		if (tunnel_walk-&gt;tunnel_id == tunnel-&gt;tunnel_id) {
 			spin_unlock_bh(&amp;pn-&gt;l2tp_tunnel_list_lock);
-
+			sock_put(sk);
 			ret = -EEXIST;
 			goto err_sock;
 		}
@@ -1490,10 +1494,6 @@ int l2tp_tunnel_register(struct l2tp_tunnel *tunnel, struct net *net,
 	list_add_rcu(&amp;tunnel-&gt;list, &amp;pn-&gt;l2tp_tunnel_list);
 	spin_unlock_bh(&amp;pn-&gt;l2tp_tunnel_list_lock);
 
-	sk = sock-&gt;sk;
-	sock_hold(sk);
-	tunnel-&gt;sock = sk;
-
 	if (tunnel-&gt;encap == L2TP_ENCAPTYPE_UDP) {
 		struct udp_tunnel_sock_cfg udp_cfg = {
 			.sk_user_data = tunnel,</pre><hr><pre>commit bdd84a6f4baf3e1d02955545cf83c6a7e6a470db
Author: Mike Shuey &lt;shuey@purdue.edu&gt;
Date:   Tue May 19 10:14:39 2015 -0400

    staging: lustre: lnet: remove LNET_MUTEX_LOCK macro
    
    LNET_MUTEX_LOCK and LNET_MUTEX_UNLOCK are verbose wrappers to mutex_lock and
    mutex_unlock.  Get rid of these.
    
    Signed-off-by: Mike Shuey &lt;shuey@purdue.edu&gt;
    Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;

diff --git a/drivers/staging/lustre/include/linux/lnet/lib-lnet.h b/drivers/staging/lustre/include/linux/lnet/lib-lnet.h
index 0038d29a37fe..d84aa9a4c56d 100644
--- a/drivers/staging/lustre/include/linux/lnet/lib-lnet.h
+++ b/drivers/staging/lustre/include/linux/lnet/lib-lnet.h
@@ -172,8 +172,6 @@ lnet_net_lock_current(void)
 #define lnet_eq_wait_unlock()	spin_unlock(&amp;the_lnet.ln_eq_wait_lock)
 #define lnet_ni_lock(ni)	spin_lock(&amp;(ni)-&gt;ni_lock)
 #define lnet_ni_unlock(ni)	spin_unlock(&amp;(ni)-&gt;ni_lock)
-#define LNET_MUTEX_LOCK(m)	mutex_lock(m)
-#define LNET_MUTEX_UNLOCK(m)	mutex_unlock(m)
 
 #define MAX_PORTALS     64
 
diff --git a/drivers/staging/lustre/lnet/lnet/api-ni.c b/drivers/staging/lustre/lnet/lnet/api-ni.c
index 1adc481c7f48..2230eb0193a1 100644
--- a/drivers/staging/lustre/lnet/lnet/api-ni.c
+++ b/drivers/staging/lustre/lnet/lnet/api-ni.c
@@ -282,7 +282,7 @@ lnet_find_lnd_by_type(int type)
 void
 lnet_register_lnd(lnd_t *lnd)
 {
-	LNET_MUTEX_LOCK(&amp;the_lnet.ln_lnd_mutex);
+	mutex_lock(&amp;the_lnet.ln_lnd_mutex);
 
 	LASSERT(the_lnet.ln_init);
 	LASSERT(libcfs_isknown_lnd(lnd-&gt;lnd_type));
@@ -293,14 +293,14 @@ lnet_register_lnd(lnd_t *lnd)
 
 	CDEBUG(D_NET, "%s LND registered\n", libcfs_lnd2str(lnd-&gt;lnd_type));
 
-	LNET_MUTEX_UNLOCK(&amp;the_lnet.ln_lnd_mutex);
+	mutex_unlock(&amp;the_lnet.ln_lnd_mutex);
 }
 EXPORT_SYMBOL(lnet_register_lnd);
 
 void
 lnet_unregister_lnd(lnd_t *lnd)
 {
-	LNET_MUTEX_LOCK(&amp;the_lnet.ln_lnd_mutex);
+	mutex_lock(&amp;the_lnet.ln_lnd_mutex);
 
 	LASSERT(the_lnet.ln_init);
 	LASSERT(lnet_find_lnd_by_type(lnd-&gt;lnd_type) == lnd);
@@ -309,7 +309,7 @@ lnet_unregister_lnd(lnd_t *lnd)
 	list_del(&amp;lnd-&gt;lnd_list);
 	CDEBUG(D_NET, "%s LND unregistered\n", libcfs_lnd2str(lnd-&gt;lnd_type));
 
-	LNET_MUTEX_UNLOCK(&amp;the_lnet.ln_lnd_mutex);
+	mutex_unlock(&amp;the_lnet.ln_lnd_mutex);
 }
 EXPORT_SYMBOL(lnet_unregister_lnd);
 
@@ -1055,18 +1055,18 @@ lnet_startup_lndnis(void)
 			goto failed;
 		}
 
-		LNET_MUTEX_LOCK(&amp;the_lnet.ln_lnd_mutex);
+		mutex_lock(&amp;the_lnet.ln_lnd_mutex);
 		lnd = lnet_find_lnd_by_type(lnd_type);
 
 		if (lnd == NULL) {
-			LNET_MUTEX_UNLOCK(&amp;the_lnet.ln_lnd_mutex);
+			mutex_unlock(&amp;the_lnet.ln_lnd_mutex);
 			rc = request_module("%s",
 						libcfs_lnd2modname(lnd_type));
-			LNET_MUTEX_LOCK(&amp;the_lnet.ln_lnd_mutex);
+			mutex_lock(&amp;the_lnet.ln_lnd_mutex);
 
 			lnd = lnet_find_lnd_by_type(lnd_type);
 			if (lnd == NULL) {
-				LNET_MUTEX_UNLOCK(&amp;the_lnet.ln_lnd_mutex);
+				mutex_unlock(&amp;the_lnet.ln_lnd_mutex);
 				CERROR("Can't load LND %s, module %s, rc=%d\n",
 				       libcfs_lnd2str(lnd_type),
 				       libcfs_lnd2modname(lnd_type), rc);
@@ -1082,7 +1082,7 @@ lnet_startup_lndnis(void)
 
 		rc = (lnd-&gt;lnd_startup)(ni);
 
-		LNET_MUTEX_UNLOCK(&amp;the_lnet.ln_lnd_mutex);
+		mutex_unlock(&amp;the_lnet.ln_lnd_mutex);
 
 		if (rc != 0) {
 			LCONSOLE_ERROR_MSG(0x105, "Error %d starting up LNI %s\n",
@@ -1272,7 +1272,7 @@ LNetNIInit(lnet_pid_t requested_pid)
 	int im_a_router = 0;
 	int rc;
 
-	LNET_MUTEX_LOCK(&amp;the_lnet.ln_api_mutex);
+	mutex_lock(&amp;the_lnet.ln_api_mutex);
 
 	LASSERT(the_lnet.ln_init);
 	CDEBUG(D_OTHER, "refs %d\n", the_lnet.ln_refcount);
@@ -1343,7 +1343,7 @@ LNetNIInit(lnet_pid_t requested_pid)
  failed0:
 	LASSERT(rc &lt; 0);
  out:
-	LNET_MUTEX_UNLOCK(&amp;the_lnet.ln_api_mutex);
+	mutex_unlock(&amp;the_lnet.ln_api_mutex);
 	return rc;
 }
 EXPORT_SYMBOL(LNetNIInit);
@@ -1360,7 +1360,7 @@ EXPORT_SYMBOL(LNetNIInit);
 int
 LNetNIFini(void)
 {
-	LNET_MUTEX_LOCK(&amp;the_lnet.ln_api_mutex);
+	mutex_lock(&amp;the_lnet.ln_api_mutex);
 
 	LASSERT(the_lnet.ln_init);
 	LASSERT(the_lnet.ln_refcount &gt; 0);
@@ -1383,7 +1383,7 @@ LNetNIFini(void)
 		lnet_unprepare();
 	}
 
-	LNET_MUTEX_UNLOCK(&amp;the_lnet.ln_api_mutex);
+	mutex_unlock(&amp;the_lnet.ln_api_mutex);
 	return 0;
 }
 EXPORT_SYMBOL(LNetNIFini);
diff --git a/drivers/staging/lustre/lnet/lnet/module.c b/drivers/staging/lustre/lnet/lnet/module.c
index f73d64468396..6881b9cf32ce 100644
--- a/drivers/staging/lustre/lnet/lnet/module.c
+++ b/drivers/staging/lustre/lnet/lnet/module.c
@@ -49,7 +49,7 @@ lnet_configure(void *arg)
 	/* 'arg' only there so I can be passed to cfs_create_thread() */
 	int rc = 0;
 
-	LNET_MUTEX_LOCK(&amp;lnet_config_mutex);
+	mutex_lock(&amp;lnet_config_mutex);
 
 	if (!the_lnet.ln_niinit_self) {
 		rc = LNetNIInit(LUSTRE_SRV_LNET_PID);
@@ -59,7 +59,7 @@ lnet_configure(void *arg)
 		}
 	}
 
-	LNET_MUTEX_UNLOCK(&amp;lnet_config_mutex);
+	mutex_unlock(&amp;lnet_config_mutex);
 	return rc;
 }
 
@@ -68,18 +68,18 @@ lnet_unconfigure(void)
 {
 	int refcount;
 
-	LNET_MUTEX_LOCK(&amp;lnet_config_mutex);
+	mutex_lock(&amp;lnet_config_mutex);
 
 	if (the_lnet.ln_niinit_self) {
 		the_lnet.ln_niinit_self = 0;
 		LNetNIFini();
 	}
 
-	LNET_MUTEX_LOCK(&amp;the_lnet.ln_api_mutex);
+	mutex_lock(&amp;the_lnet.ln_api_mutex);
 	refcount = the_lnet.ln_refcount;
-	LNET_MUTEX_UNLOCK(&amp;the_lnet.ln_api_mutex);
+	mutex_unlock(&amp;the_lnet.ln_api_mutex);
 
-	LNET_MUTEX_UNLOCK(&amp;lnet_config_mutex);
+	mutex_unlock(&amp;lnet_config_mutex);
 	return (refcount == 0) ? 0 : -EBUSY;
 }
 </pre><hr><pre>commit 74d680116219380ed946bb3ad24237f2424fdb66
Author: Mike Shuey &lt;shuey@purdue.edu&gt;
Date:   Tue May 19 10:14:38 2015 -0400

    staging: lustre: lnet: selftest: code cleanup - variable spacing, indentation
    
    Unify spacing in variable declarations, and align indentation in headers.
    General whitespace cleanups.
    
    Signed-off-by: Mike Shuey &lt;shuey@purdue.edu&gt;
    Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;

diff --git a/drivers/staging/lustre/lnet/selftest/brw_test.c b/drivers/staging/lustre/lnet/selftest/brw_test.c
index 658f4584fff8..de11f1bc8be7 100644
--- a/drivers/staging/lustre/lnet/selftest/brw_test.c
+++ b/drivers/staging/lustre/lnet/selftest/brw_test.c
@@ -91,7 +91,7 @@ brw_client_init(sfw_test_instance_t *tsi)
 		len   = npg * PAGE_CACHE_SIZE;
 
 	} else {
-		test_bulk_req_v1_t  *breq = &amp;tsi-&gt;tsi_u.bulk_v1;
+		test_bulk_req_v1_t *breq = &amp;tsi-&gt;tsi_u.bulk_v1;
 
 		/* I should never get this step if it's unknown feature
 		 * because make_session will reject unknown feature */
@@ -223,7 +223,7 @@ brw_check_page(struct page *pg, int pattern, __u64 magic)
 static void
 brw_fill_bulk(srpc_bulk_t *bk, int pattern, __u64 magic)
 {
-	int	 i;
+	int i;
 	struct page *pg;
 
 	for (i = 0; i &lt; bk-&gt;bk_niov; i++) {
@@ -235,7 +235,7 @@ brw_fill_bulk(srpc_bulk_t *bk, int pattern, __u64 magic)
 static int
 brw_check_bulk(srpc_bulk_t *bk, int pattern, __u64 magic)
 {
-	int	 i;
+	int i;
 	struct page *pg;
 
 	for (i = 0; i &lt; bk-&gt;bk_niov; i++) {
@@ -254,16 +254,16 @@ static int
 brw_client_prep_rpc(sfw_test_unit_t *tsu,
 		     lnet_process_id_t dest, srpc_client_rpc_t **rpcpp)
 {
-	srpc_bulk_t	 *bulk = tsu-&gt;tsu_private;
+	srpc_bulk_t *bulk = tsu-&gt;tsu_private;
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
-	sfw_session_t	    *sn = tsi-&gt;tsi_batch-&gt;bat_session;
-	srpc_client_rpc_t   *rpc;
-	srpc_brw_reqst_t    *req;
-	int		     flags;
-	int		     npg;
-	int		     len;
-	int		     opc;
-	int		     rc;
+	sfw_session_t *sn = tsi-&gt;tsi_batch-&gt;bat_session;
+	srpc_client_rpc_t *rpc;
+	srpc_brw_reqst_t *req;
+	int flags;
+	int npg;
+	int len;
+	int opc;
+	int rc;
 
 	LASSERT(sn != NULL);
 	LASSERT(bulk != NULL);
@@ -277,7 +277,7 @@ brw_client_prep_rpc(sfw_test_unit_t *tsu,
 		len   = npg * PAGE_CACHE_SIZE;
 
 	} else {
-		test_bulk_req_v1_t  *breq = &amp;tsi-&gt;tsi_u.bulk_v1;
+		test_bulk_req_v1_t *breq = &amp;tsi-&gt;tsi_u.bulk_v1;
 
 		/* I should never get this step if it's unknown feature
 		 * because make_session will reject unknown feature */
@@ -311,12 +311,12 @@ brw_client_prep_rpc(sfw_test_unit_t *tsu,
 static void
 brw_client_done_rpc(sfw_test_unit_t *tsu, srpc_client_rpc_t *rpc)
 {
-	__u64		magic = BRW_MAGIC;
+	__u64 magic = BRW_MAGIC;
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
-	sfw_session_t       *sn = tsi-&gt;tsi_batch-&gt;bat_session;
-	srpc_msg_t	  *msg = &amp;rpc-&gt;crpc_replymsg;
-	srpc_brw_reply_t    *reply = &amp;msg-&gt;msg_body.brw_reply;
-	srpc_brw_reqst_t    *reqst = &amp;rpc-&gt;crpc_reqstmsg.msg_body.brw_reqst;
+	sfw_session_t *sn = tsi-&gt;tsi_batch-&gt;bat_session;
+	srpc_msg_t *msg = &amp;rpc-&gt;crpc_replymsg;
+	srpc_brw_reply_t *reply = &amp;msg-&gt;msg_body.brw_reply;
+	srpc_brw_reqst_t *reqst = &amp;rpc-&gt;crpc_reqstmsg.msg_body.brw_reqst;
 
 	LASSERT(sn != NULL);
 
@@ -380,10 +380,10 @@ brw_server_rpc_done(srpc_server_rpc_t *rpc)
 static int
 brw_bulk_ready(srpc_server_rpc_t *rpc, int status)
 {
-	__u64	     magic = BRW_MAGIC;
+	__u64 magic = BRW_MAGIC;
 	srpc_brw_reply_t *reply = &amp;rpc-&gt;srpc_replymsg.msg_body.brw_reply;
 	srpc_brw_reqst_t *reqst;
-	srpc_msg_t       *reqstmsg;
+	srpc_msg_t *reqstmsg;
 
 	LASSERT(rpc-&gt;srpc_bulk != NULL);
 	LASSERT(rpc-&gt;srpc_reqstbuf != NULL);
@@ -416,13 +416,13 @@ brw_bulk_ready(srpc_server_rpc_t *rpc, int status)
 static int
 brw_server_handle(struct srpc_server_rpc *rpc)
 {
-	struct srpc_service	*sv = rpc-&gt;srpc_scd-&gt;scd_svc;
-	srpc_msg_t       *replymsg = &amp;rpc-&gt;srpc_replymsg;
-	srpc_msg_t       *reqstmsg = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
+	struct srpc_service *sv = rpc-&gt;srpc_scd-&gt;scd_svc;
+	srpc_msg_t *replymsg = &amp;rpc-&gt;srpc_replymsg;
+	srpc_msg_t *reqstmsg = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
 	srpc_brw_reply_t *reply = &amp;replymsg-&gt;msg_body.brw_reply;
 	srpc_brw_reqst_t *reqst = &amp;reqstmsg-&gt;msg_body.brw_reqst;
-	int		  npg;
-	int	       rc;
+	int npg;
+	int rc;
 
 	LASSERT(sv-&gt;sv_id == SRPC_SERVICE_BRW);
 
@@ -490,17 +490,17 @@ brw_server_handle(struct srpc_server_rpc *rpc)
 sfw_test_client_ops_t brw_test_client;
 void brw_init_test_client(void)
 {
-	brw_test_client.tso_init       = brw_client_init;
-	brw_test_client.tso_fini       = brw_client_fini;
-	brw_test_client.tso_prep_rpc   = brw_client_prep_rpc;
-	brw_test_client.tso_done_rpc   = brw_client_done_rpc;
+	brw_test_client.tso_init     = brw_client_init;
+	brw_test_client.tso_fini     = brw_client_fini;
+	brw_test_client.tso_prep_rpc = brw_client_prep_rpc;
+	brw_test_client.tso_done_rpc = brw_client_done_rpc;
 };
 
 srpc_service_t brw_test_service;
 void brw_init_test_service(void)
 {
 
-	brw_test_service.sv_id	 = SRPC_SERVICE_BRW;
+	brw_test_service.sv_id         = SRPC_SERVICE_BRW;
 	brw_test_service.sv_name       = "brw_test";
 	brw_test_service.sv_handler    = brw_server_handle;
 	brw_test_service.sv_bulk_ready = brw_bulk_ready;
diff --git a/drivers/staging/lustre/lnet/selftest/conctl.c b/drivers/staging/lustre/lnet/selftest/conctl.c
index 045fe295ad54..1a7870e91f23 100644
--- a/drivers/staging/lustre/lnet/selftest/conctl.c
+++ b/drivers/staging/lustre/lnet/selftest/conctl.c
@@ -48,11 +48,11 @@
 static int
 lst_session_new_ioctl(lstio_session_new_args_t *args)
 {
-	char      *name;
-	int	rc;
+	char *name;
+	int rc;
 
 	if (args-&gt;lstio_ses_idp   == NULL || /* address for output sid */
-	    args-&gt;lstio_ses_key   == 0 || /* no key is specified */
+	    args-&gt;lstio_ses_key   == 0 ||    /* no key is specified */
 	    args-&gt;lstio_ses_namep == NULL || /* session name */
 	    args-&gt;lstio_ses_nmlen &lt;= 0 ||
 	    args-&gt;lstio_ses_nmlen &gt; LST_NAME_SIZE)
@@ -96,12 +96,12 @@ lst_session_info_ioctl(lstio_session_info_args_t *args)
 {
 	/* no checking of key */
 
-	if (args-&gt;lstio_ses_idp   == NULL || /* address for output sid */
-	    args-&gt;lstio_ses_keyp  == NULL || /* address for output key */
+	if (args-&gt;lstio_ses_idp    == NULL || /* address for output sid */
+	    args-&gt;lstio_ses_keyp   == NULL || /* address for output key */
 	    args-&gt;lstio_ses_featp  == NULL || /* address for output features */
 	    args-&gt;lstio_ses_ndinfo == NULL || /* address for output ndinfo */
-	    args-&gt;lstio_ses_namep == NULL || /* address for output name */
-	    args-&gt;lstio_ses_nmlen &lt;= 0 ||
+	    args-&gt;lstio_ses_namep  == NULL || /* address for output name */
+	    args-&gt;lstio_ses_nmlen  &lt;= 0 ||
 	    args-&gt;lstio_ses_nmlen &gt; LST_NAME_SIZE)
 		return -EINVAL;
 
@@ -197,8 +197,8 @@ lst_debug_ioctl(lstio_debug_args_t *args)
 static int
 lst_group_add_ioctl(lstio_group_add_args_t *args)
 {
-	char	   *name;
-	int	     rc;
+	char *name;
+	int rc;
 
 	if (args-&gt;lstio_grp_key != console_session.ses_key)
 		return -EACCES;
@@ -324,8 +324,8 @@ static int
 lst_nodes_add_ioctl(lstio_group_nodes_args_t *args)
 {
 	unsigned feats;
-	int     rc;
-	char   *name;
+	int rc;
+	char *name;
 
 	if (args-&gt;lstio_grp_key != console_session.ses_key)
 		return -EACCES;
@@ -385,10 +385,10 @@ lst_group_list_ioctl(lstio_group_list_args_t *args)
 static int
 lst_group_info_ioctl(lstio_group_info_args_t *args)
 {
-	char	   *name;
-	int	     ndent;
-	int	     index;
-	int	     rc;
+	char *name;
+	int ndent;
+	int index;
+	int rc;
 
 	if (args-&gt;lstio_grp_key != console_session.ses_key)
 		return -EACCES;
@@ -449,8 +449,8 @@ lst_group_info_ioctl(lstio_group_info_args_t *args)
 static int
 lst_batch_add_ioctl(lstio_batch_add_args_t *args)
 {
-	int	     rc;
-	char	   *name;
+	int rc;
+	char *name;
 
 	if (args-&gt;lstio_bat_key != console_session.ses_key)
 		return -EACCES;
@@ -483,8 +483,8 @@ lst_batch_add_ioctl(lstio_batch_add_args_t *args)
 static int
 lst_batch_run_ioctl(lstio_batch_run_args_t *args)
 {
-	int	     rc;
-	char	   *name;
+	int rc;
+	char *name;
 
 	if (args-&gt;lstio_bat_key != console_session.ses_key)
 		return -EACCES;
@@ -518,8 +518,8 @@ lst_batch_run_ioctl(lstio_batch_run_args_t *args)
 static int
 lst_batch_stop_ioctl(lstio_batch_stop_args_t *args)
 {
-	int	     rc;
-	char	   *name;
+	int rc;
+	char *name;
 
 	if (args-&gt;lstio_bat_key != console_session.ses_key)
 		return -EACCES;
@@ -613,10 +613,10 @@ lst_batch_list_ioctl(lstio_batch_list_args_t *args)
 static int
 lst_batch_info_ioctl(lstio_batch_info_args_t *args)
 {
-	char	   *name;
-	int	     rc;
-	int	     index;
-	int	     ndent;
+	char *name;
+	int rc;
+	int index;
+	int ndent;
 
 	if (args-&gt;lstio_bat_key != console_session.ses_key)
 		return -EACCES;
@@ -678,8 +678,8 @@ lst_batch_info_ioctl(lstio_batch_info_args_t *args)
 static int
 lst_stat_query_ioctl(lstio_stat_args_t *args)
 {
-	int	     rc;
-	char	   *name;
+	int rc;
+	char *name;
 
 	/* TODO: not finished */
 	if (args-&gt;lstio_sta_key != console_session.ses_key)
diff --git a/drivers/staging/lustre/lnet/selftest/conrpc.c b/drivers/staging/lustre/lnet/selftest/conrpc.c
index 77f02b76128e..a1a4e08f7391 100644
--- a/drivers/staging/lustre/lnet/selftest/conrpc.c
+++ b/drivers/staging/lustre/lnet/selftest/conrpc.c
@@ -117,8 +117,8 @@ static int
 lstcon_rpc_prep(lstcon_node_t *nd, int service, unsigned feats,
 		int bulk_npg, int bulk_len, lstcon_rpc_t **crpcpp)
 {
-	lstcon_rpc_t  *crpc = NULL;
-	int	    rc;
+	lstcon_rpc_t *crpc = NULL;
+	int rc;
 
 	spin_lock(&amp;console_session.ses_rpc_lock);
 
@@ -151,7 +151,7 @@ void
 lstcon_rpc_put(lstcon_rpc_t *crpc)
 {
 	srpc_bulk_t *bulk = &amp;crpc-&gt;crp_rpc-&gt;crpc_bulk;
-	int	  i;
+	int i;
 
 	LASSERT(list_empty(&amp;crpc-&gt;crp_link));
 
@@ -336,8 +336,8 @@ lstcon_rpc_trans_check(lstcon_rpc_trans_t *trans)
 int
 lstcon_rpc_trans_postwait(lstcon_rpc_trans_t *trans, int timeout)
 {
-	lstcon_rpc_t  *crpc;
-	int	    rc;
+	lstcon_rpc_t *crpc;
+	int rc;
 
 	if (list_empty(&amp;trans-&gt;tas_rpcs_list))
 		return 0;
@@ -386,8 +386,8 @@ lstcon_rpc_trans_postwait(lstcon_rpc_trans_t *trans, int timeout)
 static int
 lstcon_rpc_get_reply(lstcon_rpc_t *crpc, srpc_msg_t **msgpp)
 {
-	lstcon_node_t	*nd  = crpc-&gt;crp_node;
-	srpc_client_rpc_t    *rpc = crpc-&gt;crp_rpc;
+	lstcon_node_t *nd  = crpc-&gt;crp_node;
+	srpc_client_rpc_t *rpc = crpc-&gt;crp_rpc;
 	srpc_generic_reply_t *rep;
 
 	LASSERT(nd != NULL &amp;&amp; rpc != NULL);
@@ -423,9 +423,9 @@ lstcon_rpc_get_reply(lstcon_rpc_t *crpc, srpc_msg_t **msgpp)
 void
 lstcon_rpc_trans_stat(lstcon_rpc_trans_t *trans, lstcon_trans_stat_t *stat)
 {
-	lstcon_rpc_t      *crpc;
-	srpc_msg_t	*rep;
-	int		error;
+	lstcon_rpc_t  *crpc;
+	srpc_msg_t *rep;
+	int error;
 
 	LASSERT(stat != NULL);
 
@@ -470,16 +470,16 @@ lstcon_rpc_trans_interpreter(lstcon_rpc_trans_t *trans,
 			     struct list_head *head_up,
 			     lstcon_rpc_readent_func_t readent)
 {
-	struct list_head	    tmp;
-	struct list_head	   *next;
-	lstcon_rpc_ent_t     *ent;
+	struct list_head tmp;
+	struct list_head *next;
+	lstcon_rpc_ent_t *ent;
 	srpc_generic_reply_t *rep;
-	lstcon_rpc_t	 *crpc;
-	srpc_msg_t	   *msg;
-	lstcon_node_t	*nd;
-	long	dur;
-	struct timeval	tv;
-	int		   error;
+	lstcon_rpc_t *crpc;
+	srpc_msg_t *msg;
+	lstcon_node_t *nd;
+	long dur;
+	struct timeval tv;
+	int error;
 
 	LASSERT(head_up != NULL);
 
@@ -544,9 +544,9 @@ void
 lstcon_rpc_trans_destroy(lstcon_rpc_trans_t *trans)
 {
 	srpc_client_rpc_t *rpc;
-	lstcon_rpc_t      *crpc;
-	lstcon_rpc_t      *tmp;
-	int		count = 0;
+	lstcon_rpc_t *crpc;
+	lstcon_rpc_t *tmp;
+	int count = 0;
 
 	list_for_each_entry_safe(crpc, tmp, &amp;trans-&gt;tas_rpcs_list,
 				 crp_link) {
@@ -601,7 +601,7 @@ lstcon_sesrpc_prep(lstcon_node_t *nd, int transop,
 {
 	srpc_mksn_reqst_t *msrq;
 	srpc_rmsn_reqst_t *rsrq;
-	int		rc;
+	int rc;
 
 	switch (transop) {
 	case LST_TRANS_SESNEW:
@@ -638,7 +638,7 @@ int
 lstcon_dbgrpc_prep(lstcon_node_t *nd, unsigned feats, lstcon_rpc_t **crpc)
 {
 	srpc_debug_reqst_t *drq;
-	int		    rc;
+	int rc;
 
 	rc = lstcon_rpc_prep(nd, SRPC_SERVICE_DEBUG, feats, 0, 0, crpc);
 	if (rc != 0)
@@ -707,7 +707,7 @@ static lnet_process_id_packed_t *
 lstcon_next_id(int idx, int nkiov, lnet_kiov_t *kiov)
 {
 	lnet_process_id_packed_t *pid;
-	int		       i;
+	int i;
 
 	i = idx / SFW_ID_PER_PAGE;
 
@@ -723,11 +723,11 @@ lstcon_dstnodes_prep(lstcon_group_t *grp, int idx,
 		     int dist, int span, int nkiov, lnet_kiov_t *kiov)
 {
 	lnet_process_id_packed_t *pid;
-	lstcon_ndlink_t	  *ndl;
-	lstcon_node_t	    *nd;
-	int		       start;
-	int		       end;
-	int		       i = 0;
+	lstcon_ndlink_t *ndl;
+	lstcon_node_t *nd;
+	int start;
+	int end;
+	int i = 0;
 
 	LASSERT(dist &gt;= 1);
 	LASSERT(span &gt;= 1);
@@ -777,8 +777,8 @@ lstcon_pingrpc_prep(lst_test_ping_param_t *param, srpc_test_reqst_t *req)
 {
 	test_ping_req_t *prq = &amp;req-&gt;tsr_u.ping;
 
-	prq-&gt;png_size   = param-&gt;png_size;
-	prq-&gt;png_flags  = param-&gt;png_flags;
+	prq-&gt;png_size  = param-&gt;png_size;
+	prq-&gt;png_flags = param-&gt;png_flags;
 	/* TODO dest */
 	return 0;
 }
@@ -788,9 +788,10 @@ lstcon_bulkrpc_v0_prep(lst_test_bulk_param_t *param, srpc_test_reqst_t *req)
 {
 	test_bulk_req_t *brq = &amp;req-&gt;tsr_u.bulk_v0;
 
-	brq-&gt;blk_opc    = param-&gt;blk_opc;
-	brq-&gt;blk_npg    = (param-&gt;blk_size + PAGE_CACHE_SIZE - 1) / PAGE_CACHE_SIZE;
-	brq-&gt;blk_flags  = param-&gt;blk_flags;
+	brq-&gt;blk_opc   = param-&gt;blk_opc;
+	brq-&gt;blk_npg   = (param-&gt;blk_size + PAGE_CACHE_SIZE - 1) /
+			  PAGE_CACHE_SIZE;
+	brq-&gt;blk_flags = param-&gt;blk_flags;
 
 	return 0;
 }
@@ -816,7 +817,7 @@ lstcon_testrpc_prep(lstcon_node_t *nd, int transop, unsigned feats,
 	lstcon_group_t    *dgrp = test-&gt;tes_dst_grp;
 	srpc_test_reqst_t *trq;
 	srpc_bulk_t       *bulk;
-	int		i;
+	int                i;
 	int		   npg = 0;
 	int		   nob = 0;
 	int		   rc  = 0;
@@ -835,8 +836,10 @@ lstcon_testrpc_prep(lstcon_node_t *nd, int transop, unsigned feats,
 	trq  = &amp;(*crpc)-&gt;crp_rpc-&gt;crpc_reqstmsg.msg_body.tes_reqst;
 
 	if (transop == LST_TRANS_TSBSRVADD) {
-		int ndist = (sgrp-&gt;grp_nnode + test-&gt;tes_dist - 1) / test-&gt;tes_dist;
-		int nspan = (dgrp-&gt;grp_nnode + test-&gt;tes_span - 1) / test-&gt;tes_span;
+		int ndist = (sgrp-&gt;grp_nnode + test-&gt;tes_dist - 1) /
+			    test-&gt;tes_dist;
+		int nspan = (dgrp-&gt;grp_nnode + test-&gt;tes_span - 1) /
+			    test-&gt;tes_span;
 		int nmax = (ndist + nspan - 1) / nspan;
 
 		trq-&gt;tsr_ndest = 0;
@@ -851,7 +854,8 @@ lstcon_testrpc_prep(lstcon_node_t *nd, int transop, unsigned feats,
 			LASSERT(nob &gt; 0);
 
 			len = (feats &amp; LST_FEAT_BULK_LEN) == 0 ?
-			      PAGE_CACHE_SIZE : min_t(int, nob, PAGE_CACHE_SIZE);
+			      PAGE_CACHE_SIZE :
+			      min_t(int, nob, PAGE_CACHE_SIZE);
 			nob -= len;
 
 			bulk-&gt;bk_iovs[i].kiov_offset = 0;
@@ -883,8 +887,8 @@ lstcon_testrpc_prep(lstcon_node_t *nd, int transop, unsigned feats,
 		trq-&gt;tsr_loop  = test-&gt;tes_loop;
 	}
 
-	trq-&gt;tsr_sid	= console_session.ses_id;
-	trq-&gt;tsr_bid	= test-&gt;tes_hdr.tsb_id;
+	trq-&gt;tsr_sid        = console_session.ses_id;
+	trq-&gt;tsr_bid        = test-&gt;tes_hdr.tsb_id;
 	trq-&gt;tsr_concur     = test-&gt;tes_concur;
 	trq-&gt;tsr_is_client  = (transop == LST_TRANS_TSBCLIADD) ? 1 : 0;
 	trq-&gt;tsr_stop_onerr = !!test-&gt;tes_stop_onerr;
@@ -966,7 +970,7 @@ lstcon_rpc_stat_reply(lstcon_rpc_trans_t *trans, srpc_msg_t *msg,
 	srpc_batch_reply_t *bat_rep;
 	srpc_test_reply_t  *test_rep;
 	srpc_stat_reply_t  *stat_rep;
-	int		 rc = 0;
+	int                rc = 0;
 
 	switch (trans-&gt;tas_opc) {
 	case LST_TRANS_SESNEW:
@@ -1084,11 +1088,11 @@ lstcon_rpc_trans_ndlist(struct list_head *ndlist,
 			lstcon_rpc_trans_t **transpp)
 {
 	lstcon_rpc_trans_t *trans;
-	lstcon_ndlink_t    *ndl;
-	lstcon_node_t      *nd;
-	lstcon_rpc_t       *rpc;
-	unsigned	    feats;
-	int		 rc;
+	lstcon_ndlink_t *ndl;
+	lstcon_node_t *nd;
+	lstcon_rpc_t *rpc;
+	unsigned feats;
+	int rc;
 
 	/* Creating session RPG for list of nodes */
 
@@ -1165,16 +1169,16 @@ lstcon_rpc_trans_ndlist(struct list_head *ndlist,
 static void
 lstcon_rpc_pinger(void *arg)
 {
-	stt_timer_t	*ptimer = (stt_timer_t *)arg;
+	stt_timer_t *ptimer = (stt_timer_t *)arg;
 	lstcon_rpc_trans_t *trans;
-	lstcon_rpc_t       *crpc;
-	srpc_msg_t	 *rep;
+	lstcon_rpc_t *crpc;
+	srpc_msg_t *rep;
 	srpc_debug_reqst_t *drq;
-	lstcon_ndlink_t    *ndl;
-	lstcon_node_t      *nd;
-	time_t	      intv;
-	int		 count = 0;
-	int		 rc;
+	lstcon_ndlink_t *ndl;
+	lstcon_node_t *nd;
+	time_t intv;
+	int count = 0;
+	int rc;
 
 	/* RPC pinger is a special case of transaction,
 	 * it's called by timer at 8 seconds interval.
@@ -1283,8 +1287,8 @@ lstcon_rpc_pinger(void *arg)
 int
 lstcon_rpc_pinger_start(void)
 {
-	stt_timer_t    *ptimer;
-	int	     rc;
+	stt_timer_t *ptimer;
+	int rc;
 
 	LASSERT(list_empty(&amp;console_session.ses_rpc_freelist));
 	LASSERT(atomic_read(&amp;console_session.ses_rpc_counter) == 0);
@@ -1324,9 +1328,9 @@ void
 lstcon_rpc_cleanup_wait(void)
 {
 	lstcon_rpc_trans_t *trans;
-	lstcon_rpc_t       *crpc;
-	struct list_head	 *pacer;
-	struct list_head	  zlist;
+	lstcon_rpc_t *crpc;
+	struct list_head *pacer;
+	struct list_head zlist;
 
 	/* Called with hold of global mutex */
 
diff --git a/drivers/staging/lustre/lnet/selftest/conrpc.h b/drivers/staging/lustre/lnet/selftest/conrpc.h
index 2353889c6eac..7d33cf9e9d99 100644
--- a/drivers/staging/lustre/lnet/selftest/conrpc.h
+++ b/drivers/staging/lustre/lnet/selftest/conrpc.h
@@ -64,31 +64,29 @@ struct lstcon_test;
 struct lstcon_node;
 
 typedef struct lstcon_rpc {
-	struct list_head	       crp_link;       /* chain on rpc transaction */
+	struct list_head        crp_link;       /* chain on rpc transaction */
 	srpc_client_rpc_t       *crp_rpc;	/* client rpc */
-	struct lstcon_node      *crp_node;       /* destination node */
+	struct lstcon_node      *crp_node;      /* destination node */
 	struct lstcon_rpc_trans *crp_trans;     /* conrpc transaction */
 
-	unsigned int		 crp_posted:1;   /* rpc is posted */
-	unsigned int		 crp_finished:1; /* rpc is finished */
-	unsigned int		 crp_unpacked:1; /* reply is unpacked */
+	unsigned int            crp_posted:1;   /* rpc is posted */
+	unsigned int            crp_finished:1; /* rpc is finished */
+	unsigned int            crp_unpacked:1; /* reply is unpacked */
 	/** RPC is embedded in other structure and can't free it */
-	unsigned int		 crp_embedded:1;
-	int		      crp_status;     /* console rpc errors */
-	unsigned long	       crp_stamp;      /* replied time stamp */
+	unsigned int            crp_embedded:1;
+	int                     crp_status;     /* console rpc errors */
+	unsigned long           crp_stamp;      /* replied time stamp */
 } lstcon_rpc_t;
 
 typedef struct lstcon_rpc_trans {
-	struct list_head	    tas_olink;     /* link chain on owner list */
-	struct list_head	    tas_link;      /* link chain on global list */
-	int		   tas_opc;       /* operation code of transaction */
-	/* features mask is uptodate */
-	unsigned	      tas_feats_updated;
-	/* test features mask */
-	unsigned	      tas_features;
-	wait_queue_head_t	   tas_waitq;     /* wait queue head */
-	atomic_t	  tas_remaining; /* # of un-scheduled rpcs */
-	struct list_head	    tas_rpcs_list; /* queued requests */
+	struct list_head  tas_olink;         /* link chain on owner list */
+	struct list_head  tas_link;          /* link chain on global list */
+	int               tas_opc;           /* operation code of transaction */
+	unsigned          tas_feats_updated; /* features mask is uptodate */
+	unsigned          tas_features;      /* test features mask */
+	wait_queue_head_t tas_waitq;         /* wait queue head */
+	atomic_t          tas_remaining;     /* # of un-scheduled rpcs */
+	struct list_head  tas_rpcs_list;     /* queued requests */
 } lstcon_rpc_trans_t;
 
 #define LST_TRANS_PRIVATE       0x1000
diff --git a/drivers/staging/lustre/lnet/selftest/console.c b/drivers/staging/lustre/lnet/selftest/console.c
index 2b5f53c7a730..f47c8f27f975 100644
--- a/drivers/staging/lustre/lnet/selftest/console.c
+++ b/drivers/staging/lustre/lnet/selftest/console.c
@@ -59,7 +59,7 @@ do {							\
 	(p)-&gt;nle_nnode++;				\
 } while (0)
 
-lstcon_session_t	console_session;
+lstcon_session_t console_session;
 
 static void
 lstcon_node_get(lstcon_node_t *nd)
@@ -73,7 +73,7 @@ static int
 lstcon_node_find(lnet_process_id_t id, lstcon_node_t **ndpp, int create)
 {
 	lstcon_ndlink_t *ndl;
-	unsigned int     idx = LNET_NIDADDR(id.nid) % LST_GLOBAL_HASHSIZE;
+	unsigned int idx = LNET_NIDADDR(id.nid) % LST_GLOBAL_HASHSIZE;
 
 	LASSERT(id.nid != LNET_NID_ANY);
 
@@ -117,7 +117,7 @@ lstcon_node_find(lnet_process_id_t id, lstcon_node_t **ndpp, int create)
 static void
 lstcon_node_put(lstcon_node_t *nd)
 {
-	lstcon_ndlink_t  *ndl;
+	lstcon_ndlink_t *ndl;
 
 	LASSERT(nd-&gt;nd_ref &gt; 0);
 
@@ -140,10 +140,10 @@ static int
 lstcon_ndlink_find(struct list_head *hash,
 		   lnet_process_id_t id, lstcon_ndlink_t **ndlpp, int create)
 {
-	unsigned int     idx = LNET_NIDADDR(id.nid) % LST_NODE_HASHSIZE;
+	unsigned int idx = LNET_NIDADDR(id.nid) % LST_NODE_HASHSIZE;
 	lstcon_ndlink_t *ndl;
-	lstcon_node_t   *nd;
-	int	      rc;
+	lstcon_node_t *nd;
+	int rc;
 
 	if (id.nid == LNET_NID_ANY)
 		return -EINVAL;
@@ -197,7 +197,7 @@ static int
 lstcon_group_alloc(char *name, lstcon_group_t **grpp)
 {
 	lstcon_group_t *grp;
-	int	     i;
+	int i;
 
 	LIBCFS_ALLOC(grp, offsetof(lstcon_group_t,
 				   grp_ndl_hash[LST_NODE_HASHSIZE]));
@@ -243,7 +243,7 @@ lstcon_group_drain(lstcon_group_t *grp, int keep)
 static void
 lstcon_group_decref(lstcon_group_t *grp)
 {
-	int     i;
+	int i;
 
 	if (--grp-&gt;grp_ref &gt; 0)
 		return;
@@ -264,7 +264,7 @@ lstcon_group_decref(lstcon_group_t *grp)
 static int
 lstcon_group_find(const char *name, lstcon_group_t **grpp)
 {
-	lstcon_group_t   *grp;
+	lstcon_group_t *grp;
 
 	list_for_each_entry(grp, &amp;console_session.ses_grp_list, grp_link) {
 		if (strncmp(grp-&gt;grp_name, name, LST_NAME_SIZE) != 0)
@@ -288,7 +288,7 @@ static int
 lstcon_group_ndlink_find(lstcon_group_t *grp, lnet_process_id_t id,
 			 lstcon_ndlink_t **ndlpp, int create)
 {
-	int     rc;
+	int rc;
 
 	rc = lstcon_ndlink_find(&amp;grp-&gt;grp_ndl_hash[0], id, ndlpp, create);
 	if (rc != 0)
@@ -404,12 +404,12 @@ lstcon_group_nodes_add(lstcon_group_t *grp,
 		       int count, lnet_process_id_t *ids_up,
 		       unsigned *featp, struct list_head *result_up)
 {
-	lstcon_rpc_trans_t      *trans;
-	lstcon_ndlink_t	 *ndl;
-	lstcon_group_t	  *tmp;
-	lnet_process_id_t	id;
-	int		      i;
-	int		      rc;
+	lstcon_rpc_trans_t *trans;
+	lstcon_ndlink_t *ndl;
+	lstcon_group_t *tmp;
+	lnet_process_id_t id;
+	int i;
+	int rc;
 
 	rc = lstcon_group_alloc(NULL, &amp;tmp);
 	if (rc != 0) {
@@ -471,12 +471,12 @@ lstcon_group_nodes_remove(lstcon_group_t *grp,
 			  int count, lnet_process_id_t *ids_up,
 			  struct list_head *result_up)
 {
-	lstcon_rpc_trans_t     *trans;
-	lstcon_ndlink_t	*ndl;
-	lstcon_group_t	 *tmp;
-	lnet_process_id_t       id;
-	int		     rc;
-	int		     i;
+	lstcon_rpc_trans_t *trans;
+	lstcon_ndlink_t *ndl;
+	lstcon_group_t *tmp;
+	lnet_process_id_t id;
+	int rc;
+	int i;
 
 	/* End session and remove node from the group */
 
@@ -525,7 +525,7 @@ int
 lstcon_group_add(char *name)
 {
 	lstcon_group_t *grp;
-	int	     rc;
+	int rc;
 
 	rc = (lstcon_group_find(name, &amp;grp) == 0)? -EEXIST: 0;
 	if (rc != 0) {
@@ -549,8 +549,8 @@ int
 lstcon_nodes_add(char *name, int count, lnet_process_id_t *ids_up,
 		 unsigned *featp, struct list_head *result_up)
 {
-	lstcon_group_t	 *grp;
-	int		     rc;
+	lstcon_group_t *grp;
+	int rc;
 
 	LASSERT(count &gt; 0);
 	LASSERT(ids_up != NULL);
@@ -580,8 +580,8 @@ int
 lstcon_group_del(char *name)
 {
 	lstcon_rpc_trans_t *trans;
-	lstcon_group_t     *grp;
-	int		 rc;
+	lstcon_group_t *grp;
+	int rc;
 
 	rc = lstcon_group_find(name, &amp;grp);
 	if (rc != 0) {
@@ -621,7 +621,7 @@ int
 lstcon_group_clean(char *name, int args)
 {
 	lstcon_group_t *grp = NULL;
-	int	     rc;
+	int rc;
 
 	rc = lstcon_group_find(name, &amp;grp);
 	if (rc != 0) {
@@ -654,7 +654,7 @@ lstcon_nodes_remove(char *name, int count,
 		    lnet_process_id_t *ids_up, struct list_head *result_up)
 {
 	lstcon_group_t *grp = NULL;
-	int	     rc;
+	int rc;
 
 	rc = lstcon_group_find(name, &amp;grp);
 	if (rc != 0) {
@@ -682,9 +682,9 @@ lstcon_nodes_remove(char *name, int count,
 int
 lstcon_group_refresh(char *name, struct list_head *result_up)
 {
-	lstcon_rpc_trans_t      *trans;
-	lstcon_group_t	  *grp;
-	int		      rc;
+	lstcon_rpc_trans_t *trans;
+	lstcon_group_t *grp;
+	int rc;
 
 	rc = lstcon_group_find(name, &amp;grp);
 	if (rc != 0) {
@@ -743,10 +743,10 @@ static int
 lstcon_nodes_getent(struct list_head *head, int *index_p,
 		    int *count_p, lstcon_node_ent_t *dents_up)
 {
-	lstcon_ndlink_t  *ndl;
-	lstcon_node_t    *nd;
-	int	       count = 0;
-	int	       index = 0;
+	lstcon_ndlink_t *ndl;
+	lstcon_node_t *nd;
+	int count = 0;
+	int index = 0;
 
 	LASSERT(index_p != NULL &amp;&amp; count_p != NULL);
 	LASSERT(dents_up != NULL);
@@ -784,9 +784,9 @@ lstcon_group_info(char *name, lstcon_ndlist_ent_t *gents_p,
 		  int *index_p, int *count_p, lstcon_node_ent_t *dents_up)
 {
 	lstcon_ndlist_ent_t *gentp;
-	lstcon_group_t      *grp;
-	lstcon_ndlink_t     *ndl;
-	int		  rc;
+	lstcon_group_t *grp;
+	lstcon_ndlink_t *ndl;
+	int rc;
 
 	rc = lstcon_group_find(name, &amp;grp);
 	if (rc != 0) {
@@ -828,7 +828,7 @@ lstcon_group_info(char *name, lstcon_ndlist_ent_t *gents_p,
 static int
 lstcon_batch_find(const char *name, lstcon_batch_t **batpp)
 {
-	lstcon_batch_t   *bat;
+	lstcon_batch_t *bat;
 
 	list_for_each_entry(bat, &amp;console_session.ses_bat_list, bat_link) {
 		if (strncmp(bat-&gt;bat_name, name, LST_NAME_SIZE) == 0) {
@@ -843,9 +843,9 @@ lstcon_batch_find(const char *name, lstcon_batch_t **batpp)
 int
 lstcon_batch_add(char *name)
 {
-	lstcon_batch_t   *bat;
-	int	       i;
-	int	       rc;
+	lstcon_batch_t *bat;
+	int i;
+	int rc;
 
 	rc = (lstcon_batch_find(name, &amp;bat) == 0)? -EEXIST: 0;
 	if (rc != 0) {
@@ -903,7 +903,7 @@ lstcon_batch_add(char *name)
 int
 lstcon_batch_list(int index, int len, char *name_up)
 {
-	lstcon_batch_t    *bat;
+	lstcon_batch_t *bat;
 
 	LASSERT(name_up != NULL);
 	LASSERT(index &gt;= 0);
@@ -924,12 +924,12 @@ lstcon_batch_info(char *name, lstcon_test_batch_ent_t *ent_up, int server,
 		  lstcon_node_ent_t *dents_up)
 {
 	lstcon_test_batch_ent_t *entp;
-	struct list_head	      *clilst;
-	struct list_head	      *srvlst;
-	lstcon_test_t	   *test = NULL;
-	lstcon_batch_t	  *bat;
-	lstcon_ndlink_t	 *ndl;
-	int		      rc;
+	struct list_head *clilst;
+	struct list_head *srvlst;
+	lstcon_test_t *test = NULL;
+	lstcon_batch_t *bat;
+	lstcon_ndlink_t *ndl;
+	int rc;
 
 	rc = lstcon_batch_find(name, &amp;bat);
 	if (rc != 0) {
@@ -1018,7 +1018,7 @@ lstcon_batch_op(lstcon_batch_t *bat, int transop,
 		struct list_head *result_up)
 {
 	lstcon_rpc_trans_t *trans;
-	int		 rc;
+	int rc;
 
 	rc = lstcon_rpc_trans_ndlist(&amp;bat-&gt;bat_cli_list,
 				     &amp;bat-&gt;bat_trans_list, transop,
@@ -1041,7 +1041,7 @@ int
 lstcon_batch_run(char *name, int timeout, struct list_head *result_up)
 {
 	lstcon_batch_t *bat;
-	int	     rc;
+	int rc;
 
 	if (lstcon_batch_find(name, &amp;bat) != 0) {
 		CDEBUG(D_NET, "Can't find batch %s\n", name);
@@ -1063,7 +1063,7 @@ int
 lstcon_batch_stop(char *name, int force, struct list_head *result_up)
 {
 	lstcon_batch_t *bat;
-	int	     rc;
+	int rc;
 
 	if (lstcon_batch_find(name, &amp;bat) != 0) {
 		CDEBUG(D_NET, "Can't find batch %s\n", name);
@@ -1084,9 +1084,9 @@ lstcon_batch_stop(char *name, int force, struct list_head *result_up)
 static void
 lstcon_batch_destroy(lstcon_batch_t *bat)
 {
-	lstcon_ndlink_t    *ndl;
-	lstcon_test_t      *test;
-	int		 i;
+	lstcon_ndlink_t *ndl;
+	lstcon_test_t *test;
+	int i;
 
 	list_del(&amp;bat-&gt;bat_link);
 
@@ -1137,11 +1137,11 @@ lstcon_batch_destroy(lstcon_batch_t *bat)
 static int
 lstcon_testrpc_condition(int transop, lstcon_node_t *nd, void *arg)
 {
-	lstcon_test_t    *test;
-	lstcon_batch_t   *batch;
-	lstcon_ndlink_t  *ndl;
-	struct list_head       *hash;
-	struct list_head       *head;
+	lstcon_test_t *test;
+	lstcon_batch_t *batch;
+	lstcon_ndlink_t *ndl;
+	struct list_head *hash;
+	struct list_head *head;
 
 	test = (lstcon_test_t *)arg;
 	LASSERT(test != NULL);
@@ -1181,10 +1181,10 @@ lstcon_testrpc_condition(int transop, lstcon_node_t *nd, void *arg)
 static int
 lstcon_test_nodes_add(lstcon_test_t *test, struct list_head *result_up)
 {
-	lstcon_rpc_trans_t     *trans;
-	lstcon_group_t	 *grp;
-	int		     transop;
-	int		     rc;
+	lstcon_rpc_trans_t *trans;
+	lstcon_group_t *grp;
+	int transop;
+	int rc;
 
 	LASSERT(test-&gt;tes_src_grp != NULL);
 	LASSERT(test-&gt;tes_dst_grp != NULL);
@@ -1251,8 +1251,8 @@ lstcon_verify_batch(const char *name, lstcon_batch_t **batch)
 static int
 lstcon_verify_group(const char *name, lstcon_group_t **grp)
 {
-	int			rc;
-	lstcon_ndlink_t		*ndl;
+	int rc;
+	lstcon_ndlink_t *ndl;
 
 	rc = lstcon_group_find(name, grp);
 	if (rc != 0) {
@@ -1398,13 +1398,13 @@ lstcon_test_batch_query(char *name, int testidx, int client,
 			int timeout, struct list_head *result_up)
 {
 	lstcon_rpc_trans_t *trans;
-	struct list_head	 *translist;
-	struct list_head	 *ndlist;
-	lstcon_tsb_hdr_t   *hdr;
-	lstcon_batch_t     *batch;
-	lstcon_test_t      *test = NULL;
-	int		 transop;
-	int		 rc;
+	struct list_head *translist;
+	struct list_head *ndlist;
+	lstcon_tsb_hdr_t *hdr;
+	lstcon_batch_t *batch;
+	lstcon_test_t *test = NULL;
+	int transop;
+	int rc;
 
 	rc = lstcon_batch_find(name, &amp;batch);
 	if (rc != 0) {
@@ -1460,9 +1460,9 @@ lstcon_statrpc_readent(int transop, srpc_msg_t *msg,
 		       lstcon_rpc_ent_t *ent_up)
 {
 	srpc_stat_reply_t *rep = &amp;msg-&gt;msg_body.stat_reply;
-	sfw_counters_t    *sfwk_stat;
-	srpc_counters_t   *srpc_stat;
-	lnet_counters_t   *lnet_stat;
+	sfw_counters_t *sfwk_stat;
+	srpc_counters_t *srpc_stat;
+	lnet_counters_t *lnet_stat;
 
 	if (rep-&gt;str_status != 0)
 		return 0;
@@ -1483,9 +1483,9 @@ static int
 lstcon_ndlist_stat(struct list_head *ndlist,
 		   int timeout, struct list_head *result_up)
 {
-	struct list_head	  head;
+	struct list_head head;
 	lstcon_rpc_trans_t *trans;
-	int		 rc;
+	int rc;
 
 	INIT_LIST_HEAD(&amp;head);
 
@@ -1508,8 +1508,8 @@ lstcon_ndlist_stat(struct list_head *ndlist,
 int
 lstcon_group_stat(char *grp_name, int timeout, struct list_head *result_up)
 {
-	lstcon_group_t     *grp;
-	int		 rc;
+	lstcon_group_t *grp;
+	int rc;
 
 	rc = lstcon_group_find(grp_name, &amp;grp);
 	if (rc != 0) {
@@ -1528,11 +1528,11 @@ int
 lstcon_nodes_stat(int count, lnet_process_id_t *ids_up,
 		  int timeout, struct list_head *result_up)
 {
-	lstcon_ndlink_t	 *ndl;
-	lstcon_group_t	  *tmp;
-	lnet_process_id_t	id;
-	int		      i;
-	int		      rc;
+	lstcon_ndlink_t *ndl;
+	lstcon_group_t *tmp;
+	lnet_process_id_t id;
+	int i;
+	int rc;
 
 	rc = lstcon_group_alloc(NULL, &amp;tmp);
 	if (rc != 0) {
@@ -1604,7 +1604,7 @@ lstcon_batch_debug(int timeout, char *name,
 		   int client, struct list_head *result_up)
 {
 	lstcon_batch_t *bat;
-	int	     rc;
+	int rc;
 
 	rc = lstcon_batch_find(name, &amp;bat);
 	if (rc != 0)
@@ -1622,7 +1622,7 @@ lstcon_group_debug(int timeout, char *name,
 		   struct list_head *result_up)
 {
 	lstcon_group_t *grp;
-	int	     rc;
+	int rc;
 
 	rc = lstcon_group_find(name, &amp;grp);
 	if (rc != 0)
@@ -1640,11 +1640,11 @@ lstcon_nodes_debug(int timeout,
 		   int count, lnet_process_id_t *ids_up,
 		   struct list_head *result_up)
 {
-	lnet_process_id_t  id;
-	lstcon_ndlink_t   *ndl;
-	lstcon_group_t    *grp;
-	int		i;
-	int		rc;
+	lnet_process_id_t id;
+	lstcon_ndlink_t *ndl;
+	lstcon_group_t *grp;
+	int i;
+	int rc;
 
 	rc = lstcon_group_alloc(NULL, &amp;grp);
 	if (rc != 0) {
@@ -1689,7 +1689,7 @@ lstcon_session_match(lst_sid_t sid)
 static void
 lstcon_new_session_id(lst_sid_t *sid)
 {
-	lnet_process_id_t      id;
+	lnet_process_id_t id;
 
 	LASSERT(console_session.ses_state == LST_SESSION_NONE);
 
@@ -1704,8 +1704,8 @@ int
 lstcon_session_new(char *name, int key, unsigned feats,
 		   int timeout, int force, lst_sid_t *sid_up)
 {
-	int     rc = 0;
-	int     i;
+	int rc = 0;
+	int i;
 
 	if (console_session.ses_state != LST_SESSION_NONE) {
 		/* session exists */
@@ -1733,9 +1733,9 @@ lstcon_session_new(char *name, int key, unsigned feats,
 
 	lstcon_new_session_id(&amp;console_session.ses_id);
 
-	console_session.ses_key	    = key;
-	console_session.ses_state   = LST_SESSION_ACTIVE;
-	console_session.ses_force   = !!force;
+	console_session.ses_key = key;
+	console_session.ses_state = LST_SESSION_ACTIVE;
+	console_session.ses_force = !!force;
 	console_session.ses_features = feats;
 	console_session.ses_feats_updated = 0;
 	console_session.ses_timeout = (timeout &lt;= 0) ?
@@ -1770,8 +1770,8 @@ lstcon_session_info(lst_sid_t *sid_up, int *key_up, unsigned *featp,
 		    lstcon_ndlist_ent_t *ndinfo_up, char *name_up, int len)
 {
 	lstcon_ndlist_ent_t *entp;
-	lstcon_ndlink_t     *ndl;
-	int		  rc = 0;
+	lstcon_ndlink_t *ndl;
+	int rc = 0;
 
 	if (console_session.ses_state != LST_SESSION_ACTIVE)
 		return -ESRCH;
@@ -1802,9 +1802,9 @@ int
 lstcon_session_end(void)
 {
 	lstcon_rpc_trans_t *trans;
-	lstcon_group_t     *grp;
-	lstcon_batch_t     *bat;
-	int		 rc = 0;
+	lstcon_group_t *grp;
+	lstcon_batch_t *bat;
+	int rc = 0;
 
 	LASSERT(console_session.ses_state == LST_SESSION_ACTIVE);
 
@@ -1894,13 +1894,13 @@ lstcon_session_feats_check(unsigned feats)
 static int
 lstcon_acceptor_handle(srpc_server_rpc_t *rpc)
 {
-	srpc_msg_t	*rep  = &amp;rpc-&gt;srpc_replymsg;
-	srpc_msg_t	*req  = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
+	srpc_msg_t *rep  = &amp;rpc-&gt;srpc_replymsg;
+	srpc_msg_t *req  = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
 	srpc_join_reqst_t *jreq = &amp;req-&gt;msg_body.join_reqst;
 	srpc_join_reply_t *jrep = &amp;rep-&gt;msg_body.join_reply;
-	lstcon_group_t    *grp  = NULL;
-	lstcon_ndlink_t   *ndl;
-	int		rc   = 0;
+	lstcon_group_t *grp  = NULL;
+	lstcon_ndlink_t *ndl;
+	int rc   = 0;
 
 	sfw_unpack_message(req);
 
@@ -1978,9 +1978,9 @@ srpc_service_t lstcon_acceptor_service;
 static void lstcon_init_acceptor_service(void)
 {
 	/* initialize selftest console acceptor service table */
-	lstcon_acceptor_service.sv_name    = "join session";
-	lstcon_acceptor_service.sv_handler = lstcon_acceptor_handle;
-	lstcon_acceptor_service.sv_id      = SRPC_SERVICE_JOIN;
+	lstcon_acceptor_service.sv_name     = "join session";
+	lstcon_acceptor_service.sv_handler  = lstcon_acceptor_handle;
+	lstcon_acceptor_service.sv_id       = SRPC_SERVICE_JOIN;
 	lstcon_acceptor_service.sv_wi_total = SFW_FRWK_WI_MAX;
 }
 
@@ -1992,19 +1992,19 @@ static DECLARE_IOCTL_HANDLER(lstcon_ioctl_handler, lstcon_ioctl_entry);
 int
 lstcon_console_init(void)
 {
-	int     i;
-	int     rc;
+	int i;
+	int rc;
 
 	memset(&amp;console_session, 0, sizeof(lstcon_session_t));
 
-	console_session.ses_id		    = LST_INVALID_SID;
-	console_session.ses_state	    = LST_SESSION_NONE;
-	console_session.ses_timeout	    = 0;
-	console_session.ses_force	    = 0;
-	console_session.ses_expired	    = 0;
-	console_session.ses_feats_updated   = 0;
-	console_session.ses_features	    = LST_FEATS_MASK;
-	console_session.ses_laststamp	    = get_seconds();
+	console_session.ses_id		  = LST_INVALID_SID;
+	console_session.ses_state	  = LST_SESSION_NONE;
+	console_session.ses_timeout	  = 0;
+	console_session.ses_force	  = 0;
+	console_session.ses_expired	  = 0;
+	console_session.ses_feats_updated = 0;
+	console_session.ses_features	  = LST_FEATS_MASK;
+	console_session.ses_laststamp	  = get_seconds();
 
 	mutex_init(&amp;console_session.ses_mutex);
 
@@ -2062,7 +2062,7 @@ lstcon_console_init(void)
 int
 lstcon_console_fini(void)
 {
-	int     i;
+	int i;
 
 	libcfs_deregister_ioctl(&amp;lstcon_ioctl_handler);
 
diff --git a/drivers/staging/lustre/lnet/selftest/console.h b/drivers/staging/lustre/lnet/selftest/console.h
index e41ca89f10ba..c4cf0aed80e1 100644
--- a/drivers/staging/lustre/lnet/selftest/console.h
+++ b/drivers/staging/lustre/lnet/selftest/console.h
@@ -52,119 +52,121 @@
 #include "conrpc.h"
 
 typedef struct lstcon_node {
-	lnet_process_id_t    nd_id;	  /* id of the node */
-	int		  nd_ref;	 /* reference count */
-	int		  nd_state;       /* state of the node */
-	int		  nd_timeout;     /* session timeout */
-	unsigned long	   nd_stamp;       /* timestamp of last replied RPC */
-	struct lstcon_rpc    nd_ping;	/* ping rpc */
-} lstcon_node_t;				/*** node descriptor */
+	lnet_process_id_t nd_id;      /* id of the node */
+	int               nd_ref;     /* reference count */
+	int               nd_state;   /* state of the node */
+	int               nd_timeout; /* session timeout */
+	unsigned long     nd_stamp;   /* timestamp of last replied RPC */
+	struct lstcon_rpc nd_ping;    /* ping rpc */
+} lstcon_node_t; /* node descriptor */
 
 typedef struct {
-	struct list_head	   ndl_link;       /* chain on list */
-	struct list_head	   ndl_hlink;      /* chain on hash */
-	lstcon_node_t       *ndl_node;       /* pointer to node */
-} lstcon_ndlink_t;			      /*** node link descriptor */
+	struct list_head ndl_link;    /* chain on list */
+	struct list_head ndl_hlink;   /* chain on hash */
+	lstcon_node_t    *ndl_node;   /* pointer to node */
+} lstcon_ndlink_t; /* node link descriptor */
 
 typedef struct {
-	struct list_head	   grp_link;       /* chain on global group list */
-	int		  grp_ref;	/* reference count */
-	int		  grp_userland;   /* has userland nodes */
-	int		  grp_nnode;      /* # of nodes */
-	char		 grp_name[LST_NAME_SIZE]; /* group name */
+	struct list_head grp_link;                /* chain on global group list
+						   */
+	int              grp_ref;                 /* reference count */
+	int              grp_userland;            /* has userland nodes */
+	int              grp_nnode;               /* # of nodes */
+	char             grp_name[LST_NAME_SIZE]; /* group name */
 
-	struct list_head	   grp_trans_list; /* transaction list */
-	struct list_head	   grp_ndl_list;   /* nodes list */
-	struct list_head	   grp_ndl_hash[0];/* hash table for nodes */
-} lstcon_group_t;		    /*** (alias of nodes) group descriptor */
+	struct list_head grp_trans_list;          /* transaction list */
+	struct list_head grp_ndl_list;            /* nodes list */
+	struct list_head grp_ndl_hash[0];         /* hash table for nodes */
+} lstcon_group_t; /* (alias of nodes) group descriptor */
 
-#define LST_BATCH_IDLE	  0xB0	    /* idle batch */
-#define LST_BATCH_RUNNING       0xB1	    /* running batch */
+#define LST_BATCH_IDLE    0xB0	    /* idle batch */
+#define LST_BATCH_RUNNING 0xB1	    /* running batch */
 
 typedef struct lstcon_tsb_hdr {
-	lst_bid_t	       tsb_id;	 /* batch ID */
-	int		     tsb_index;      /* test index */
+	lst_bid_t        tsb_id;	 /* batch ID */
+	int              tsb_index;      /* test index */
 } lstcon_tsb_hdr_t;
 
 typedef struct {
-	lstcon_tsb_hdr_t	bat_hdr;	/* test_batch header */
-	struct list_head	      bat_link;       /* chain on session's batches list */
-	int		     bat_ntest;      /* # of test */
-	int		     bat_state;      /* state of the batch */
-	int		     bat_arg;	/* parameter for run|stop, timeout for run, force for stop */
-	char		    bat_name[LST_NAME_SIZE]; /* name of batch */
-
-	struct list_head	      bat_test_list;  /* list head of tests (lstcon_test_t) */
-	struct list_head	      bat_trans_list; /* list head of transaction */
-	struct list_head	      bat_cli_list;   /* list head of client nodes (lstcon_node_t) */
-	struct list_head	     *bat_cli_hash;   /* hash table of client nodes */
-	struct list_head	      bat_srv_list;   /* list head of server nodes */
-	struct list_head	     *bat_srv_hash;   /* hash table of server nodes */
-} lstcon_batch_t;			     /*** (tests ) batch descriptor */
+	lstcon_tsb_hdr_t bat_hdr;         /* test_batch header */
+	struct list_head bat_link;        /* chain on session's batches list */
+	int              bat_ntest;       /* # of test */
+	int              bat_state;       /* state of the batch */
+	int              bat_arg;         /* parameter for run|stop, timeout
+					   * for run, force for stop */
+	char             bat_name[LST_NAME_SIZE];/* name of batch */
+
+	struct list_head bat_test_list;   /* list head of tests (lstcon_test_t)
+					   */
+	struct list_head bat_trans_list;  /* list head of transaction */
+	struct list_head bat_cli_list;    /* list head of client nodes
+					   * (lstcon_node_t) */
+	struct list_head *bat_cli_hash;   /* hash table of client nodes */
+	struct list_head bat_srv_list;    /* list head of server nodes */
+	struct list_head *bat_srv_hash;   /* hash table of server nodes */
+} lstcon_batch_t; /* (tests ) batch descriptor */
 
 typedef struct lstcon_test {
-	lstcon_tsb_hdr_t      tes_hdr;	/* test batch header */
-	struct list_head	    tes_link;       /* chain on batch's tests list */
-	lstcon_batch_t       *tes_batch;      /* pointer to batch */
+	lstcon_tsb_hdr_t tes_hdr;        /* test batch header */
+	struct list_head tes_link;       /* chain on batch's tests list */
+	lstcon_batch_t   *tes_batch;     /* pointer to batch */
 
-	int		   tes_type;       /* type of the test, i.e: bulk, ping */
-	int		   tes_stop_onerr; /* stop on error */
-	int		   tes_oneside;    /* one-sided test */
-	int		   tes_concur;     /* concurrency */
-	int		   tes_loop;       /* loop count */
-	int		   tes_dist;       /* nodes distribution of target group */
-	int		   tes_span;       /* nodes span of target group */
-	int		   tes_cliidx;     /* client index, used for RPC creating */
+	int              tes_type;       /* type of the test, i.e: bulk, ping */
+	int              tes_stop_onerr; /* stop on error */
+	int              tes_oneside;    /* one-sided test */
+	int              tes_concur;     /* concurrency */
+	int              tes_loop;       /* loop count */
+	int              tes_dist;       /* nodes distribution of target group */
+	int              tes_span;       /* nodes span of target group */
+	int              tes_cliidx;     /* client index, used for RPC creating */
 
-	struct list_head  tes_trans_list; /* transaction list */
-	lstcon_group_t       *tes_src_grp;    /* group run the test */
-	lstcon_group_t       *tes_dst_grp;    /* target group */
+	struct list_head tes_trans_list; /* transaction list */
+	lstcon_group_t   *tes_src_grp;   /* group run the test */
+	lstcon_group_t   *tes_dst_grp;   /* target group */
 
-	int		   tes_paramlen;   /* test parameter length */
-	char		  tes_param[0];   /* test parameter */
-} lstcon_test_t;				/*** a single test descriptor */
+	int              tes_paramlen;   /* test parameter length */
+	char             tes_param[0];   /* test parameter */
+} lstcon_test_t; /* a single test descriptor */
 
-#define LST_GLOBAL_HASHSIZE     503	     /* global nodes hash table size */
-#define LST_NODE_HASHSIZE       239	     /* node hash table (for batch or group) */
+#define LST_GLOBAL_HASHSIZE 503	     /* global nodes hash table size */
+#define LST_NODE_HASHSIZE   239	     /* node hash table (for batch or group) */
 
-#define LST_SESSION_NONE	0x0	     /* no session */
-#define LST_SESSION_ACTIVE      0x1	     /* working session */
+#define LST_SESSION_NONE    0x0	     /* no session */
+#define LST_SESSION_ACTIVE  0x1	     /* working session */
 
-#define LST_CONSOLE_TIMEOUT     300	     /* default console timeout */
+#define LST_CONSOLE_TIMEOUT 300	     /* default console timeout */
 
 typedef struct {
-	struct mutex		ses_mutex;      /* only 1 thread in session */
-	lst_sid_t	       ses_id;	 /* global session id */
-	int		     ses_key;	/* local session key */
-	int		     ses_state;      /* state of session */
-	int		     ses_timeout;    /* timeout in seconds */
-	time_t		  ses_laststamp;  /* last operation stamp (seconds) */
-	/** tests features of the session */
-	unsigned		ses_features;
-	/** features are synced with remote test nodes */
-	unsigned		ses_feats_updated:1;
-	/** force creating */
-	unsigned		ses_force:1;
-	/** session is shutting down */
-	unsigned		ses_shutdown:1;
-	/** console is timedout */
-	unsigned		ses_expired:1;
-	__u64		   ses_id_cookie;  /* batch id cookie */
-	char		    ses_name[LST_NAME_SIZE];  /* session name */
-	lstcon_rpc_trans_t     *ses_ping;       /* session pinger */
-	stt_timer_t	     ses_ping_timer; /* timer for pinger */
-	lstcon_trans_stat_t     ses_trans_stat; /* transaction stats */
-
-	struct list_head	      ses_trans_list; /* global list of transaction */
-	struct list_head	      ses_grp_list;   /* global list of groups */
-	struct list_head	      ses_bat_list;   /* global list of batches */
-	struct list_head	      ses_ndl_list;   /* global list of nodes */
-	struct list_head	     *ses_ndl_hash;   /* hash table of nodes */
-
-	spinlock_t	  ses_rpc_lock;   /* serialize */
-	atomic_t	    ses_rpc_counter;/* # of initialized RPCs */
-	struct list_head	      ses_rpc_freelist; /* idle console rpc */
-} lstcon_session_t;			     /*** session descriptor */
+	struct mutex        ses_mutex;        /* only 1 thread in session */
+	lst_sid_t           ses_id;           /* global session id */
+	int                 ses_key;          /* local session key */
+	int                 ses_state;        /* state of session */
+	int                 ses_timeout;      /* timeout in seconds */
+	time_t              ses_laststamp;    /* last operation stamp (seconds)
+					       */
+	unsigned            ses_features;     /* tests features of the session
+					       */
+	unsigned            ses_feats_updated:1; /* features are synced with
+						  * remote test nodes */
+	unsigned            ses_force:1;      /* force creating */
+	unsigned            ses_shutdown:1;   /* session is shutting down */
+	unsigned            ses_expired:1;    /* console is timedout */
+	__u64               ses_id_cookie;    /* batch id cookie */
+	char                ses_name[LST_NAME_SIZE];/* session name */
+	lstcon_rpc_trans_t  *ses_ping;        /* session pinger */
+	stt_timer_t         ses_ping_timer;   /* timer for pinger */
+	lstcon_trans_stat_t ses_trans_stat;   /* transaction stats */
+
+	struct list_head    ses_trans_list;   /* global list of transaction */
+	struct list_head    ses_grp_list;     /* global list of groups */
+	struct list_head    ses_bat_list;     /* global list of batches */
+	struct list_head    ses_ndl_list;     /* global list of nodes */
+	struct list_head    *ses_ndl_hash;    /* hash table of nodes */
+
+	spinlock_t          ses_rpc_lock;     /* serialize */
+	atomic_t            ses_rpc_counter;  /* # of initialized RPCs */
+	struct list_head    ses_rpc_freelist; /* idle console rpc */
+} lstcon_session_t; /* session descriptor */
 
 extern lstcon_session_t	 console_session;
 
diff --git a/drivers/staging/lustre/lnet/selftest/framework.c b/drivers/staging/lustre/lnet/selftest/framework.c
index a93a90de0f85..7c5185a2a795 100644
--- a/drivers/staging/lustre/lnet/selftest/framework.c
+++ b/drivers/staging/lustre/lnet/selftest/framework.c
@@ -53,20 +53,20 @@ static int rpc_timeout = 64;
 module_param(rpc_timeout, int, 0644);
 MODULE_PARM_DESC(rpc_timeout, "rpc timeout in seconds (64 by default, 0 == never)");
 
-#define sfw_unpack_id(id)	       \
-do {				    \
+#define sfw_unpack_id(id)          \
+do {                               \
 	__swab64s(&amp;(id).nid);	   \
 	__swab32s(&amp;(id).pid);	   \
 } while (0)
 
-#define sfw_unpack_sid(sid)	     \
-do {				    \
+#define sfw_unpack_sid(sid)             \
+do {                                    \
 	__swab64s(&amp;(sid).ses_nid);      \
 	__swab64s(&amp;(sid).ses_stamp);    \
 } while (0)
 
-#define sfw_unpack_fw_counters(fc)	\
-do {				      \
+#define sfw_unpack_fw_counters(fc)        \
+do {                                      \
 	__swab32s(&amp;(fc).running_ms);      \
 	__swab32s(&amp;(fc).active_batches);  \
 	__swab32s(&amp;(fc).zombie_sessions); \
@@ -75,7 +75,7 @@ do {				      \
 } while (0)
 
 #define sfw_unpack_rpc_counters(rc)     \
-do {				    \
+do {                                    \
 	__swab32s(&amp;(rc).errors);	\
 	__swab32s(&amp;(rc).rpcs_sent);     \
 	__swab32s(&amp;(rc).rpcs_rcvd);     \
@@ -86,7 +86,7 @@ do {				    \
 } while (0)
 
 #define sfw_unpack_lnet_counters(lc)    \
-do {				    \
+do {                                    \
 	__swab32s(&amp;(lc).errors);	\
 	__swab32s(&amp;(lc).msgs_max);      \
 	__swab32s(&amp;(lc).msgs_alloc);    \
@@ -104,14 +104,14 @@ do {				    \
 #define sfw_batch_active(b)     (atomic_read(&amp;(b)-&gt;bat_nactive) != 0)
 
 static struct smoketest_framework {
-	struct list_head	 fw_zombie_rpcs;     /* RPCs to be recycled */
-	struct list_head	 fw_zombie_sessions; /* stopping sessions */
-	struct list_head	 fw_tests;	   /* registered test cases */
-	atomic_t       fw_nzombies;	/* # zombie sessions */
-	spinlock_t	   fw_lock;		/* serialise */
-	sfw_session_t	  *fw_session;		/* _the_ session */
-	int		   fw_shuttingdown;	/* shutdown in progress */
-	srpc_server_rpc_t *fw_active_srpc;	/* running RPC */
+	struct list_head  fw_zombie_rpcs;     /* RPCs to be recycled */
+	struct list_head  fw_zombie_sessions; /* stopping sessions */
+	struct list_head  fw_tests;           /* registered test cases */
+	atomic_t          fw_nzombies;        /* # zombie sessions */
+	spinlock_t        fw_lock;            /* serialise */
+	sfw_session_t     *fw_session;        /* _the_ session */
+	int               fw_shuttingdown;    /* shutdown in progress */
+	srpc_server_rpc_t *fw_active_srpc;    /* running RPC */
 } sfw_data;
 
 /* forward ref's */
@@ -160,7 +160,7 @@ static void
 sfw_add_session_timer(void)
 {
 	sfw_session_t *sn = sfw_data.fw_session;
-	stt_timer_t   *timer = &amp;sn-&gt;sn_timer;
+	stt_timer_t *timer = &amp;sn-&gt;sn_timer;
 
 	LASSERT(!sfw_data.fw_shuttingdown);
 
@@ -199,8 +199,8 @@ sfw_deactivate_session(void)
 	__must_hold(&amp;sfw_data.fw_lock)
 {
 	sfw_session_t *sn = sfw_data.fw_session;
-	int	    nactive = 0;
-	sfw_batch_t   *tsb;
+	int nactive = 0;
+	sfw_batch_t *tsb;
 	sfw_test_case_t *tsc;
 
 	if (sn == NULL) return;
@@ -273,7 +273,7 @@ sfw_init_session(sfw_session_t *sn, lst_sid_t sid,
 	strlcpy(&amp;sn-&gt;sn_name[0], name, sizeof(sn-&gt;sn_name));
 
 	sn-&gt;sn_timer_active = 0;
-	sn-&gt;sn_id	   = sid;
+	sn-&gt;sn_id           = sid;
 	sn-&gt;sn_features	    = features;
 	sn-&gt;sn_timeout      = session_timeout;
 	sn-&gt;sn_started      = cfs_time_current();
@@ -287,8 +287,8 @@ sfw_init_session(sfw_session_t *sn, lst_sid_t sid,
 static void
 sfw_server_rpc_done(struct srpc_server_rpc *rpc)
 {
-	struct srpc_service	*sv	= rpc-&gt;srpc_scd-&gt;scd_svc;
-	int			status	= rpc-&gt;srpc_status;
+	struct srpc_service *sv	= rpc-&gt;srpc_scd-&gt;scd_svc;
+	int status = rpc-&gt;srpc_status;
 
 	CDEBUG(D_NET,
 		"Incoming framework RPC done: service %s, peer %s, status %s:%d\n",
@@ -327,7 +327,7 @@ static sfw_batch_t *
 sfw_find_batch(lst_bid_t bid)
 {
 	sfw_session_t *sn = sfw_data.fw_session;
-	sfw_batch_t   *bat;
+	sfw_batch_t *bat;
 
 	LASSERT(sn != NULL);
 
@@ -343,7 +343,7 @@ static sfw_batch_t *
 sfw_bid2batch(lst_bid_t bid)
 {
 	sfw_session_t *sn = sfw_data.fw_session;
-	sfw_batch_t   *bat;
+	sfw_batch_t *bat;
 
 	LASSERT(sn != NULL);
 
@@ -368,10 +368,10 @@ sfw_bid2batch(lst_bid_t bid)
 static int
 sfw_get_stats(srpc_stat_reqst_t *request, srpc_stat_reply_t *reply)
 {
-	sfw_session_t  *sn = sfw_data.fw_session;
+	sfw_session_t *sn = sfw_data.fw_session;
 	sfw_counters_t *cnt = &amp;reply-&gt;str_fw;
-	sfw_batch_t    *bat;
-	struct timeval  tv;
+	sfw_batch_t *bat;
+	struct timeval tv;
 
 	reply-&gt;str_sid = (sn == NULL) ? LST_INVALID_SID : sn-&gt;sn_id;
 
@@ -412,9 +412,9 @@ int
 sfw_make_session(srpc_mksn_reqst_t *request, srpc_mksn_reply_t *reply)
 {
 	sfw_session_t *sn = sfw_data.fw_session;
-	srpc_msg_t    *msg = container_of(request, srpc_msg_t,
+	srpc_msg_t *msg = container_of(request, srpc_msg_t,
 					  msg_body.mksn_reqst);
-	int	       cplen = 0;
+	int cplen = 0;
 
 	if (request-&gt;mksn_sid.ses_nid == LNET_NID_ANY) {
 		reply-&gt;mksn_sid = (sn == NULL) ? LST_INVALID_SID : sn-&gt;sn_id;
@@ -533,7 +533,7 @@ sfw_debug_session(srpc_debug_reqst_t *request, srpc_debug_reply_t *reply)
 static void
 sfw_test_rpc_fini(srpc_client_rpc_t *rpc)
 {
-	sfw_test_unit_t     *tsu = rpc-&gt;crpc_priv;
+	sfw_test_unit_t *tsu = rpc-&gt;crpc_priv;
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
 
 	/* Called with hold of tsi-&gt;tsi_lock */
@@ -544,9 +544,9 @@ sfw_test_rpc_fini(srpc_client_rpc_t *rpc)
 static inline int
 sfw_test_buffers(sfw_test_instance_t *tsi)
 {
-	struct sfw_test_case	*tsc = sfw_find_test_case(tsi-&gt;tsi_service);
-	struct srpc_service	*svc = tsc-&gt;tsc_srv_service;
-	int			nbuf;
+	struct sfw_test_case *tsc = sfw_find_test_case(tsi-&gt;tsi_service);
+	struct srpc_service *svc = tsc-&gt;tsc_srv_service;
+	int nbuf;
 
 	nbuf = min(svc-&gt;sv_wi_total, tsi-&gt;tsi_loop) / svc-&gt;sv_ncpts;
 	return max(SFW_TEST_WI_MIN, nbuf + SFW_TEST_WI_EXTRA);
@@ -555,10 +555,10 @@ sfw_test_buffers(sfw_test_instance_t *tsi)
 static int
 sfw_load_test(struct sfw_test_instance *tsi)
 {
-	struct sfw_test_case	*tsc;
-	struct srpc_service	*svc;
-	int			nbuf;
-	int			rc;
+	struct sfw_test_case *tsc;
+	struct srpc_service *svc;
+	int nbuf;
+	int rc;
 
 	LASSERT(tsi != NULL);
 	tsc = sfw_find_test_case(tsi-&gt;tsi_service);
@@ -611,7 +611,7 @@ static void
 sfw_destroy_test_instance(sfw_test_instance_t *tsi)
 {
 	srpc_client_rpc_t *rpc;
-	sfw_test_unit_t   *tsu;
+	sfw_test_unit_t *tsu;
 
 	if (!tsi-&gt;tsi_is_client) goto clean;
 
@@ -728,14 +728,14 @@ sfw_unpack_addtest_req(srpc_msg_t *msg)
 static int
 sfw_add_test_instance(sfw_batch_t *tsb, srpc_server_rpc_t *rpc)
 {
-	srpc_msg_t	  *msg = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
-	srpc_test_reqst_t   *req = &amp;msg-&gt;msg_body.tes_reqst;
-	srpc_bulk_t	 *bk = rpc-&gt;srpc_bulk;
-	int		  ndest = req-&gt;tsr_ndest;
-	sfw_test_unit_t     *tsu;
+	srpc_msg_t *msg = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
+	srpc_test_reqst_t *req = &amp;msg-&gt;msg_body.tes_reqst;
+	srpc_bulk_t *bk = rpc-&gt;srpc_bulk;
+	int ndest = req-&gt;tsr_ndest;
+	sfw_test_unit_t *tsu;
 	sfw_test_instance_t *tsi;
-	int		  i;
-	int		  rc;
+	int i;
+	int rc;
 
 	LIBCFS_ALLOC(tsi, sizeof(*tsi));
 	if (tsi == NULL) {
@@ -751,9 +751,9 @@ sfw_add_test_instance(sfw_batch_t *tsb, srpc_server_rpc_t *rpc)
 	INIT_LIST_HEAD(&amp;tsi-&gt;tsi_active_rpcs);
 
 	tsi-&gt;tsi_stopping      = 0;
-	tsi-&gt;tsi_batch	 = tsb;
-	tsi-&gt;tsi_loop	  = req-&gt;tsr_loop;
-	tsi-&gt;tsi_concur	= req-&gt;tsr_concur;
+	tsi-&gt;tsi_batch         = tsb;
+	tsi-&gt;tsi_loop          = req-&gt;tsr_loop;
+	tsi-&gt;tsi_concur        = req-&gt;tsr_concur;
 	tsi-&gt;tsi_service       = req-&gt;tsr_service;
 	tsi-&gt;tsi_is_client     = !!(req-&gt;tsr_is_client);
 	tsi-&gt;tsi_stoptsu_onerr = !!(req-&gt;tsr_stop_onerr);
@@ -782,8 +782,8 @@ sfw_add_test_instance(sfw_batch_t *tsb, srpc_server_rpc_t *rpc)
 
 	for (i = 0; i &lt; ndest; i++) {
 		lnet_process_id_packed_t *dests;
-		lnet_process_id_packed_t  id;
-		int		       j;
+		lnet_process_id_packed_t id;
+		int j;
 
 		dests = page_address(bk-&gt;bk_iovs[i / SFW_ID_PER_PAGE].kiov_page);
 		LASSERT(dests != NULL);  /* my pages are within KVM always */
@@ -824,8 +824,8 @@ static void
 sfw_test_unit_done(sfw_test_unit_t *tsu)
 {
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
-	sfw_batch_t	 *tsb = tsi-&gt;tsi_batch;
-	sfw_session_t       *sn = tsb-&gt;bat_session;
+	sfw_batch_t *tsb = tsi-&gt;tsi_batch;
+	sfw_session_t *sn = tsb-&gt;bat_session;
 
 	LASSERT(sfw_test_active(tsi));
 
@@ -866,9 +866,9 @@ sfw_test_unit_done(sfw_test_unit_t *tsu)
 static void
 sfw_test_rpc_done(srpc_client_rpc_t *rpc)
 {
-	sfw_test_unit_t     *tsu = rpc-&gt;crpc_priv;
+	sfw_test_unit_t *tsu = rpc-&gt;crpc_priv;
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
-	int		  done = 0;
+	int done = 0;
 
 	tsi-&gt;tsi_ops-&gt;tso_done_rpc(tsu, rpc);
 
@@ -904,7 +904,7 @@ sfw_create_test_rpc(sfw_test_unit_t *tsu, lnet_process_id_t peer,
 		    unsigned features, int nblk, int blklen,
 		    srpc_client_rpc_t **rpcpp)
 {
-	srpc_client_rpc_t   *rpc = NULL;
+	srpc_client_rpc_t *rpc = NULL;
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
 
 	spin_lock(&amp;tsi-&gt;tsi_lock);
@@ -945,9 +945,9 @@ sfw_create_test_rpc(sfw_test_unit_t *tsu, lnet_process_id_t peer,
 static int
 sfw_run_test(swi_workitem_t *wi)
 {
-	sfw_test_unit_t     *tsu = wi-&gt;swi_workitem.wi_data;
+	sfw_test_unit_t *tsu = wi-&gt;swi_workitem.wi_data;
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
-	srpc_client_rpc_t   *rpc = NULL;
+	srpc_client_rpc_t *rpc = NULL;
 
 	LASSERT(wi == &amp;tsu-&gt;tsu_worker);
 
@@ -995,8 +995,8 @@ sfw_run_test(swi_workitem_t *wi)
 static int
 sfw_run_batch(sfw_batch_t *tsb)
 {
-	swi_workitem_t      *wi;
-	sfw_test_unit_t     *tsu;
+	swi_workitem_t *wi;
+	sfw_test_unit_t *tsu;
 	sfw_test_instance_t *tsi;
 
 	if (sfw_batch_active(tsb)) {
@@ -1032,7 +1032,7 @@ int
 sfw_stop_batch(sfw_batch_t *tsb, int force)
 {
 	sfw_test_instance_t *tsi;
-	srpc_client_rpc_t   *rpc;
+	srpc_client_rpc_t *rpc;
 
 	if (!sfw_batch_active(tsb)) {
 		CDEBUG(D_NET, "Batch %llu inactive\n", tsb-&gt;bat_id.bat_id);
@@ -1118,11 +1118,11 @@ sfw_alloc_pages(struct srpc_server_rpc *rpc, int cpt, int npages, int len,
 static int
 sfw_add_test(srpc_server_rpc_t *rpc)
 {
-	sfw_session_t     *sn = sfw_data.fw_session;
+	sfw_session_t *sn = sfw_data.fw_session;
 	srpc_test_reply_t *reply = &amp;rpc-&gt;srpc_replymsg.msg_body.tes_reply;
 	srpc_test_reqst_t *request;
-	int		rc;
-	sfw_batch_t       *bat;
+	int rc;
+	sfw_batch_t *bat;
 
 	request = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg.msg_body.tes_reqst;
 	reply-&gt;tsr_sid = (sn == NULL) ? LST_INVALID_SID : sn-&gt;sn_id;
@@ -1160,8 +1160,8 @@ sfw_add_test(srpc_server_rpc_t *rpc)
 
 	if (request-&gt;tsr_is_client &amp;&amp; rpc-&gt;srpc_bulk == NULL) {
 		/* rpc will be resumed later in sfw_bulk_ready */
-		int	npg = sfw_id_pages(request-&gt;tsr_ndest);
-		int	len;
+		int npg = sfw_id_pages(request-&gt;tsr_ndest);
+		int len;
 
 		if ((sn-&gt;sn_features &amp; LST_FEAT_BULK_LEN) == 0) {
 			len = npg * PAGE_CACHE_SIZE;
@@ -1189,8 +1189,8 @@ static int
 sfw_control_batch(srpc_batch_reqst_t *request, srpc_batch_reply_t *reply)
 {
 	sfw_session_t *sn = sfw_data.fw_session;
-	int	    rc = 0;
-	sfw_batch_t   *bat;
+	int rc = 0;
+	sfw_batch_t *bat;
 
 	reply-&gt;bar_sid = (sn == NULL) ? LST_INVALID_SID : sn-&gt;sn_id;
 
@@ -1229,11 +1229,11 @@ sfw_control_batch(srpc_batch_reqst_t *request, srpc_batch_reply_t *reply)
 static int
 sfw_handle_server_rpc(struct srpc_server_rpc *rpc)
 {
-	struct srpc_service	*sv = rpc-&gt;srpc_scd-&gt;scd_svc;
-	srpc_msg_t     *reply	= &amp;rpc-&gt;srpc_replymsg;
-	srpc_msg_t     *request	= &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
-	unsigned	features = LST_FEATS_MASK;
-	int		rc = 0;
+	struct srpc_service *sv = rpc-&gt;srpc_scd-&gt;scd_svc;
+	srpc_msg_t *reply = &amp;rpc-&gt;srpc_replymsg;
+	srpc_msg_t *request = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
+	unsigned features = LST_FEATS_MASK;
+	int rc = 0;
 
 	LASSERT(sfw_data.fw_active_srpc == NULL);
 	LASSERT(sv-&gt;sv_id &lt;= SRPC_FRAMEWORK_SERVICE_MAX_ID);
@@ -1334,8 +1334,8 @@ sfw_handle_server_rpc(struct srpc_server_rpc *rpc)
 static int
 sfw_bulk_ready(struct srpc_server_rpc *rpc, int status)
 {
-	struct srpc_service	*sv = rpc-&gt;srpc_scd-&gt;scd_svc;
-	int			rc;
+	struct srpc_service *sv = rpc-&gt;srpc_scd-&gt;scd_svc;
+	int rc;
 
 	LASSERT(rpc-&gt;srpc_bulk != NULL);
 	LASSERT(sv-&gt;sv_id == SRPC_SERVICE_TEST);
@@ -1640,10 +1640,10 @@ extern void brw_init_test_service(void);
 int
 sfw_startup(void)
 {
-	int	      i;
-	int	      rc;
-	int	      error;
-	srpc_service_t  *sv;
+	int i;
+	int rc;
+	int error;
+	srpc_service_t *sv;
 	sfw_test_case_t *tsc;
 
 
@@ -1735,9 +1735,9 @@ sfw_startup(void)
 void
 sfw_shutdown(void)
 {
-	srpc_service_t	*sv;
+	srpc_service_t *sv;
 	sfw_test_case_t	*tsc;
-	int		 i;
+	int i;
 
 	spin_lock(&amp;sfw_data.fw_lock);
 
diff --git a/drivers/staging/lustre/lnet/selftest/module.c b/drivers/staging/lustre/lnet/selftest/module.c
index 7ad62f167cea..09b8f4649796 100644
--- a/drivers/staging/lustre/lnet/selftest/module.c
+++ b/drivers/staging/lustre/lnet/selftest/module.c
@@ -39,7 +39,7 @@
 #include "selftest.h"
 
 enum {
-	LST_INIT_NONE		= 0,
+	LST_INIT_NONE = 0,
 	LST_INIT_WI_SERIAL,
 	LST_INIT_WI_TEST,
 	LST_INIT_RPC,
@@ -58,7 +58,7 @@ struct cfs_wi_sched **lst_sched_test;
 static void
 lnet_selftest_fini(void)
 {
-	int	i;
+	int i;
 
 	switch (lst_init_step) {
 	case LST_INIT_CONSOLE:
@@ -92,9 +92,9 @@ lnet_selftest_fini(void)
 static int
 lnet_selftest_init(void)
 {
-	int	nscheds;
-	int	rc;
-	int	i;
+	int nscheds;
+	int rc;
+	int i;
 
 	rc = cfs_wi_sched_create("lst_s", lnet_cpt_table(), CFS_CPT_ANY,
 				 1, &amp;lst_sched_serial);
diff --git a/drivers/staging/lustre/lnet/selftest/ping_test.c b/drivers/staging/lustre/lnet/selftest/ping_test.c
index 644069a9fe4e..1dab9984c58e 100644
--- a/drivers/staging/lustre/lnet/selftest/ping_test.c
+++ b/drivers/staging/lustre/lnet/selftest/ping_test.c
@@ -73,7 +73,7 @@ static void
 ping_client_fini(sfw_test_instance_t *tsi)
 {
 	sfw_session_t *sn = tsi-&gt;tsi_batch-&gt;bat_session;
-	int	    errors;
+	int errors;
 
 	LASSERT(sn != NULL);
 	LASSERT(tsi-&gt;tsi_is_client);
@@ -89,11 +89,11 @@ static int
 ping_client_prep_rpc(sfw_test_unit_t *tsu,
 		     lnet_process_id_t dest, srpc_client_rpc_t **rpc)
 {
-	srpc_ping_reqst_t   *req;
+	srpc_ping_reqst_t *req;
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
-	sfw_session_t       *sn  = tsi-&gt;tsi_batch-&gt;bat_session;
-	struct timeval       tv;
-	int		     rc;
+	sfw_session_t *sn = tsi-&gt;tsi_batch-&gt;bat_session;
+	struct timeval tv;
+	int rc;
 
 	LASSERT(sn != NULL);
 	LASSERT((sn-&gt;sn_features &amp; ~LST_FEATS_MASK) == 0);
@@ -121,10 +121,10 @@ static void
 ping_client_done_rpc(sfw_test_unit_t *tsu, srpc_client_rpc_t *rpc)
 {
 	sfw_test_instance_t *tsi = tsu-&gt;tsu_instance;
-	sfw_session_t       *sn = tsi-&gt;tsi_batch-&gt;bat_session;
-	srpc_ping_reqst_t   *reqst = &amp;rpc-&gt;crpc_reqstmsg.msg_body.ping_reqst;
-	srpc_ping_reply_t   *reply = &amp;rpc-&gt;crpc_replymsg.msg_body.ping_reply;
-	struct timeval       tv;
+	sfw_session_t *sn = tsi-&gt;tsi_batch-&gt;bat_session;
+	srpc_ping_reqst_t *reqst = &amp;rpc-&gt;crpc_reqstmsg.msg_body.ping_reqst;
+	srpc_ping_reply_t *reply = &amp;rpc-&gt;crpc_replymsg.msg_body.ping_reply;
+	struct timeval tv;
 
 	LASSERT(sn != NULL);
 
@@ -171,9 +171,9 @@ ping_client_done_rpc(sfw_test_unit_t *tsu, srpc_client_rpc_t *rpc)
 static int
 ping_server_handle(struct srpc_server_rpc *rpc)
 {
-	struct srpc_service	*sv  = rpc-&gt;srpc_scd-&gt;scd_svc;
-	srpc_msg_t	*reqstmsg = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
-	srpc_msg_t	  *replymsg = &amp;rpc-&gt;srpc_replymsg;
+	struct srpc_service *sv  = rpc-&gt;srpc_scd-&gt;scd_svc;
+	srpc_msg_t *reqstmsg = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
+	srpc_msg_t *replymsg = &amp;rpc-&gt;srpc_replymsg;
 	srpc_ping_reqst_t *req = &amp;reqstmsg-&gt;msg_body.ping_reqst;
 	srpc_ping_reply_t *rep = &amp;rpc-&gt;srpc_replymsg.msg_body.ping_reply;
 
diff --git a/drivers/staging/lustre/lnet/selftest/rpc.c b/drivers/staging/lustre/lnet/selftest/rpc.c
index 080788ab749e..59cf01ff4334 100644
--- a/drivers/staging/lustre/lnet/selftest/rpc.c
+++ b/drivers/staging/lustre/lnet/selftest/rpc.c
@@ -104,7 +104,7 @@ srpc_add_bulk_page(srpc_bulk_t *bk, struct page *pg, int i, int nob)
 void
 srpc_free_bulk(srpc_bulk_t *bk)
 {
-	int	 i;
+	int i;
 	struct page *pg;
 
 	LASSERT(bk != NULL);
@@ -124,8 +124,8 @@ srpc_free_bulk(srpc_bulk_t *bk)
 srpc_bulk_t *
 srpc_alloc_bulk(int cpt, unsigned bulk_npg, unsigned bulk_len, int sink)
 {
-	srpc_bulk_t  *bk;
-	int	      i;
+	srpc_bulk_t *bk;
+	int i;
 
 	LASSERT(bulk_npg &gt; 0 &amp;&amp; bulk_npg &lt;= LNET_MAX_IOV);
 
@@ -143,7 +143,7 @@ srpc_alloc_bulk(int cpt, unsigned bulk_npg, unsigned bulk_len, int sink)
 
 	for (i = 0; i &lt; bulk_npg; i++) {
 		struct page *pg;
-		int	    nob;
+		int nob;
 
 		pg = alloc_pages_node(cfs_cpt_spread_node(lnet_cpt_table(), cpt),
 				      GFP_IOFS, 0);
@@ -193,11 +193,11 @@ srpc_init_server_rpc(struct srpc_server_rpc *rpc,
 static void
 srpc_service_fini(struct srpc_service *svc)
 {
-	struct srpc_service_cd	*scd;
-	struct srpc_server_rpc	*rpc;
-	struct srpc_buffer	*buf;
-	struct list_head		*q;
-	int			i;
+	struct srpc_service_cd *scd;
+	struct srpc_server_rpc *rpc;
+	struct srpc_buffer *buf;
+	struct list_head *q;
+	int i;
 
 	if (svc-&gt;sv_cpt_data == NULL)
 		return;
@@ -249,11 +249,11 @@ int srpc_add_buffer(struct swi_workitem *wi);
 static int
 srpc_service_init(struct srpc_service *svc)
 {
-	struct srpc_service_cd	*scd;
-	struct srpc_server_rpc	*rpc;
-	int			nrpcs;
-	int			i;
-	int			j;
+	struct srpc_service_cd *scd;
+	struct srpc_server_rpc *rpc;
+	int nrpcs;
+	int i;
+	int j;
 
 	svc-&gt;sv_shuttingdown = 0;
 
@@ -357,8 +357,8 @@ srpc_post_passive_rdma(int portal, int local, __u64 matchbits, void *buf,
 		       int len, int options, lnet_process_id_t peer,
 		       lnet_handle_md_t *mdh, srpc_event_t *ev)
 {
-	int		 rc;
-	lnet_md_t	 md;
+	int rc;
+	lnet_md_t md;
 	lnet_handle_me_t meh;
 
 	rc = LNetMEAttach(portal, peer, matchbits, 0, LNET_UNLINK,
@@ -397,7 +397,7 @@ srpc_post_active_rdma(int portal, __u64 matchbits, void *buf, int len,
 		      int options, lnet_process_id_t peer, lnet_nid_t self,
 		      lnet_handle_md_t *mdh, srpc_event_t *ev)
 {
-	int       rc;
+	int rc;
 	lnet_md_t md;
 
 	md.user_ptr  = ev;
@@ -471,9 +471,9 @@ static int
 srpc_service_post_buffer(struct srpc_service_cd *scd, struct srpc_buffer *buf)
 	__must_hold(&amp;scd-&gt;scd_lock)
 {
-	struct srpc_service	*sv = scd-&gt;scd_svc;
-	struct srpc_msg		*msg = &amp;buf-&gt;buf_msg;
-	int			rc;
+	struct srpc_service *sv = scd-&gt;scd_svc;
+	struct srpc_msg *msg = &amp;buf-&gt;buf_msg;
+	int rc;
 
 	LNetInvalidateHandle(&amp;buf-&gt;buf_mdh);
 	list_add(&amp;buf-&gt;buf_list, &amp;scd-&gt;scd_buf_posted);
@@ -519,9 +519,9 @@ srpc_service_post_buffer(struct srpc_service_cd *scd, struct srpc_buffer *buf)
 int
 srpc_add_buffer(struct swi_workitem *wi)
 {
-	struct srpc_service_cd	*scd = wi-&gt;swi_workitem.wi_data;
-	struct srpc_buffer	*buf;
-	int			rc = 0;
+	struct srpc_service_cd *scd = wi-&gt;swi_workitem.wi_data;
+	struct srpc_buffer *buf;
+	int rc = 0;
 
 	/* it's called by workitem scheduler threads, these threads
 	 * should have been set CPT affinity, so buffers will be posted
@@ -579,9 +579,9 @@ srpc_add_buffer(struct swi_workitem *wi)
 int
 srpc_service_add_buffers(struct srpc_service *sv, int nbuffer)
 {
-	struct srpc_service_cd	*scd;
-	int			rc = 0;
-	int			i;
+	struct srpc_service_cd *scd;
+	int rc = 0;
+	int i;
 
 	LASSERTF(nbuffer &gt; 0, "nbuffer must be positive: %d\n", nbuffer);
 
@@ -633,9 +633,9 @@ srpc_service_add_buffers(struct srpc_service *sv, int nbuffer)
 void
 srpc_service_remove_buffers(struct srpc_service *sv, int nbuffer)
 {
-	struct srpc_service_cd	*scd;
-	int			num;
-	int			i;
+	struct srpc_service_cd *scd;
+	int num;
+	int i;
 
 	LASSERT(!sv-&gt;sv_shuttingdown);
 
@@ -653,9 +653,9 @@ srpc_service_remove_buffers(struct srpc_service *sv, int nbuffer)
 int
 srpc_finish_service(struct srpc_service *sv)
 {
-	struct srpc_service_cd	*scd;
-	struct srpc_server_rpc	*rpc;
-	int			i;
+	struct srpc_service_cd *scd;
+	struct srpc_server_rpc *rpc;
+	int i;
 
 	LASSERT(sv-&gt;sv_shuttingdown); /* srpc_shutdown_service called */
 
@@ -731,9 +731,9 @@ srpc_service_recycle_buffer(struct srpc_service_cd *scd, srpc_buffer_t *buf)
 void
 srpc_abort_service(struct srpc_service *sv)
 {
-	struct srpc_service_cd	*scd;
-	struct srpc_server_rpc	*rpc;
-	int			i;
+	struct srpc_service_cd *scd;
+	struct srpc_server_rpc *rpc;
+	int i;
 
 	CDEBUG(D_NET, "Aborting service: id %d, name %s\n",
 	       sv-&gt;sv_id, sv-&gt;sv_name);
@@ -756,10 +756,10 @@ srpc_abort_service(struct srpc_service *sv)
 void
 srpc_shutdown_service(srpc_service_t *sv)
 {
-	struct srpc_service_cd	*scd;
-	struct srpc_server_rpc	*rpc;
-	srpc_buffer_t		*buf;
-	int			i;
+	struct srpc_service_cd *scd;
+	struct srpc_server_rpc *rpc;
+	srpc_buffer_t *buf;
+	int i;
 
 	CDEBUG(D_NET, "Shutting down service: id %d, name %s\n",
 	       sv-&gt;sv_id, sv-&gt;sv_name);
@@ -792,7 +792,7 @@ static int
 srpc_send_request(srpc_client_rpc_t *rpc)
 {
 	srpc_event_t *ev = &amp;rpc-&gt;crpc_reqstev;
-	int	   rc;
+	int rc;
 
 	ev-&gt;ev_fired = 0;
 	ev-&gt;ev_data  = rpc;
@@ -812,8 +812,8 @@ static int
 srpc_prepare_reply(srpc_client_rpc_t *rpc)
 {
 	srpc_event_t *ev = &amp;rpc-&gt;crpc_replyev;
-	__u64	*id = &amp;rpc-&gt;crpc_reqstmsg.msg_body.reqst.rpyid;
-	int	   rc;
+	__u64 *id = &amp;rpc-&gt;crpc_reqstmsg.msg_body.reqst.rpyid;
+	int rc;
 
 	ev-&gt;ev_fired = 0;
 	ev-&gt;ev_data  = rpc;
@@ -835,11 +835,11 @@ srpc_prepare_reply(srpc_client_rpc_t *rpc)
 static int
 srpc_prepare_bulk(srpc_client_rpc_t *rpc)
 {
-	srpc_bulk_t  *bk = &amp;rpc-&gt;crpc_bulk;
+	srpc_bulk_t *bk = &amp;rpc-&gt;crpc_bulk;
 	srpc_event_t *ev = &amp;rpc-&gt;crpc_bulkev;
 	__u64	*id = &amp;rpc-&gt;crpc_reqstmsg.msg_body.reqst.bulkid;
-	int	   rc;
-	int	   opt;
+	int rc;
+	int opt;
 
 	LASSERT(bk-&gt;bk_niov &lt;= LNET_MAX_IOV);
 
@@ -868,11 +868,11 @@ srpc_prepare_bulk(srpc_client_rpc_t *rpc)
 static int
 srpc_do_bulk(srpc_server_rpc_t *rpc)
 {
-	srpc_event_t  *ev = &amp;rpc-&gt;srpc_ev;
-	srpc_bulk_t   *bk = rpc-&gt;srpc_bulk;
-	__u64	  id = rpc-&gt;srpc_reqstbuf-&gt;buf_msg.msg_body.reqst.bulkid;
-	int	    rc;
-	int	    opt;
+	srpc_event_t *ev = &amp;rpc-&gt;srpc_ev;
+	srpc_bulk_t *bk = rpc-&gt;srpc_bulk;
+	__u64 id = rpc-&gt;srpc_reqstbuf-&gt;buf_msg.msg_body.reqst.bulkid;
+	int rc;
+	int opt;
 
 	LASSERT(bk != NULL);
 
@@ -896,9 +896,9 @@ srpc_do_bulk(srpc_server_rpc_t *rpc)
 static void
 srpc_server_rpc_done(srpc_server_rpc_t *rpc, int status)
 {
-	struct srpc_service_cd	*scd = rpc-&gt;srpc_scd;
-	struct srpc_service	*sv  = scd-&gt;scd_svc;
-	srpc_buffer_t		*buffer;
+	struct srpc_service_cd *scd = rpc-&gt;srpc_scd;
+	struct srpc_service *sv  = scd-&gt;scd_svc;
+	srpc_buffer_t *buffer;
 
 	LASSERT(status != 0 || rpc-&gt;srpc_wi.swi_state == SWI_STATE_DONE);
 
@@ -959,11 +959,11 @@ srpc_server_rpc_done(srpc_server_rpc_t *rpc, int status)
 int
 srpc_handle_rpc(swi_workitem_t *wi)
 {
-	struct srpc_server_rpc	*rpc = wi-&gt;swi_workitem.wi_data;
-	struct srpc_service_cd	*scd = rpc-&gt;srpc_scd;
-	struct srpc_service	*sv = scd-&gt;scd_svc;
-	srpc_event_t		*ev = &amp;rpc-&gt;srpc_ev;
-	int			rc = 0;
+	struct srpc_server_rpc *rpc = wi-&gt;swi_workitem.wi_data;
+	struct srpc_service_cd *scd = rpc-&gt;srpc_scd;
+	struct srpc_service *sv = scd-&gt;scd_svc;
+	srpc_event_t *ev = &amp;rpc-&gt;srpc_ev;
+	int rc = 0;
 
 	LASSERT(wi == &amp;rpc-&gt;srpc_wi);
 
@@ -989,7 +989,7 @@ srpc_handle_rpc(swi_workitem_t *wi)
 	default:
 		LBUG();
 	case SWI_STATE_NEWBORN: {
-		srpc_msg_t	   *msg;
+		srpc_msg_t *msg;
 		srpc_generic_reply_t *reply;
 
 		msg = &amp;rpc-&gt;srpc_reqstbuf-&gt;buf_msg;
@@ -1173,10 +1173,10 @@ srpc_client_rpc_done(srpc_client_rpc_t *rpc, int status)
 int
 srpc_send_rpc(swi_workitem_t *wi)
 {
-	int		rc = 0;
+	int rc = 0;
 	srpc_client_rpc_t *rpc;
-	srpc_msg_t	*reply;
-	int		do_bulk;
+	srpc_msg_t *reply;
+	int do_bulk;
 
 	LASSERT(wi != NULL);
 
@@ -1359,13 +1359,13 @@ srpc_post_rpc(srpc_client_rpc_t *rpc)
 int
 srpc_send_reply(struct srpc_server_rpc *rpc)
 {
-	srpc_event_t		*ev = &amp;rpc-&gt;srpc_ev;
-	struct srpc_msg		*msg = &amp;rpc-&gt;srpc_replymsg;
-	struct srpc_buffer	*buffer = rpc-&gt;srpc_reqstbuf;
-	struct srpc_service_cd	*scd = rpc-&gt;srpc_scd;
-	struct srpc_service	*sv = scd-&gt;scd_svc;
-	__u64			rpyid;
-	int			rc;
+	srpc_event_t *ev = &amp;rpc-&gt;srpc_ev;
+	struct srpc_msg *msg = &amp;rpc-&gt;srpc_replymsg;
+	struct srpc_buffer *buffer = rpc-&gt;srpc_reqstbuf;
+	struct srpc_service_cd *scd = rpc-&gt;srpc_scd;
+	struct srpc_service *sv = scd-&gt;scd_svc;
+	__u64 rpyid;
+	int rc;
 
 	LASSERT(buffer != NULL);
 	rpyid = buffer-&gt;buf_msg.msg_body.reqst.rpyid;
@@ -1403,14 +1403,14 @@ srpc_send_reply(struct srpc_server_rpc *rpc)
 static void
 srpc_lnet_ev_handler(lnet_event_t *ev)
 {
-	struct srpc_service_cd	*scd;
-	srpc_event_t      *rpcev = ev-&gt;md.user_ptr;
+	struct srpc_service_cd *scd;
+	srpc_event_t *rpcev = ev-&gt;md.user_ptr;
 	srpc_client_rpc_t *crpc;
 	srpc_server_rpc_t *srpc;
-	srpc_buffer_t     *buffer;
-	srpc_service_t    *sv;
-	srpc_msg_t	*msg;
-	srpc_msg_type_t    type;
+	srpc_buffer_t *buffer;
+	srpc_service_t *sv;
+	srpc_msg_t *msg;
+	srpc_msg_type_t type;
 
 	LASSERT(!in_interrupt());
 
diff --git a/drivers/staging/lustre/lnet/selftest/rpc.h b/drivers/staging/lustre/lnet/selftest/rpc.h
index fbeb75fe5922..b7b00c6b1004 100644
--- a/drivers/staging/lustre/lnet/selftest/rpc.h
+++ b/drivers/staging/lustre/lnet/selftest/rpc.h
@@ -79,60 +79,61 @@ typedef struct {
 } WIRE_ATTR srpc_generic_reqst_t;
 
 typedef struct {
-	__u32		   status;
-	lst_sid_t	       sid;
+	__u32                   status;
+	lst_sid_t               sid;
 } WIRE_ATTR srpc_generic_reply_t;
 
 /* FRAMEWORK RPCs */
 typedef struct {
-	__u64			mksn_rpyid;      /* reply buffer matchbits */
-	lst_sid_t	       mksn_sid;	/* session id */
-	__u32			mksn_force;      /* use brute force */
+	__u64                   mksn_rpyid;     /* reply buffer matchbits */
+	lst_sid_t               mksn_sid;	/* session id */
+	__u32			mksn_force;     /* use brute force */
 	char			mksn_name[LST_NAME_SIZE];
-} WIRE_ATTR srpc_mksn_reqst_t;			/* make session request */
+} WIRE_ATTR srpc_mksn_reqst_t; /* make session request */
 
 typedef struct {
-	__u32		   mksn_status;      /* session status */
-	lst_sid_t	       mksn_sid;	 /* session id */
-	__u32		   mksn_timeout;     /* session timeout */
-	char			mksn_name[LST_NAME_SIZE];
+	__u32                   mksn_status;    /* session status */
+	lst_sid_t               mksn_sid;       /* session id */
+	__u32                   mksn_timeout;   /* session timeout */
+	char                    mksn_name[LST_NAME_SIZE];
 } WIRE_ATTR srpc_mksn_reply_t; /* make session reply */
 
 typedef struct {
-	__u64			rmsn_rpyid;      /* reply buffer matchbits */
-	lst_sid_t		rmsn_sid;	/* session id */
+	__u64                   rmsn_rpyid;     /* reply buffer matchbits */
+	lst_sid_t               rmsn_sid;       /* session id */
 } WIRE_ATTR srpc_rmsn_reqst_t; /* remove session request */
 
 typedef struct {
-	__u32			rmsn_status;
-	lst_sid_t		rmsn_sid;	/* session id */
+	__u32                   rmsn_status;
+	lst_sid_t               rmsn_sid;       /* session id */
 } WIRE_ATTR srpc_rmsn_reply_t; /* remove session reply */
 
 typedef struct {
-	__u64			join_rpyid;     /* reply buffer matchbits */
-	lst_sid_t	       join_sid;       /* session id to join */
-	char		    join_group[LST_NAME_SIZE]; /* group name */
+	__u64                   join_rpyid;     /* reply buffer matchbits */
+	lst_sid_t               join_sid;       /* session id to join */
+	char                    join_group[LST_NAME_SIZE]; /* group name */
 } WIRE_ATTR srpc_join_reqst_t;
 
 typedef struct {
-	__u32		   join_status;    /* returned status */
-	lst_sid_t	       join_sid;       /* session id */
-	__u32			join_timeout;   /* # seconds' inactivity to expire */
-	char		    join_session[LST_NAME_SIZE]; /* session name */
+	__u32                   join_status;    /* returned status */
+	lst_sid_t               join_sid;       /* session id */
+	__u32			join_timeout;   /* # seconds' inactivity to
+						 * expire */
+	char                    join_session[LST_NAME_SIZE]; /* session name */
 } WIRE_ATTR srpc_join_reply_t;
 
 typedef struct {
-	__u64		   dbg_rpyid;      /* reply buffer matchbits */
-	lst_sid_t	       dbg_sid;	/* session id */
-	__u32		   dbg_flags;      /* bitmap of debug */
+	__u64                   dbg_rpyid;      /* reply buffer matchbits */
+	lst_sid_t               dbg_sid;        /* session id */
+	__u32                   dbg_flags;      /* bitmap of debug */
 } WIRE_ATTR srpc_debug_reqst_t;
 
 typedef struct {
-	__u32		   dbg_status;     /* returned code */
-	lst_sid_t	       dbg_sid;	/* session id */
-	__u32		   dbg_timeout;    /* session timeout */
-	__u32		   dbg_nbatch;     /* # of batches in the node */
-	char		    dbg_name[LST_NAME_SIZE]; /* session name */
+	__u32                   dbg_status;     /* returned code */
+	lst_sid_t               dbg_sid;        /* session id */
+	__u32                   dbg_timeout;    /* session timeout */
+	__u32                   dbg_nbatch;     /* # of batches in the node */
+	char                    dbg_name[LST_NAME_SIZE]; /* session name */
 } WIRE_ATTR srpc_debug_reply_t;
 
 #define SRPC_BATCH_OPC_RUN      1
@@ -140,55 +141,51 @@ typedef struct {
 #define SRPC_BATCH_OPC_QUERY    3
 
 typedef struct {
-	__u64		   bar_rpyid;      /* reply buffer matchbits */
-	lst_sid_t	       bar_sid;	/* session id */
-	lst_bid_t	       bar_bid;	/* batch id */
-	__u32		   bar_opc;	/* create/start/stop batch */
-	__u32		   bar_testidx;    /* index of test */
-	__u32		   bar_arg;	/* parameters */
+	__u64              bar_rpyid;      /* reply buffer matchbits */
+	lst_sid_t          bar_sid;        /* session id */
+	lst_bid_t          bar_bid;        /* batch id */
+	__u32              bar_opc;        /* create/start/stop batch */
+	__u32              bar_testidx;    /* index of test */
+	__u32              bar_arg;        /* parameters */
 } WIRE_ATTR srpc_batch_reqst_t;
 
 typedef struct {
-	__u32		   bar_status;     /* status of request */
-	lst_sid_t	       bar_sid;	/* session id */
-	__u32		   bar_active;     /* # of active tests in batch/test */
-	__u32		   bar_time;       /* remained time */
+	__u32              bar_status;     /* status of request */
+	lst_sid_t          bar_sid;        /* session id */
+	__u32              bar_active;     /* # of active tests in batch/test */
+	__u32              bar_time;       /* remained time */
 } WIRE_ATTR srpc_batch_reply_t;
 
 typedef struct {
-	__u64		   str_rpyid;      /* reply buffer matchbits */
-	lst_sid_t	       str_sid;	/* session id */
-	__u32		   str_type;       /* type of stat */
+	__u64              str_rpyid;      /* reply buffer matchbits */
+	lst_sid_t          str_sid;        /* session id */
+	__u32              str_type;       /* type of stat */
 } WIRE_ATTR srpc_stat_reqst_t;
 
 typedef struct {
-	__u32		   str_status;
-	lst_sid_t	       str_sid;
-	sfw_counters_t	  str_fw;
-	srpc_counters_t	 str_rpc;
-	lnet_counters_t	 str_lnet;
+	__u32              str_status;
+	lst_sid_t          str_sid;
+	sfw_counters_t     str_fw;
+	srpc_counters_t    str_rpc;
+	lnet_counters_t    str_lnet;
 } WIRE_ATTR srpc_stat_reply_t;
 
 typedef struct {
-	__u32		   blk_opc;	/* bulk operation code */
-	__u32		   blk_npg;	/* # of pages */
-	__u32		   blk_flags;      /* reserved flags */
+	__u32              blk_opc;        /* bulk operation code */
+	__u32              blk_npg;        /* # of pages */
+	__u32              blk_flags;      /* reserved flags */
 } WIRE_ATTR test_bulk_req_t;
 
 typedef struct {
-	/** bulk operation code */
-	__u16			blk_opc;
-	/** data check flags */
-	__u16			blk_flags;
-	/** data length */
-	__u32			blk_len;
-	/** reserved: offset */
-	__u32		   blk_offset;
+	__u16              blk_opc;        /* bulk operation code */
+	__u16              blk_flags;      /* data check flags */
+	__u32              blk_len;        /* data length */
+	__u32              blk_offset;     /* reserved: offset */
 } WIRE_ATTR test_bulk_req_v1_t;
 
 typedef struct {
-	__u32			png_size;       /* size of ping message */
-	__u32			png_flags;      /* reserved flags */
+	__u32              png_size;       /* size of ping message */
+	__u32              png_flags;      /* reserved flags */
 } WIRE_ATTR test_ping_req_t;
 
 typedef struct {
@@ -197,8 +194,8 @@ typedef struct {
 	lst_sid_t		tsr_sid;	/* session id */
 	lst_bid_t		tsr_bid;	/* batch id */
 	__u32			tsr_service;    /* test type: bulk|ping|... */
-	/* test client loop count or # server buffers needed */
-	__u32			tsr_loop;
+	__u32			tsr_loop;       /* test client loop count or
+						 * # server buffers needed */
 	__u32			tsr_concur;     /* concurrency of test */
 	__u8			tsr_is_client;  /* is test client or not */
 	__u8			tsr_stop_onerr; /* stop on error */
@@ -234,8 +231,8 @@ typedef struct {
 typedef struct {
 	__u64		   brw_rpyid;      /* reply buffer matchbits */
 	__u64		   brw_bulkid;     /* bulk buffer matchbits */
-	__u32		   brw_rw;	 /* read or write */
-	__u32		   brw_len;	/* bulk data len */
+	__u32		   brw_rw;         /* read or write */
+	__u32		   brw_len;        /* bulk data len */
 	__u32		   brw_flags;      /* bulk data patterns */
 } WIRE_ATTR srpc_brw_reqst_t; /* bulk r/w request */
 
@@ -243,20 +240,16 @@ typedef struct {
 	__u32		   brw_status;
 } WIRE_ATTR srpc_brw_reply_t; /* bulk r/w reply */
 
-#define SRPC_MSG_MAGIC		  0xeeb0f00d
-#define SRPC_MSG_VERSION		1
+#define SRPC_MSG_MAGIC   0xeeb0f00d
+#define SRPC_MSG_VERSION 1
 
 typedef struct srpc_msg {
-	/** magic number */
-	__u32	msg_magic;
-	/** message version number */
-	__u32	msg_version;
-	/** type of message body: srpc_msg_type_t */
-	__u32	msg_type;
+	__u32	msg_magic;     /* magic number */
+	__u32	msg_version;   /* message version number */
+	__u32	msg_type;      /* type of message body: srpc_msg_type_t */
 	__u32	msg_reserved0;
 	__u32	msg_reserved1;
-	/** test session features */
-	__u32	msg_ses_feats;
+	__u32	msg_ses_feats; /* test session features */
 	union {
 		srpc_generic_reqst_t reqst;
 		srpc_generic_reply_t reply;
diff --git a/drivers/staging/lustre/lnet/selftest/selftest.h b/drivers/staging/lustre/lnet/selftest/selftest.h
index d48701834b18..7939e4e04d90 100644
--- a/drivers/staging/lustre/lnet/selftest/selftest.h
+++ b/drivers/staging/lustre/lnet/selftest/selftest.h
@@ -57,14 +57,14 @@
 #endif
 
 
-#define SWI_STATE_NEWBORN		  0
-#define SWI_STATE_REPLY_SUBMITTED	  1
-#define SWI_STATE_REPLY_SENT	       2
-#define SWI_STATE_REQUEST_SUBMITTED	3
-#define SWI_STATE_REQUEST_SENT	     4
-#define SWI_STATE_REPLY_RECEIVED	   5
-#define SWI_STATE_BULK_STARTED	     6
-#define SWI_STATE_DONE		     10
+#define SWI_STATE_NEWBORN           0
+#define SWI_STATE_REPLY_SUBMITTED   1
+#define SWI_STATE_REPLY_SENT        2
+#define SWI_STATE_REQUEST_SUBMITTED 3
+#define SWI_STATE_REQUEST_SENT      4
+#define SWI_STATE_REPLY_RECEIVED    5
+#define SWI_STATE_BULK_STARTED      6
+#define SWI_STATE_DONE             10
 
 /* forward refs */
 struct srpc_service;
@@ -75,24 +75,24 @@ struct sfw_test_instance;
 /* services below SRPC_FRAMEWORK_SERVICE_MAX_ID are framework
  * services, e.g. create/modify session.
  */
-#define SRPC_SERVICE_DEBUG	      0
-#define SRPC_SERVICE_MAKE_SESSION       1
-#define SRPC_SERVICE_REMOVE_SESSION     2
-#define SRPC_SERVICE_BATCH	      3
-#define SRPC_SERVICE_TEST	       4
-#define SRPC_SERVICE_QUERY_STAT	 5
-#define SRPC_SERVICE_JOIN	       6
-#define SRPC_FRAMEWORK_SERVICE_MAX_ID   10
+#define SRPC_SERVICE_DEBUG             0
+#define SRPC_SERVICE_MAKE_SESSION      1
+#define SRPC_SERVICE_REMOVE_SESSION    2
+#define SRPC_SERVICE_BATCH             3
+#define SRPC_SERVICE_TEST              4
+#define SRPC_SERVICE_QUERY_STAT        5
+#define SRPC_SERVICE_JOIN              6
+#define SRPC_FRAMEWORK_SERVICE_MAX_ID 10
 /* other services start from SRPC_FRAMEWORK_SERVICE_MAX_ID+1 */
-#define SRPC_SERVICE_BRW		11
-#define SRPC_SERVICE_PING	       12
-#define SRPC_SERVICE_MAX_ID	     12
+#define SRPC_SERVICE_BRW              11
+#define SRPC_SERVICE_PING             12
+#define SRPC_SERVICE_MAX_ID           12
 
-#define SRPC_REQUEST_PORTAL	     50
+#define SRPC_REQUEST_PORTAL           50
 /* a lazy portal for framework RPC requests */
-#define SRPC_FRAMEWORK_REQUEST_PORTAL   51
+#define SRPC_FRAMEWORK_REQUEST_PORTAL 51
 /* all reply/bulk RDMAs go to this portal */
-#define SRPC_RDMA_PORTAL		52
+#define SRPC_RDMA_PORTAL              52
 
 static inline srpc_msg_type_t
 srpc_service2request (int service)
@@ -136,7 +136,8 @@ srpc_service2reply (int service)
 }
 
 typedef enum {
-	SRPC_BULK_REQ_RCVD   = 1, /* passive bulk request(PUT sink/GET source) received */
+	SRPC_BULK_REQ_RCVD   = 1, /* passive bulk request(PUT sink/GET source)
+				   * received */
 	SRPC_BULK_PUT_SENT   = 2, /* active bulk PUT sent (source) */
 	SRPC_BULK_GET_RPLD   = 3, /* active bulk GET replied (sink) */
 	SRPC_REPLY_RCVD      = 4, /* incoming reply received */
@@ -149,114 +150,114 @@ typedef enum {
 typedef struct {
 	srpc_event_type_t ev_type;   /* what's up */
 	lnet_event_kind_t ev_lnet;   /* LNet event type */
-	int	       ev_fired;  /* LNet event fired? */
-	int	       ev_status; /* LNet event status */
-	void	     *ev_data;   /* owning server/client RPC */
+	int               ev_fired;  /* LNet event fired? */
+	int               ev_status; /* LNet event status */
+	void              *ev_data;  /* owning server/client RPC */
 } srpc_event_t;
 
 typedef struct {
-	int	      bk_len;  /* len of bulk data */
+	int              bk_len;     /* len of bulk data */
 	lnet_handle_md_t bk_mdh;
-	int	      bk_sink; /* sink/source */
-	int	      bk_niov; /* # iov in bk_iovs */
+	int              bk_sink;    /* sink/source */
+	int              bk_niov;    /* # iov in bk_iovs */
 	lnet_kiov_t      bk_iovs[0];
 } srpc_bulk_t; /* bulk descriptor */
 
 /* message buffer descriptor */
 typedef struct srpc_buffer {
-	struct list_head	   buf_list; /* chain on srpc_service::*_msgq */
-	srpc_msg_t	   buf_msg;
-	lnet_handle_md_t     buf_mdh;
-	lnet_nid_t	   buf_self;
-	lnet_process_id_t    buf_peer;
+	struct list_head  buf_list; /* chain on srpc_service::*_msgq */
+	srpc_msg_t        buf_msg;
+	lnet_handle_md_t  buf_mdh;
+	lnet_nid_t        buf_self;
+	lnet_process_id_t buf_peer;
 } srpc_buffer_t;
 
 struct swi_workitem;
 typedef int (*swi_action_t) (struct swi_workitem *);
 
 typedef struct swi_workitem {
-	struct cfs_wi_sched	*swi_sched;
-	cfs_workitem_t       swi_workitem;
-	swi_action_t	 swi_action;
-	int		  swi_state;
+	struct cfs_wi_sched *swi_sched;
+	cfs_workitem_t      swi_workitem;
+	swi_action_t        swi_action;
+	int                 swi_state;
 } swi_workitem_t;
 
 /* server-side state of a RPC */
 typedef struct srpc_server_rpc {
 	/* chain on srpc_service::*_rpcq */
-	struct list_head		srpc_list;
+	struct list_head       srpc_list;
 	struct srpc_service_cd *srpc_scd;
-	swi_workitem_t       srpc_wi;
-	srpc_event_t	 srpc_ev;      /* bulk/reply event */
-	lnet_nid_t	   srpc_self;
-	lnet_process_id_t    srpc_peer;
-	srpc_msg_t	   srpc_replymsg;
-	lnet_handle_md_t     srpc_replymdh;
-	srpc_buffer_t       *srpc_reqstbuf;
-	srpc_bulk_t	 *srpc_bulk;
-
-	unsigned int	 srpc_aborted; /* being given up */
-	int		  srpc_status;
-	void	       (*srpc_done)(struct srpc_server_rpc *);
+	swi_workitem_t         srpc_wi;
+	srpc_event_t           srpc_ev;      /* bulk/reply event */
+	lnet_nid_t             srpc_self;
+	lnet_process_id_t      srpc_peer;
+	srpc_msg_t             srpc_replymsg;
+	lnet_handle_md_t       srpc_replymdh;
+	srpc_buffer_t          *srpc_reqstbuf;
+	srpc_bulk_t            *srpc_bulk;
+
+	unsigned int           srpc_aborted; /* being given up */
+	int                    srpc_status;
+	void                   (*srpc_done)(struct srpc_server_rpc *);
 } srpc_server_rpc_t;
 
 /* client-side state of a RPC */
 typedef struct srpc_client_rpc {
-	struct list_head		crpc_list;	/* chain on user's lists */
-	spinlock_t		crpc_lock;	/* serialize */
-	int		  crpc_service;
-	atomic_t	 crpc_refcount;
-	int		  crpc_timeout; /* # seconds to wait for reply */
-	stt_timer_t	  crpc_timer;
-	swi_workitem_t       crpc_wi;
-	lnet_process_id_t    crpc_dest;
-
-	void	       (*crpc_done)(struct srpc_client_rpc *);
-	void	       (*crpc_fini)(struct srpc_client_rpc *);
-	int		  crpc_status;    /* completion status */
-	void		*crpc_priv;      /* caller data */
+	struct list_head  crpc_list;      /* chain on user's lists */
+	spinlock_t        crpc_lock;      /* serialize */
+	int               crpc_service;
+	atomic_t          crpc_refcount;
+	int               crpc_timeout;   /* # seconds to wait for reply */
+	stt_timer_t       crpc_timer;
+	swi_workitem_t    crpc_wi;
+	lnet_process_id_t crpc_dest;
+
+	void              (*crpc_done)(struct srpc_client_rpc *);
+	void              (*crpc_fini)(struct srpc_client_rpc *);
+	int               crpc_status;    /* completion status */
+	void              *crpc_priv;     /* caller data */
 
 	/* state flags */
-	unsigned int	 crpc_aborted:1; /* being given up */
-	unsigned int	 crpc_closed:1;  /* completed */
+	unsigned int      crpc_aborted:1; /* being given up */
+	unsigned int      crpc_closed:1;  /* completed */
 
 	/* RPC events */
-	srpc_event_t	 crpc_bulkev;    /* bulk event */
-	srpc_event_t	 crpc_reqstev;   /* request event */
-	srpc_event_t	 crpc_replyev;   /* reply event */
+	srpc_event_t      crpc_bulkev;    /* bulk event */
+	srpc_event_t      crpc_reqstev;   /* request event */
+	srpc_event_t      crpc_replyev;   /* reply event */
 
 	/* bulk, request(reqst), and reply exchanged on wire */
-	srpc_msg_t	   crpc_reqstmsg;
-	srpc_msg_t	   crpc_replymsg;
-	lnet_handle_md_t     crpc_reqstmdh;
-	lnet_handle_md_t     crpc_replymdh;
-	srpc_bulk_t	  crpc_bulk;
+	srpc_msg_t        crpc_reqstmsg;
+	srpc_msg_t        crpc_replymsg;
+	lnet_handle_md_t  crpc_reqstmdh;
+	lnet_handle_md_t  crpc_replymdh;
+	srpc_bulk_t       crpc_bulk;
 } srpc_client_rpc_t;
 
-#define srpc_client_rpc_size(rpc)				       \
+#define srpc_client_rpc_size(rpc)                                        \
 offsetof(srpc_client_rpc_t, crpc_bulk.bk_iovs[(rpc)-&gt;crpc_bulk.bk_niov])
 
-#define srpc_client_rpc_addref(rpc)				     \
-do {								    \
-	CDEBUG(D_NET, "RPC[%p] -&gt; %s (%d)++\n",			 \
-	       (rpc), libcfs_id2str((rpc)-&gt;crpc_dest),		  \
-	       atomic_read(&amp;(rpc)-&gt;crpc_refcount));		 \
-	LASSERT(atomic_read(&amp;(rpc)-&gt;crpc_refcount) &gt; 0);	    \
-	atomic_inc(&amp;(rpc)-&gt;crpc_refcount);			  \
+#define srpc_client_rpc_addref(rpc)                                      \
+do {                                                                     \
+	CDEBUG(D_NET, "RPC[%p] -&gt; %s (%d)++\n",                          \
+	       (rpc), libcfs_id2str((rpc)-&gt;crpc_dest),                   \
+	       atomic_read(&amp;(rpc)-&gt;crpc_refcount));                      \
+	LASSERT(atomic_read(&amp;(rpc)-&gt;crpc_refcount) &gt; 0);                 \
+	atomic_inc(&amp;(rpc)-&gt;crpc_refcount);                               \
 } while (0)
 
-#define srpc_client_rpc_decref(rpc)				     \
-do {								    \
-	CDEBUG(D_NET, "RPC[%p] -&gt; %s (%d)--\n",			 \
-	       (rpc), libcfs_id2str((rpc)-&gt;crpc_dest),		  \
-	       atomic_read(&amp;(rpc)-&gt;crpc_refcount));		 \
-	LASSERT(atomic_read(&amp;(rpc)-&gt;crpc_refcount) &gt; 0);	    \
-	if (atomic_dec_and_test(&amp;(rpc)-&gt;crpc_refcount))	     \
-		srpc_destroy_client_rpc(rpc);			   \
+#define srpc_client_rpc_decref(rpc)                                      \
+do {                                                                     \
+	CDEBUG(D_NET, "RPC[%p] -&gt; %s (%d)--\n",                          \
+	       (rpc), libcfs_id2str((rpc)-&gt;crpc_dest),                   \
+	       atomic_read(&amp;(rpc)-&gt;crpc_refcount));                      \
+	LASSERT(atomic_read(&amp;(rpc)-&gt;crpc_refcount) &gt; 0);                 \
+	if (atomic_dec_and_test(&amp;(rpc)-&gt;crpc_refcount))                  \
+		srpc_destroy_client_rpc(rpc);                            \
 } while (0)
 
-#define srpc_event_pending(rpc)   ((rpc)-&gt;crpc_bulkev.ev_fired == 0 ||  \
-				   (rpc)-&gt;crpc_reqstev.ev_fired == 0 || \
+#define srpc_event_pending(rpc)   ((rpc)-&gt;crpc_bulkev.ev_fired == 0 ||   \
+				   (rpc)-&gt;crpc_reqstev.ev_fired == 0 ||  \
 				   (rpc)-&gt;crpc_replyev.ev_fired == 0)
 
 /* CPU partition data of srpc service */
@@ -268,9 +269,9 @@ struct srpc_service_cd {
 	/** event buffer */
 	srpc_event_t		scd_ev;
 	/** free RPC descriptors */
-	struct list_head		scd_rpc_free;
+	struct list_head        scd_rpc_free;
 	/** in-flight RPCs */
-	struct list_head		scd_rpc_active;
+	struct list_head        scd_rpc_active;
 	/** workitem for posting buffer */
 	swi_workitem_t		scd_buf_wi;
 	/** CPT id */
@@ -278,7 +279,7 @@ struct srpc_service_cd {
 	/** error code for scd_buf_wi */
 	int			scd_buf_err;
 	/** timestamp for scd_buf_err */
-	unsigned long	   scd_buf_err_stamp;
+	unsigned long           scd_buf_err_stamp;
 	/** total # request buffers */
 	int			scd_buf_total;
 	/** # posted request buffers */
@@ -290,9 +291,9 @@ struct srpc_service_cd {
 	/** increase/decrease some buffers */
 	int			scd_buf_adjust;
 	/** posted message buffers */
-	struct list_head		scd_buf_posted;
+	struct list_head        scd_buf_posted;
 	/** blocked for RPC descriptor */
-	struct list_head		scd_buf_blocked;
+	struct list_head        scd_buf_blocked;
 };
 
 /* number of server workitems (mini-thread) for testing service */
@@ -318,40 +319,42 @@ typedef struct srpc_service {
 	 * - sv_handler: process incoming RPC request
 	 * - sv_bulk_ready: notify bulk data
 	 */
-	int	      (*sv_handler) (srpc_server_rpc_t *);
-	int	      (*sv_bulk_ready) (srpc_server_rpc_t *, int);
+	int                     (*sv_handler) (srpc_server_rpc_t *);
+	int                     (*sv_bulk_ready) (srpc_server_rpc_t *, int);
 } srpc_service_t;
 
 typedef struct {
-	struct list_head	sn_list;    /* chain on fw_zombie_sessions */
-	lst_sid_t	 sn_id;      /* unique identifier */
-	unsigned int      sn_timeout; /* # seconds' inactivity to expire */
-	int	       sn_timer_active;
-	unsigned int	  sn_features;
-	stt_timer_t       sn_timer;
-	struct list_head	sn_batches; /* list of batches */
-	char	      sn_name[LST_NAME_SIZE];
-	atomic_t      sn_refcount;
-	atomic_t      sn_brw_errors;
-	atomic_t      sn_ping_errors;
-	unsigned long	sn_started;
+	struct list_head sn_list;    /* chain on fw_zombie_sessions */
+	lst_sid_t        sn_id;      /* unique identifier */
+	unsigned int     sn_timeout; /* # seconds' inactivity to expire */
+	int              sn_timer_active;
+	unsigned int     sn_features;
+	stt_timer_t      sn_timer;
+	struct list_head sn_batches; /* list of batches */
+	char             sn_name[LST_NAME_SIZE];
+	atomic_t         sn_refcount;
+	atomic_t         sn_brw_errors;
+	atomic_t         sn_ping_errors;
+	unsigned long    sn_started;
 } sfw_session_t;
 
 #define sfw_sid_equal(sid0, sid1)     ((sid0).ses_nid == (sid1).ses_nid &amp;&amp; \
 				       (sid0).ses_stamp == (sid1).ses_stamp)
 
 typedef struct {
-	struct list_head	bat_list;      /* chain on sn_batches */
-	lst_bid_t	 bat_id;	/* batch id */
-	int	       bat_error;     /* error code of batch */
-	sfw_session_t    *bat_session;   /* batch's session */
-	atomic_t      bat_nactive;   /* # of active tests */
-	struct list_head	bat_tests;     /* test instances */
+	struct list_head bat_list;      /* chain on sn_batches */
+	lst_bid_t        bat_id;	/* batch id */
+	int              bat_error;     /* error code of batch */
+	sfw_session_t    *bat_session;  /* batch's session */
+	atomic_t         bat_nactive;   /* # of active tests */
+	struct list_head bat_tests;     /* test instances */
 } sfw_batch_t;
 
 typedef struct {
-	int  (*tso_init)(struct sfw_test_instance *tsi); /* initialize test client */
-	void (*tso_fini)(struct sfw_test_instance *tsi); /* finalize test client */
+	int  (*tso_init)(struct sfw_test_instance *tsi); /* initialize test
+							  * client */
+	void (*tso_fini)(struct sfw_test_instance *tsi); /* finalize test
+							  * client */
 	int  (*tso_prep_rpc)(struct sfw_test_unit *tsu,
 			     lnet_process_id_t dest,
 			     srpc_client_rpc_t **rpc);   /* prep a tests rpc */
@@ -360,29 +363,31 @@ typedef struct {
 } sfw_test_client_ops_t;
 
 typedef struct sfw_test_instance {
-	struct list_head	      tsi_list;	 /* chain on batch */
-	int		     tsi_service;      /* test type */
-	sfw_batch_t	    *tsi_batch;	/* batch */
-	sfw_test_client_ops_t  *tsi_ops;	  /* test client operations */
+	struct list_head           tsi_list;            /* chain on batch */
+	int                        tsi_service;         /* test type */
+	sfw_batch_t                *tsi_batch;          /* batch */
+	sfw_test_client_ops_t      *tsi_ops;            /* test client operation
+							 */
 
 	/* public parameter for all test units */
-	unsigned int		tsi_is_client:1;     /* is test client */
-	unsigned int		tsi_stoptsu_onerr:1; /* stop tsu on error */
-	int		     tsi_concur;	  /* concurrency */
-	int		     tsi_loop;	    /* loop count */
+	unsigned int               tsi_is_client:1;     /* is test client */
+	unsigned int               tsi_stoptsu_onerr:1; /* stop tsu on error */
+	int                        tsi_concur;          /* concurrency */
+	int                        tsi_loop;            /* loop count */
 
 	/* status of test instance */
-	spinlock_t		tsi_lock;	  /* serialize */
-	unsigned int		tsi_stopping:1;   /* test is stopping */
-	atomic_t	    tsi_nactive;      /* # of active test unit */
-	struct list_head	      tsi_units;	/* test units */
-	struct list_head	      tsi_free_rpcs;    /* free rpcs */
-	struct list_head	      tsi_active_rpcs;  /* active rpcs */
+	spinlock_t                 tsi_lock;            /* serialize */
+	unsigned int               tsi_stopping:1;      /* test is stopping */
+	atomic_t                   tsi_nactive;         /* # of active test
+							 * unit */
+	struct list_head           tsi_units;           /* test units */
+	struct list_head           tsi_free_rpcs;       /* free rpcs */
+	struct list_head           tsi_active_rpcs;     /* active rpcs */
 
 	union {
-		test_ping_req_t		ping;	  /* ping parameter */
-		test_bulk_req_t		bulk_v0;  /* bulk parameter */
-		test_bulk_req_v1_t	bulk_v1;  /* bulk v1 parameter */
+		test_ping_req_t	   ping;    /* ping parameter */
+		test_bulk_req_t    bulk_v0; /* bulk parameter */
+		test_bulk_req_v1_t bulk_v1; /* bulk v1 parameter */
 	} tsi_u;
 } sfw_test_instance_t;
 
@@ -394,18 +399,18 @@ typedef struct sfw_test_instance {
 #define sfw_id_pages(n)    (((n) + SFW_ID_PER_PAGE - 1) / SFW_ID_PER_PAGE)
 
 typedef struct sfw_test_unit {
-	struct list_head	    tsu_list;	 /* chain on lst_test_instance */
-	lnet_process_id_t     tsu_dest;	 /* id of dest node */
-	int		   tsu_loop;	 /* loop count of the test */
-	sfw_test_instance_t  *tsu_instance;     /* pointer to test instance */
-	void		 *tsu_private;      /* private data */
-	swi_workitem_t	tsu_worker;       /* workitem of the test unit */
+	struct list_head    tsu_list;      /* chain on lst_test_instance */
+	lnet_process_id_t   tsu_dest;      /* id of dest node */
+	int                 tsu_loop;      /* loop count of the test */
+	sfw_test_instance_t *tsu_instance; /* pointer to test instance */
+	void                *tsu_private;  /* private data */
+	swi_workitem_t      tsu_worker;    /* workitem of the test unit */
 } sfw_test_unit_t;
 
 typedef struct sfw_test_case {
-	struct list_head	      tsc_list;	 /* chain on fw_tests */
-	srpc_service_t	 *tsc_srv_service;  /* test service */
-	sfw_test_client_ops_t  *tsc_cli_ops;      /* ops of test client */
+	struct list_head      tsc_list;         /* chain on fw_tests */
+	srpc_service_t        *tsc_srv_service; /* test service */
+	sfw_test_client_ops_t *tsc_cli_ops;     /* ops of test client */
 } sfw_test_case_t;
 
 srpc_client_rpc_t *
@@ -501,9 +506,9 @@ void srpc_shutdown(void);
 static inline void
 srpc_destroy_client_rpc (srpc_client_rpc_t *rpc)
 {
-	LASSERT (rpc != NULL);
-	LASSERT (!srpc_event_pending(rpc));
-	LASSERT (atomic_read(&amp;rpc-&gt;crpc_refcount) == 0);
+	LASSERT(rpc != NULL);
+	LASSERT(!srpc_event_pending(rpc));
+	LASSERT(atomic_read(&amp;rpc-&gt;crpc_refcount) == 0);
 
 	if (rpc-&gt;crpc_fini == NULL) {
 		LIBCFS_FREE(rpc, srpc_client_rpc_size(rpc));
@@ -520,7 +525,7 @@ srpc_init_client_rpc (srpc_client_rpc_t *rpc, lnet_process_id_t peer,
 		      void (*rpc_done)(srpc_client_rpc_t *),
 		      void (*rpc_fini)(srpc_client_rpc_t *), void *priv)
 {
-	LASSERT (nbulkiov &lt;= LNET_MAX_IOV);
+	LASSERT(nbulkiov &lt;= LNET_MAX_IOV);
 
 	memset(rpc, 0, offsetof(srpc_client_rpc_t,
 				crpc_bulk.bk_iovs[nbulkiov]));
@@ -531,13 +536,13 @@ srpc_init_client_rpc (srpc_client_rpc_t *rpc, lnet_process_id_t peer,
 	spin_lock_init(&amp;rpc-&gt;crpc_lock);
 	atomic_set(&amp;rpc-&gt;crpc_refcount, 1); /* 1 ref for caller */
 
-	rpc-&gt;crpc_dest	 = peer;
-	rpc-&gt;crpc_priv	 = priv;
+	rpc-&gt;crpc_dest         = peer;
+	rpc-&gt;crpc_priv         = priv;
 	rpc-&gt;crpc_service      = service;
 	rpc-&gt;crpc_bulk.bk_len  = bulklen;
 	rpc-&gt;crpc_bulk.bk_niov = nbulkiov;
-	rpc-&gt;crpc_done	 = rpc_done;
-	rpc-&gt;crpc_fini	 = rpc_fini;
+	rpc-&gt;crpc_done         = rpc_done;
+	rpc-&gt;crpc_fini         = rpc_fini;
 	LNetInvalidateHandle(&amp;rpc-&gt;crpc_reqstmdh);
 	LNetInvalidateHandle(&amp;rpc-&gt;crpc_replymdh);
 	LNetInvalidateHandle(&amp;rpc-&gt;crpc_bulk.bk_mdh);
diff --git a/drivers/staging/lustre/lnet/selftest/timer.c b/drivers/staging/lustre/lnet/selftest/timer.c
index 441f9472a834..6133b54f4a82 100644
--- a/drivers/staging/lustre/lnet/selftest/timer.c
+++ b/drivers/staging/lustre/lnet/selftest/timer.c
@@ -50,7 +50,7 @@
  * sorted by increasing expiry time. The number of slots is 2**7 (128),
  * to cover a time period of 1024 seconds into the future before wrapping.
  */
-#define STTIMER_MINPOLL	3   /* log2 min poll interval (8 s) */
+#define STTIMER_MINPOLL        3   /* log2 min poll interval (8 s) */
 #define STTIMER_SLOTTIME       (1 &lt;&lt; STTIMER_MINPOLL)
 #define STTIMER_SLOTTIMEMASK   (~(STTIMER_SLOTTIME - 1))
 #define STTIMER_NSLOTS	       (1 &lt;&lt; 7)
@@ -58,13 +58,13 @@
 						    (STTIMER_NSLOTS - 1))])
 
 static struct st_timer_data {
-	spinlock_t	 stt_lock;
-	/* start time of the slot processed previously */
-	unsigned long       stt_prev_slot;
-	struct list_head       stt_hash[STTIMER_NSLOTS];
-	int	      stt_shuttingdown;
-	wait_queue_head_t      stt_waitq;
-	int	      stt_nthreads;
+	spinlock_t        stt_lock;
+	unsigned long     stt_prev_slot; /* start time of the slot processed
+					  * previously */
+	struct list_head  stt_hash[STTIMER_NSLOTS];
+	int               stt_shuttingdown;
+	wait_queue_head_t stt_waitq;
+	int               stt_nthreads;
 } stt_data;
 
 void
@@ -124,7 +124,7 @@ stt_del_timer(stt_timer_t *timer)
 static int
 stt_expire_list(struct list_head *slot, unsigned long now)
 {
-	int	  expired = 0;
+	int expired = 0;
 	stt_timer_t *timer;
 
 	while (!list_empty(slot)) {
@@ -148,7 +148,7 @@ stt_expire_list(struct list_head *slot, unsigned long now)
 static int
 stt_check_timers(unsigned long *last)
 {
-	int	expired = 0;
+	int expired = 0;
 	unsigned long now;
 	unsigned long this_slot;
 
diff --git a/drivers/staging/lustre/lnet/selftest/timer.h b/drivers/staging/lustre/lnet/selftest/timer.h
index d727c1e2b0ce..2a8803d89de4 100644
--- a/drivers/staging/lustre/lnet/selftest/timer.h
+++ b/drivers/staging/lustre/lnet/selftest/timer.h
@@ -39,15 +39,15 @@
 #define __SELFTEST_TIMER_H__
 
 typedef struct {
-	struct list_head	stt_list;
-	unsigned long	stt_expires;
-	void	    (*stt_func) (void *);
-	void	     *stt_data;
+	struct list_head stt_list;
+	unsigned long    stt_expires;
+	void             (*stt_func) (void *);
+	void             *stt_data;
 } stt_timer_t;
 
-void stt_add_timer (stt_timer_t *timer);
-int stt_del_timer (stt_timer_t *timer);
-int stt_startup (void);
-void stt_shutdown (void);
+void stt_add_timer(stt_timer_t *timer);
+int stt_del_timer(stt_timer_t *timer);
+int stt_startup(void);
+void stt_shutdown(void);
 
 #endif /* __SELFTEST_TIMER_H__ */</pre><hr><pre>commit 97d10d0a40dddd317f1c659ec42544b4793a140e
Author: Mike Shuey &lt;shuey@purdue.edu&gt;
Date:   Tue May 19 10:14:37 2015 -0400

    staging: lustre: lnet: socklnd: code cleanup - align spacing
    
    Unify variable declarations to use a single space.  Also include several
    miscellaneous whitespace cleanups, particularly in socklnd.h.
    
    Signed-off-by: Mike Shuey &lt;shuey@purdue.edu&gt;
    Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;

diff --git a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd.c b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd.c
index 7586b7e4040b..7b5d4078ba4e 100644
--- a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd.c
+++ b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd.c
@@ -49,8 +49,8 @@ ksock_nal_data_t ksocknal_data;
 static ksock_interface_t *
 ksocknal_ip2iface(lnet_ni_t *ni, __u32 ip)
 {
-	ksock_net_t       *net = ni-&gt;ni_data;
-	int		i;
+	ksock_net_t *net = ni-&gt;ni_data;
+	int i;
 	ksock_interface_t *iface;
 
 	for (i = 0; i &lt; net-&gt;ksnn_ninterfaces; i++) {
@@ -102,8 +102,8 @@ ksocknal_destroy_route(ksock_route_t *route)
 static int
 ksocknal_create_peer(ksock_peer_t **peerp, lnet_ni_t *ni, lnet_process_id_t id)
 {
-	ksock_net_t   *net = ni-&gt;ni_data;
-	ksock_peer_t  *peer;
+	ksock_net_t *net = ni-&gt;ni_data;
+	ksock_peer_t *peer;
 
 	LASSERT(id.nid != LNET_NID_ANY);
 	LASSERT(id.pid != LNET_PID_ANY);
@@ -149,7 +149,7 @@ ksocknal_create_peer(ksock_peer_t **peerp, lnet_ni_t *ni, lnet_process_id_t id)
 void
 ksocknal_destroy_peer(ksock_peer_t *peer)
 {
-	ksock_net_t    *net = peer-&gt;ksnp_ni-&gt;ni_data;
+	ksock_net_t *net = peer-&gt;ksnp_ni-&gt;ni_data;
 
 	CDEBUG(D_NET, "peer %s %p deleted\n",
 		libcfs_id2str(peer-&gt;ksnp_id), peer);
@@ -175,9 +175,9 @@ ksocknal_destroy_peer(ksock_peer_t *peer)
 ksock_peer_t *
 ksocknal_find_peer_locked(lnet_ni_t *ni, lnet_process_id_t id)
 {
-	struct list_head       *peer_list = ksocknal_nid2peerlist(id.nid);
-	struct list_head       *tmp;
-	ksock_peer_t     *peer;
+	struct list_head *peer_list = ksocknal_nid2peerlist(id.nid);
+	struct list_head *tmp;
+	ksock_peer_t *peer;
 
 	list_for_each(tmp, peer_list) {
 
@@ -203,7 +203,7 @@ ksocknal_find_peer_locked(lnet_ni_t *ni, lnet_process_id_t id)
 ksock_peer_t *
 ksocknal_find_peer(lnet_ni_t *ni, lnet_process_id_t id)
 {
-	ksock_peer_t     *peer;
+	ksock_peer_t *peer;
 
 	read_lock(&amp;ksocknal_data.ksnd_global_lock);
 	peer = ksocknal_find_peer_locked(ni, id);
@@ -217,8 +217,8 @@ ksocknal_find_peer(lnet_ni_t *ni, lnet_process_id_t id)
 static void
 ksocknal_unlink_peer_locked(ksock_peer_t *peer)
 {
-	int		i;
-	__u32	      ip;
+	int i;
+	__u32 ip;
 	ksock_interface_t *iface;
 
 	for (i = 0; i &lt; peer-&gt;ksnp_n_passive_ips; i++) {
@@ -249,13 +249,13 @@ ksocknal_get_peer_info(lnet_ni_t *ni, int index,
 			lnet_process_id_t *id, __u32 *myip, __u32 *peer_ip,
 			int *port, int *conn_count, int *share_count)
 {
-	ksock_peer_t      *peer;
-	struct list_head	*ptmp;
-	ksock_route_t     *route;
-	struct list_head	*rtmp;
-	int		i;
-	int		j;
-	int		rc = -ENOENT;
+	ksock_peer_t *peer;
+	struct list_head *ptmp;
+	ksock_route_t *route;
+	struct list_head *rtmp;
+	int i;
+	int j;
+	int rc = -ENOENT;
 
 	read_lock(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -322,8 +322,8 @@ ksocknal_get_peer_info(lnet_ni_t *ni, int index,
 static void
 ksocknal_associate_route_conn_locked(ksock_route_t *route, ksock_conn_t *conn)
 {
-	ksock_peer_t      *peer = route-&gt;ksnr_peer;
-	int		type = conn-&gt;ksnc_type;
+	ksock_peer_t *peer = route-&gt;ksnr_peer;
+	int type = conn-&gt;ksnc_type;
 	ksock_interface_t *iface;
 
 	conn-&gt;ksnc_route = route;
@@ -366,9 +366,9 @@ ksocknal_associate_route_conn_locked(ksock_route_t *route, ksock_conn_t *conn)
 static void
 ksocknal_add_route_locked(ksock_peer_t *peer, ksock_route_t *route)
 {
-	struct list_head	*tmp;
-	ksock_conn_t      *conn;
-	ksock_route_t     *route2;
+	struct list_head *tmp;
+	ksock_conn_t *conn;
+	ksock_route_t *route2;
 
 	LASSERT(!peer-&gt;ksnp_closing);
 	LASSERT(route-&gt;ksnr_peer == NULL);
@@ -407,11 +407,11 @@ ksocknal_add_route_locked(ksock_peer_t *peer, ksock_route_t *route)
 static void
 ksocknal_del_route_locked(ksock_route_t *route)
 {
-	ksock_peer_t      *peer = route-&gt;ksnr_peer;
+	ksock_peer_t *peer = route-&gt;ksnr_peer;
 	ksock_interface_t *iface;
-	ksock_conn_t      *conn;
-	struct list_head	*ctmp;
-	struct list_head	*cnxt;
+	ksock_conn_t *conn;
+	struct list_head *ctmp;
+	struct list_head *cnxt;
 
 	LASSERT(!route-&gt;ksnr_deleted);
 
@@ -447,12 +447,12 @@ ksocknal_del_route_locked(ksock_route_t *route)
 int
 ksocknal_add_peer(lnet_ni_t *ni, lnet_process_id_t id, __u32 ipaddr, int port)
 {
-	struct list_head	*tmp;
-	ksock_peer_t      *peer;
-	ksock_peer_t      *peer2;
-	ksock_route_t     *route;
-	ksock_route_t     *route2;
-	int		rc;
+	struct list_head *tmp;
+	ksock_peer_t *peer;
+	ksock_peer_t *peer2;
+	ksock_route_t *route;
+	ksock_route_t *route2;
+	int rc;
 
 	if (id.nid == LNET_NID_ANY ||
 	    id.pid == LNET_PID_ANY)
@@ -509,11 +509,11 @@ ksocknal_add_peer(lnet_ni_t *ni, lnet_process_id_t id, __u32 ipaddr, int port)
 static void
 ksocknal_del_peer_locked(ksock_peer_t *peer, __u32 ip)
 {
-	ksock_conn_t     *conn;
-	ksock_route_t    *route;
-	struct list_head       *tmp;
-	struct list_head       *nxt;
-	int	       nshared;
+	ksock_conn_t *conn;
+	ksock_route_t *route;
+	struct list_head *tmp;
+	struct list_head *nxt;
+	int nshared;
 
 	LASSERT(!peer-&gt;ksnp_closing);
 
@@ -565,13 +565,13 @@ static int
 ksocknal_del_peer(lnet_ni_t *ni, lnet_process_id_t id, __u32 ip)
 {
 	LIST_HEAD(zombies);
-	struct list_head	*ptmp;
-	struct list_head	*pnxt;
-	ksock_peer_t      *peer;
-	int		lo;
-	int		hi;
-	int		i;
-	int		rc = -ENOENT;
+	struct list_head *ptmp;
+	struct list_head *pnxt;
+	ksock_peer_t *peer;
+	int lo;
+	int hi;
+	int i;
+	int rc = -ENOENT;
 
 	write_lock_bh(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -623,11 +623,11 @@ ksocknal_del_peer(lnet_ni_t *ni, lnet_process_id_t id, __u32 ip)
 static ksock_conn_t *
 ksocknal_get_conn_by_idx(lnet_ni_t *ni, int index)
 {
-	ksock_peer_t      *peer;
-	struct list_head	*ptmp;
-	ksock_conn_t      *conn;
-	struct list_head	*ctmp;
-	int		i;
+	ksock_peer_t *peer;
+	struct list_head *ptmp;
+	ksock_conn_t *conn;
+	struct list_head *ctmp;
+	int i;
 
 	read_lock(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -661,8 +661,8 @@ static ksock_sched_t *
 ksocknal_choose_scheduler_locked(unsigned int cpt)
 {
 	struct ksock_sched_info	*info = ksocknal_data.ksnd_sched_info[cpt];
-	ksock_sched_t		*sched;
-	int			i;
+	ksock_sched_t *sched;
+	int i;
 
 	LASSERT(info-&gt;ksi_nthreads &gt; 0);
 
@@ -683,9 +683,9 @@ ksocknal_choose_scheduler_locked(unsigned int cpt)
 static int
 ksocknal_local_ipvec(lnet_ni_t *ni, __u32 *ipaddrs)
 {
-	ksock_net_t       *net = ni-&gt;ni_data;
-	int		i;
-	int		nip;
+	ksock_net_t *net = ni-&gt;ni_data;
+	int i;
+	int nip;
 
 	read_lock(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -711,12 +711,12 @@ ksocknal_local_ipvec(lnet_ni_t *ni, __u32 *ipaddrs)
 static int
 ksocknal_match_peerip(ksock_interface_t *iface, __u32 *ips, int nips)
 {
-	int   best_netmatch = 0;
-	int   best_xor      = 0;
-	int   best	  = -1;
-	int   this_xor;
-	int   this_netmatch;
-	int   i;
+	int best_netmatch = 0;
+	int best_xor      = 0;
+	int best	  = -1;
+	int this_xor;
+	int this_netmatch;
+	int i;
 
 	for (i = 0; i &lt; nips; i++) {
 		if (ips[i] == 0)
@@ -743,19 +743,19 @@ ksocknal_match_peerip(ksock_interface_t *iface, __u32 *ips, int nips)
 static int
 ksocknal_select_ips(ksock_peer_t *peer, __u32 *peerips, int n_peerips)
 {
-	rwlock_t		*global_lock = &amp;ksocknal_data.ksnd_global_lock;
-	ksock_net_t	*net = peer-&gt;ksnp_ni-&gt;ni_data;
-	ksock_interface_t  *iface;
-	ksock_interface_t  *best_iface;
-	int		 n_ips;
-	int		 i;
-	int		 j;
-	int		 k;
-	__u32	       ip;
-	__u32	       xor;
-	int		 this_netmatch;
-	int		 best_netmatch;
-	int		 best_npeers;
+	rwlock_t *global_lock = &amp;ksocknal_data.ksnd_global_lock;
+	ksock_net_t *net = peer-&gt;ksnp_ni-&gt;ni_data;
+	ksock_interface_t *iface;
+	ksock_interface_t *best_iface;
+	int n_ips;
+	int i;
+	int j;
+	int k;
+	__u32 ip;
+	__u32 xor;
+	int this_netmatch;
+	int best_netmatch;
+	int best_npeers;
 
 	/* CAVEAT EMPTOR: We do all our interface matching with an
 	 * exclusive hold of global lock at IRQ priority.  We're only
@@ -846,19 +846,19 @@ static void
 ksocknal_create_routes(ksock_peer_t *peer, int port,
 		       __u32 *peer_ipaddrs, int npeer_ipaddrs)
 {
-	ksock_route_t       *newroute = NULL;
-	rwlock_t		*global_lock = &amp;ksocknal_data.ksnd_global_lock;
-	lnet_ni_t	   *ni = peer-&gt;ksnp_ni;
-	ksock_net_t	 *net = ni-&gt;ni_data;
-	struct list_head	  *rtmp;
-	ksock_route_t       *route;
-	ksock_interface_t   *iface;
-	ksock_interface_t   *best_iface;
-	int		  best_netmatch;
-	int		  this_netmatch;
-	int		  best_nroutes;
-	int		  i;
-	int		  j;
+	ksock_route_t *newroute = NULL;
+	rwlock_t *global_lock = &amp;ksocknal_data.ksnd_global_lock;
+	lnet_ni_t *ni = peer-&gt;ksnp_ni;
+	ksock_net_t *net = ni-&gt;ni_data;
+	struct list_head *rtmp;
+	ksock_route_t *route;
+	ksock_interface_t *iface;
+	ksock_interface_t *best_iface;
+	int best_netmatch;
+	int this_netmatch;
+	int best_nroutes;
+	int i;
+	int j;
 
 	/* CAVEAT EMPTOR: We do all our interface matching with an
 	 * exclusive hold of global lock at IRQ priority.  We're only
@@ -963,10 +963,10 @@ ksocknal_create_routes(ksock_peer_t *peer, int port,
 int
 ksocknal_accept(lnet_ni_t *ni, struct socket *sock)
 {
-	ksock_connreq_t    *cr;
-	int		 rc;
-	__u32	       peer_ip;
-	int		 peer_port;
+	ksock_connreq_t *cr;
+	int rc;
+	__u32 peer_ip;
+	int peer_port;
 
 	rc = libcfs_sock_getaddr(sock, 1, &amp;peer_ip, &amp;peer_port);
 	LASSERT(rc == 0);		      /* we succeeded before */
@@ -994,7 +994,7 @@ ksocknal_accept(lnet_ni_t *ni, struct socket *sock)
 static int
 ksocknal_connecting(ksock_peer_t *peer, __u32 ipaddr)
 {
-	ksock_route_t   *route;
+	ksock_route_t *route;
 
 	list_for_each_entry(route, &amp;peer-&gt;ksnp_routes, ksnr_list) {
 
@@ -1008,23 +1008,23 @@ int
 ksocknal_create_conn(lnet_ni_t *ni, ksock_route_t *route,
 		      struct socket *sock, int type)
 {
-	rwlock_t		*global_lock = &amp;ksocknal_data.ksnd_global_lock;
+	rwlock_t *global_lock = &amp;ksocknal_data.ksnd_global_lock;
 	LIST_HEAD(zombies);
-	lnet_process_id_t  peerid;
-	struct list_head	*tmp;
-	__u64	      incarnation;
-	ksock_conn_t      *conn;
-	ksock_conn_t      *conn2;
-	ksock_peer_t      *peer = NULL;
-	ksock_peer_t      *peer2;
-	ksock_sched_t     *sched;
+	lnet_process_id_t peerid;
+	struct list_head *tmp;
+	__u64 incarnation;
+	ksock_conn_t *conn;
+	ksock_conn_t *conn2;
+	ksock_peer_t *peer = NULL;
+	ksock_peer_t *peer2;
+	ksock_sched_t *sched;
 	ksock_hello_msg_t *hello;
-	int		   cpt;
-	ksock_tx_t	*tx;
-	ksock_tx_t	*txtmp;
-	int		rc;
-	int		active;
-	char	      *warn = NULL;
+	int cpt;
+	ksock_tx_t *tx;
+	ksock_tx_t *txtmp;
+	int rc;
+	int active;
+	char *warn = NULL;
 
 	active = (route != NULL);
 
@@ -1396,10 +1396,10 @@ ksocknal_close_conn_locked(ksock_conn_t *conn, int error)
 	/* This just does the immmediate housekeeping, and queues the
 	 * connection for the reaper to terminate.
 	 * Caller holds ksnd_global_lock exclusively in irq context */
-	ksock_peer_t      *peer = conn-&gt;ksnc_peer;
-	ksock_route_t     *route;
-	ksock_conn_t      *conn2;
-	struct list_head	*tmp;
+	ksock_peer_t *peer = conn-&gt;ksnc_peer;
+	ksock_route_t *route;
+	ksock_conn_t *conn2;
+	struct list_head *tmp;
 
 	LASSERT(peer-&gt;ksnp_error == 0);
 	LASSERT(!conn-&gt;ksnc_closing);
@@ -1479,7 +1479,7 @@ ksocknal_close_conn_locked(ksock_conn_t *conn, int error)
 void
 ksocknal_peer_failed(ksock_peer_t *peer)
 {
-	int	notify = 0;
+	int notify = 0;
 	unsigned long last_alive = 0;
 
 	/* There has been a connection failure or comms error; but I'll only
@@ -1506,9 +1506,9 @@ ksocknal_peer_failed(ksock_peer_t *peer)
 void
 ksocknal_finalize_zcreq(ksock_conn_t *conn)
 {
-	ksock_peer_t     *peer = conn-&gt;ksnc_peer;
-	ksock_tx_t       *tx;
-	ksock_tx_t       *tmp;
+	ksock_peer_t *peer = conn-&gt;ksnc_peer;
+	ksock_tx_t *tx;
+	ksock_tx_t *tmp;
 	LIST_HEAD(zlist);
 
 	/* NB safe to finalize TXs because closing of socket will
@@ -1546,9 +1546,9 @@ ksocknal_terminate_conn(ksock_conn_t *conn)
 	 * disengage the socket from its callbacks and close it.
 	 * ksnc_refcount will eventually hit zero, and then the reaper will
 	 * destroy it. */
-	ksock_peer_t     *peer = conn-&gt;ksnc_peer;
-	ksock_sched_t    *sched = conn-&gt;ksnc_scheduler;
-	int	       failed = 0;
+	ksock_peer_t *peer = conn-&gt;ksnc_peer;
+	ksock_sched_t *sched = conn-&gt;ksnc_scheduler;
+	int failed = 0;
 
 	LASSERT(conn-&gt;ksnc_closing);
 
@@ -1617,7 +1617,7 @@ ksocknal_queue_zombie_conn(ksock_conn_t *conn)
 void
 ksocknal_destroy_conn(ksock_conn_t *conn)
 {
-	unsigned long      last_rcv;
+	unsigned long last_rcv;
 
 	/* Final coup-de-grace of the reaper */
 	CDEBUG(D_NET, "connection %p\n", conn);
@@ -1677,10 +1677,10 @@ ksocknal_destroy_conn(ksock_conn_t *conn)
 int
 ksocknal_close_peer_conns_locked(ksock_peer_t *peer, __u32 ipaddr, int why)
 {
-	ksock_conn_t       *conn;
-	struct list_head	 *ctmp;
-	struct list_head	 *cnxt;
-	int		 count = 0;
+	ksock_conn_t *conn;
+	struct list_head *ctmp;
+	struct list_head *cnxt;
+	int count = 0;
 
 	list_for_each_safe(ctmp, cnxt, &amp;peer-&gt;ksnp_conns) {
 		conn = list_entry(ctmp, ksock_conn_t, ksnc_list);
@@ -1698,9 +1698,9 @@ ksocknal_close_peer_conns_locked(ksock_peer_t *peer, __u32 ipaddr, int why)
 int
 ksocknal_close_conn_and_siblings(ksock_conn_t *conn, int why)
 {
-	ksock_peer_t     *peer = conn-&gt;ksnc_peer;
-	__u32	     ipaddr = conn-&gt;ksnc_ipaddr;
-	int	       count;
+	ksock_peer_t *peer = conn-&gt;ksnc_peer;
+	__u32 ipaddr = conn-&gt;ksnc_ipaddr;
+	int count;
 
 	write_lock_bh(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -1714,13 +1714,13 @@ ksocknal_close_conn_and_siblings(ksock_conn_t *conn, int why)
 int
 ksocknal_close_matching_conns(lnet_process_id_t id, __u32 ipaddr)
 {
-	ksock_peer_t       *peer;
-	struct list_head	 *ptmp;
-	struct list_head	 *pnxt;
-	int		 lo;
-	int		 hi;
-	int		 i;
-	int		 count = 0;
+	ksock_peer_t *peer;
+	struct list_head *ptmp;
+	struct list_head *pnxt;
+	int lo;
+	int hi;
+	int i;
+	int count = 0;
 
 	write_lock_bh(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -1762,7 +1762,7 @@ ksocknal_notify(lnet_ni_t *ni, lnet_nid_t gw_nid, int alive)
 {
 	/* The router is telling me she's been notified of a change in
 	 * gateway state.... */
-	lnet_process_id_t  id = {0};
+	lnet_process_id_t id = {0};
 
 	id.nid = gw_nid;
 	id.pid = LNET_PID_ANY;
@@ -1783,20 +1783,20 @@ ksocknal_notify(lnet_ni_t *ni, lnet_nid_t gw_nid, int alive)
 void
 ksocknal_query(lnet_ni_t *ni, lnet_nid_t nid, unsigned long *when)
 {
-	int		connect = 1;
-	unsigned long	 last_alive = 0;
-	unsigned long	 now = cfs_time_current();
-	ksock_peer_t      *peer = NULL;
-	rwlock_t		*glock = &amp;ksocknal_data.ksnd_global_lock;
-	lnet_process_id_t  id = {.nid = nid, .pid = LUSTRE_SRV_LNET_PID};
+	int connect = 1;
+	unsigned long last_alive = 0;
+	unsigned long now = cfs_time_current();
+	ksock_peer_t *peer = NULL;
+	rwlock_t *glock = &amp;ksocknal_data.ksnd_global_lock;
+	lnet_process_id_t id = {.nid = nid, .pid = LUSTRE_SRV_LNET_PID};
 
 	read_lock(glock);
 
 	peer = ksocknal_find_peer_locked(ni, id);
 	if (peer != NULL) {
-		struct list_head       *tmp;
-		ksock_conn_t     *conn;
-		int	       bufnob;
+		struct list_head *tmp;
+		ksock_conn_t *conn;
+		int bufnob;
 
 		list_for_each(tmp, &amp;peer-&gt;ksnp_conns) {
 			conn = list_entry(tmp, ksock_conn_t, ksnc_list);
@@ -1844,10 +1844,10 @@ ksocknal_query(lnet_ni_t *ni, lnet_nid_t nid, unsigned long *when)
 static void
 ksocknal_push_peer(ksock_peer_t *peer)
 {
-	int	       index;
-	int	       i;
-	struct list_head       *tmp;
-	ksock_conn_t     *conn;
+	int index;
+	int i;
+	struct list_head *tmp;
+	ksock_conn_t *conn;
 
 	for (index = 0; ; index++) {
 		read_lock(&amp;ksocknal_data.ksnd_global_lock);
@@ -1877,12 +1877,12 @@ ksocknal_push_peer(ksock_peer_t *peer)
 static int
 ksocknal_push(lnet_ni_t *ni, lnet_process_id_t id)
 {
-	ksock_peer_t      *peer;
-	struct list_head	*tmp;
-	int		index;
-	int		i;
-	int		j;
-	int		rc = -ENOENT;
+	ksock_peer_t *peer;
+	struct list_head *tmp;
+	int index;
+	int i;
+	int j;
+	int rc = -ENOENT;
 
 	for (i = 0; i &lt; ksocknal_data.ksnd_peer_hash_size; i++) {
 		for (j = 0; ; j++) {
@@ -1926,15 +1926,15 @@ ksocknal_push(lnet_ni_t *ni, lnet_process_id_t id)
 static int
 ksocknal_add_interface(lnet_ni_t *ni, __u32 ipaddress, __u32 netmask)
 {
-	ksock_net_t       *net = ni-&gt;ni_data;
+	ksock_net_t *net = ni-&gt;ni_data;
 	ksock_interface_t *iface;
-	int		rc;
-	int		i;
-	int		j;
-	struct list_head	*ptmp;
-	ksock_peer_t      *peer;
-	struct list_head	*rtmp;
-	ksock_route_t     *route;
+	int rc;
+	int i;
+	int j;
+	struct list_head *ptmp;
+	ksock_peer_t *peer;
+	struct list_head *rtmp;
+	ksock_route_t *route;
 
 	if (ipaddress == 0 ||
 	    netmask == 0)
@@ -1988,12 +1988,12 @@ ksocknal_add_interface(lnet_ni_t *ni, __u32 ipaddress, __u32 netmask)
 static void
 ksocknal_peer_del_interface_locked(ksock_peer_t *peer, __u32 ipaddr)
 {
-	struct list_head	 *tmp;
-	struct list_head	 *nxt;
-	ksock_route_t      *route;
-	ksock_conn_t       *conn;
-	int		 i;
-	int		 j;
+	struct list_head *tmp;
+	struct list_head *nxt;
+	ksock_route_t *route;
+	ksock_conn_t *conn;
+	int i;
+	int j;
 
 	for (i = 0; i &lt; peer-&gt;ksnp_n_passive_ips; i++)
 		if (peer-&gt;ksnp_passive_ips[i] == ipaddr) {
@@ -2029,14 +2029,14 @@ ksocknal_peer_del_interface_locked(ksock_peer_t *peer, __u32 ipaddr)
 static int
 ksocknal_del_interface(lnet_ni_t *ni, __u32 ipaddress)
 {
-	ksock_net_t       *net = ni-&gt;ni_data;
-	int		rc = -ENOENT;
-	struct list_head	*tmp;
-	struct list_head	*nxt;
-	ksock_peer_t      *peer;
-	__u32	      this_ip;
-	int		i;
-	int		j;
+	ksock_net_t *net = ni-&gt;ni_data;
+	int rc = -ENOENT;
+	struct list_head *tmp;
+	struct list_head *nxt;
+	ksock_peer_t *peer;
+	__u32 this_ip;
+	int i;
+	int j;
 
 	write_lock_bh(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -2114,11 +2114,11 @@ ksocknal_ctl(lnet_ni_t *ni, unsigned int cmd, void *arg)
 					      data-&gt;ioc_u32[0]); /* IP address */
 
 	case IOC_LIBCFS_GET_PEER: {
-		__u32	    myip = 0;
-		__u32	    ip = 0;
-		int	      port = 0;
-		int	      conn_count = 0;
-		int	      share_count = 0;
+		__u32 myip = 0;
+		__u32 ip = 0;
+		int port = 0;
+		int conn_count = 0;
+		int share_count = 0;
 
 		rc = ksocknal_get_peer_info(ni, data-&gt;ioc_count,
 					    &amp;id, &amp;myip, &amp;ip, &amp;port,
@@ -2150,9 +2150,9 @@ ksocknal_ctl(lnet_ni_t *ni, unsigned int cmd, void *arg)
 					  data-&gt;ioc_u32[0]); /* IP */
 
 	case IOC_LIBCFS_GET_CONN: {
-		int	   txmem;
-		int	   rxmem;
-		int	   nagle;
+		int txmem;
+		int rxmem;
+		int nagle;
 		ksock_conn_t *conn = ksocknal_get_conn_by_idx(ni, data-&gt;ioc_count);
 
 		if (conn == NULL)
@@ -2207,8 +2207,8 @@ ksocknal_free_buffers(void)
 	LASSERT(atomic_read(&amp;ksocknal_data.ksnd_nactive_txs) == 0);
 
 	if (ksocknal_data.ksnd_sched_info != NULL) {
-		struct ksock_sched_info	*info;
-		int			i;
+		struct ksock_sched_info *info;
+		int i;
 
 		cfs_percpt_for_each(info, i, ksocknal_data.ksnd_sched_info) {
 			if (info-&gt;ksi_scheds != NULL) {
@@ -2227,8 +2227,8 @@ ksocknal_free_buffers(void)
 	spin_lock(&amp;ksocknal_data.ksnd_tx_lock);
 
 	if (!list_empty(&amp;ksocknal_data.ksnd_idle_noop_txs)) {
-		struct list_head	zlist;
-		ksock_tx_t	*tx;
+		struct list_head zlist;
+		ksock_tx_t *tx;
 
 		list_add(&amp;zlist, &amp;ksocknal_data.ksnd_idle_noop_txs);
 		list_del_init(&amp;ksocknal_data.ksnd_idle_noop_txs);
@@ -2248,9 +2248,9 @@ static void
 ksocknal_base_shutdown(void)
 {
 	struct ksock_sched_info *info;
-	ksock_sched_t		*sched;
-	int			i;
-	int			j;
+	ksock_sched_t *sched;
+	int i;
+	int j;
 
 	CDEBUG(D_MALLOC, "before NAL cleanup: kmem %d\n",
 	       atomic_read(&amp;libcfs_kmemory));
@@ -2351,8 +2351,8 @@ static int
 ksocknal_base_startup(void)
 {
 	struct ksock_sched_info	*info;
-	int			rc;
-	int			i;
+	int rc;
+	int i;
 
 	LASSERT(ksocknal_data.ksnd_init == SOCKNAL_INIT_NOTHING);
 	LASSERT(ksocknal_data.ksnd_nnets == 0);
@@ -2398,8 +2398,8 @@ ksocknal_base_startup(void)
 		goto failed;
 
 	cfs_percpt_for_each(info, i, ksocknal_data.ksnd_sched_info) {
-		ksock_sched_t	*sched;
-		int		nthrs;
+		ksock_sched_t *sched;
+		int nthrs;
 
 		nthrs = cfs_cpt_weight(lnet_cpt_table(), i);
 		if (*ksocknal_tunables.ksnd_nscheds &gt; 0) {
@@ -2430,9 +2430,9 @@ ksocknal_base_startup(void)
 		}
 	}
 
-	ksocknal_data.ksnd_connd_starting	 = 0;
-	ksocknal_data.ksnd_connd_failed_stamp     = 0;
-	ksocknal_data.ksnd_connd_starting_stamp   = get_seconds();
+	ksocknal_data.ksnd_connd_starting       = 0;
+	ksocknal_data.ksnd_connd_failed_stamp   = 0;
+	ksocknal_data.ksnd_connd_starting_stamp = get_seconds();
 	/* must have at least 2 connds to remain responsive to accepts while
 	 * connecting */
 	if (*ksocknal_tunables.ksnd_nconnds &lt; SOCKNAL_CONND_RESV + 1)
@@ -2482,9 +2482,9 @@ ksocknal_base_startup(void)
 static void
 ksocknal_debug_peerhash(lnet_ni_t *ni)
 {
-	ksock_peer_t	*peer = NULL;
-	struct list_head	*tmp;
-	int		i;
+	ksock_peer_t *peer = NULL;
+	struct list_head *tmp;
+	int i;
 
 	read_lock(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -2536,12 +2536,12 @@ ksocknal_debug_peerhash(lnet_ni_t *ni)
 void
 ksocknal_shutdown(lnet_ni_t *ni)
 {
-	ksock_net_t      *net = ni-&gt;ni_data;
-	int	       i;
+	ksock_net_t *net = ni-&gt;ni_data;
+	int i;
 	lnet_process_id_t anyid = {0};
 
-	anyid.nid =  LNET_NID_ANY;
-	anyid.pid =  LNET_PID_ANY;
+	anyid.nid = LNET_NID_ANY;
+	anyid.pid = LNET_PID_ANY;
 
 	LASSERT(ksocknal_data.ksnd_init == SOCKNAL_INIT_ALL);
 	LASSERT(ksocknal_data.ksnd_nnets &gt; 0);
@@ -2588,11 +2588,11 @@ ksocknal_shutdown(lnet_ni_t *ni)
 static int
 ksocknal_enumerate_interfaces(ksock_net_t *net)
 {
-	char      **names;
-	int	 i;
-	int	 j;
-	int	 rc;
-	int	 n;
+	char **names;
+	int i;
+	int j;
+	int rc;
+	int n;
 
 	n = libcfs_ipif_enumerate(&amp;names);
 	if (n &lt;= 0) {
@@ -2601,9 +2601,9 @@ ksocknal_enumerate_interfaces(ksock_net_t *net)
 	}
 
 	for (i = j = 0; i &lt; n; i++) {
-		int	up;
-		__u32      ip;
-		__u32      mask;
+		int up;
+		__u32 ip;
+		__u32 mask;
 
 		if (!strcmp(names[i], "lo")) /* skip the loopback IF */
 			continue;
@@ -2645,15 +2645,15 @@ ksocknal_enumerate_interfaces(ksock_net_t *net)
 static int
 ksocknal_search_new_ipif(ksock_net_t *net)
 {
-	int	new_ipif = 0;
-	int	i;
+	int new_ipif = 0;
+	int i;
 
 	for (i = 0; i &lt; net-&gt;ksnn_ninterfaces; i++) {
-		char		*ifnam = &amp;net-&gt;ksnn_interfaces[i].ksni_name[0];
-		char		*colon = strchr(ifnam, ':');
-		int		found  = 0;
-		ksock_net_t	*tmp;
-		int		j;
+		char *ifnam = &amp;net-&gt;ksnn_interfaces[i].ksni_name[0];
+		char *colon = strchr(ifnam, ':');
+		int found  = 0;
+		ksock_net_t *tmp;
+		int j;
 
 		if (colon != NULL) /* ignore alias device */
 			*colon = 0;
@@ -2687,9 +2687,9 @@ ksocknal_search_new_ipif(ksock_net_t *net)
 static int
 ksocknal_start_schedulers(struct ksock_sched_info *info)
 {
-	int	nthrs;
-	int	rc = 0;
-	int	i;
+	int nthrs;
+	int rc = 0;
+	int i;
 
 	if (info-&gt;ksi_nthreads == 0) {
 		if (*ksocknal_tunables.ksnd_nscheds &gt; 0) {
@@ -2708,9 +2708,9 @@ ksocknal_start_schedulers(struct ksock_sched_info *info)
 	}
 
 	for (i = 0; i &lt; nthrs; i++) {
-		long		id;
-		char		name[20];
-		ksock_sched_t	*sched;
+		long id;
+		char name[20];
+		ksock_sched_t *sched;
 		id = KSOCK_THREAD_ID(info-&gt;ksi_cpt, info-&gt;ksi_nthreads + i);
 		sched = &amp;info-&gt;ksi_scheds[KSOCK_THREAD_SID(id)];
 		snprintf(name, sizeof(name), "socknal_sd%02d_%02d",
@@ -2733,14 +2733,14 @@ ksocknal_start_schedulers(struct ksock_sched_info *info)
 static int
 ksocknal_net_start_threads(ksock_net_t *net, __u32 *cpts, int ncpts)
 {
-	int	newif = ksocknal_search_new_ipif(net);
-	int	rc;
-	int	i;
+	int newif = ksocknal_search_new_ipif(net);
+	int rc;
+	int i;
 
 	LASSERT(ncpts &gt; 0 &amp;&amp; ncpts &lt;= cfs_cpt_number(lnet_cpt_table()));
 
 	for (i = 0; i &lt; ncpts; i++) {
-		struct ksock_sched_info	*info;
+		struct ksock_sched_info *info;
 		int cpt = (cpts == NULL) ? i : cpts[i];
 
 		LASSERT(cpt &lt; cfs_cpt_number(lnet_cpt_table()));
@@ -2759,9 +2759,9 @@ ksocknal_net_start_threads(ksock_net_t *net, __u32 *cpts, int ncpts)
 int
 ksocknal_startup(lnet_ni_t *ni)
 {
-	ksock_net_t  *net;
-	int	   rc;
-	int	   i;
+	ksock_net_t *net;
+	int rc;
+	int i;
 
 	LASSERT(ni-&gt;ni_lnd == &amp;the_ksocklnd);
 
@@ -2791,7 +2791,7 @@ ksocknal_startup(lnet_ni_t *ni)
 		net-&gt;ksnn_ninterfaces = 1;
 	} else {
 		for (i = 0; i &lt; LNET_MAX_INTERFACES; i++) {
-			int    up;
+			int up;
 
 			if (ni-&gt;ni_interfaces[i] == NULL)
 				break;
@@ -2851,7 +2851,7 @@ ksocknal_module_fini(void)
 static int __init
 ksocknal_module_init(void)
 {
-	int    rc;
+	int rc;
 
 	/* check ksnr_connected/connecting field large enough */
 	CLASSERT(SOCKLND_CONN_NTYPES &lt;= 4);
diff --git a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd.h b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd.h
index c54c9955164e..c34378c0220a 100644
--- a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd.h
+++ b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd.h
@@ -36,15 +36,15 @@
 #include "../../../include/linux/lnet/socklnd.h"
 #include "../../../include/linux/lnet/lnet-sysctl.h"
 
-#define SOCKNAL_PEER_HASH_SIZE  101	     /* # peer lists */
-#define SOCKNAL_RESCHED	 100	     /* # scheduler loops before reschedule */
-#define SOCKNAL_INSANITY_RECONN 5000	    /* connd is trying on reconn infinitely */
-#define SOCKNAL_ENOMEM_RETRY    CFS_TICK	/* jiffies between retries */
+#define SOCKNAL_PEER_HASH_SIZE  101   /* # peer lists */
+#define SOCKNAL_RESCHED         100   /* # scheduler loops before reschedule */
+#define SOCKNAL_INSANITY_RECONN 5000  /* connd is trying on reconn infinitely */
+#define SOCKNAL_ENOMEM_RETRY    CFS_TICK /* jiffies between retries */
 
-#define SOCKNAL_SINGLE_FRAG_TX      0	   /* disable multi-fragment sends */
-#define SOCKNAL_SINGLE_FRAG_RX      0	   /* disable multi-fragment receives */
+#define SOCKNAL_SINGLE_FRAG_TX  0     /* disable multi-fragment sends */
+#define SOCKNAL_SINGLE_FRAG_RX  0     /* disable multi-fragment receives */
 
-#define SOCKNAL_VERSION_DEBUG       0	   /* enable protocol version debugging */
+#define SOCKNAL_VERSION_DEBUG   0     /* enable protocol version debugging */
 
 /* risk kmap deadlock on multi-frag I/O (backs off to single-frag if disabled).
  * no risk if we're not running on a CONFIG_HIGHMEM platform. */
@@ -58,33 +58,31 @@ struct ksock_sched_info;
 
 typedef struct				  /* per scheduler state */
 {
-	spinlock_t		kss_lock;	/* serialise */
-	struct list_head		kss_rx_conns;	/* conn waiting to be read */
-	/* conn waiting to be written */
-	struct list_head		kss_tx_conns;
-	/* zombie noop tx list */
-	struct list_head		kss_zombie_noop_txs;
-	wait_queue_head_t		kss_waitq;	/* where scheduler sleeps */
-	/* # connections assigned to this scheduler */
-	int			kss_nconns;
-	struct ksock_sched_info	*kss_info;	/* owner of it */
-	struct page		*kss_rx_scratch_pgs[LNET_MAX_IOV];
-	struct kvec		kss_scratch_iov[LNET_MAX_IOV];
+	spinlock_t              kss_lock;       /* serialise */
+	struct list_head        kss_rx_conns;   /* conn waiting to be read */
+	struct list_head        kss_tx_conns;   /* conn waiting to be written */
+	struct list_head        kss_zombie_noop_txs; /* zombie noop tx list */
+	wait_queue_head_t       kss_waitq;	/* where scheduler sleeps */
+	int                     kss_nconns;     /* # connections assigned to
+						 * this scheduler */
+	struct ksock_sched_info *kss_info;	/* owner of it */
+	struct page             *kss_rx_scratch_pgs[LNET_MAX_IOV];
+	struct kvec             kss_scratch_iov[LNET_MAX_IOV];
 } ksock_sched_t;
 
 struct ksock_sched_info {
-	int			ksi_nthreads_max; /* max allowed threads */
-	int			ksi_nthreads;	/* number of threads */
-	int			ksi_cpt;	/* CPT id */
-	ksock_sched_t		*ksi_scheds;	/* array of schedulers */
+	int                     ksi_nthreads_max; /* max allowed threads */
+	int                     ksi_nthreads;     /* number of threads */
+	int                     ksi_cpt;          /* CPT id */
+	ksock_sched_t           *ksi_scheds;      /* array of schedulers */
 };
 
-#define KSOCK_CPT_SHIFT			16
-#define KSOCK_THREAD_ID(cpt, sid)	(((cpt) &lt;&lt; KSOCK_CPT_SHIFT) | (sid))
-#define KSOCK_THREAD_CPT(id)		((id) &gt;&gt; KSOCK_CPT_SHIFT)
-#define KSOCK_THREAD_SID(id)		((id) &amp; ((1UL &lt;&lt; KSOCK_CPT_SHIFT) - 1))
+#define KSOCK_CPT_SHIFT           16
+#define KSOCK_THREAD_ID(cpt, sid) (((cpt) &lt;&lt; KSOCK_CPT_SHIFT) | (sid))
+#define KSOCK_THREAD_CPT(id)      ((id) &gt;&gt; KSOCK_CPT_SHIFT)
+#define KSOCK_THREAD_SID(id)      ((id) &amp; ((1UL &lt;&lt; KSOCK_CPT_SHIFT) - 1))
 
-typedef struct				  /* in-use interface */
+typedef struct                                  /* in-use interface */
 {
 	__u32		ksni_ipaddr;		/* interface's IP address */
 	__u32		ksni_netmask;		/* interface's network mask */
@@ -94,35 +92,48 @@ typedef struct				  /* in-use interface */
 } ksock_interface_t;
 
 typedef struct {
-	/* "stuck" socket timeout (seconds) */
-	int	      *ksnd_timeout;
-	/* # scheduler threads in each pool while starting */
-	int		 *ksnd_nscheds;
-	int	      *ksnd_nconnds;	 /* # connection daemons */
-	int	      *ksnd_nconnds_max;     /* max # connection daemons */
-	int	      *ksnd_min_reconnectms; /* first connection retry after (ms)... */
-	int	      *ksnd_max_reconnectms; /* ...exponentially increasing to this */
-	int	      *ksnd_eager_ack;       /* make TCP ack eagerly? */
-	int	      *ksnd_typed_conns;     /* drive sockets by type? */
-	int	      *ksnd_min_bulk;	/* smallest "large" message */
-	int	      *ksnd_tx_buffer_size;  /* socket tx buffer size */
-	int	      *ksnd_rx_buffer_size;  /* socket rx buffer size */
-	int	      *ksnd_nagle;	   /* enable NAGLE? */
-	int	      *ksnd_round_robin;     /* round robin for multiple interfaces */
-	int	      *ksnd_keepalive;       /* # secs for sending keepalive NOOP */
-	int	      *ksnd_keepalive_idle;  /* # idle secs before 1st probe */
-	int	      *ksnd_keepalive_count; /* # probes */
-	int	      *ksnd_keepalive_intvl; /* time between probes */
-	int	      *ksnd_credits;	 /* # concurrent sends */
-	int	      *ksnd_peertxcredits;   /* # concurrent sends to 1 peer */
-	int	      *ksnd_peerrtrcredits;  /* # per-peer router buffer credits */
-	int	      *ksnd_peertimeout;     /* seconds to consider peer dead */
-	int	      *ksnd_enable_csum;     /* enable check sum */
-	int	      *ksnd_inject_csum_error; /* set non-zero to inject checksum error */
-	int	      *ksnd_nonblk_zcack;    /* always send zc-ack on non-blocking connection */
-	unsigned int     *ksnd_zc_min_payload;  /* minimum zero copy payload size */
-	int	      *ksnd_zc_recv;	 /* enable ZC receive (for Chelsio TOE) */
-	int	      *ksnd_zc_recv_min_nfrags; /* minimum # of fragments to enable ZC receive */
+	int          *ksnd_timeout;            /* "stuck" socket timeout
+						* (seconds) */
+	int          *ksnd_nscheds;            /* # scheduler threads in each
+						* pool while starting */
+	int          *ksnd_nconnds;            /* # connection daemons */
+	int          *ksnd_nconnds_max;        /* max # connection daemons */
+	int          *ksnd_min_reconnectms;    /* first connection retry after
+						* (ms)... */
+	int          *ksnd_max_reconnectms;    /* ...exponentially increasing to
+						* this */
+	int          *ksnd_eager_ack;          /* make TCP ack eagerly? */
+	int          *ksnd_typed_conns;        /* drive sockets by type? */
+	int          *ksnd_min_bulk;           /* smallest "large" message */
+	int          *ksnd_tx_buffer_size;     /* socket tx buffer size */
+	int          *ksnd_rx_buffer_size;     /* socket rx buffer size */
+	int          *ksnd_nagle;              /* enable NAGLE? */
+	int          *ksnd_round_robin;        /* round robin for multiple
+						* interfaces */
+	int          *ksnd_keepalive;          /* # secs for sending keepalive
+						* NOOP */
+	int          *ksnd_keepalive_idle;     /* # idle secs before 1st probe
+						*/
+	int          *ksnd_keepalive_count;    /* # probes */
+	int          *ksnd_keepalive_intvl;    /* time between probes */
+	int          *ksnd_credits;            /* # concurrent sends */
+	int          *ksnd_peertxcredits;      /* # concurrent sends to 1 peer
+						*/
+	int          *ksnd_peerrtrcredits;     /* # per-peer router buffer
+						* credits */
+	int          *ksnd_peertimeout;        /* seconds to consider peer dead
+						*/
+	int          *ksnd_enable_csum;        /* enable check sum */
+	int          *ksnd_inject_csum_error;  /* set non-zero to inject
+						* checksum error */
+	int          *ksnd_nonblk_zcack;       /* always send zc-ack on
+						* non-blocking connection */
+	unsigned int *ksnd_zc_min_payload;     /* minimum zero copy payload
+						* size */
+	int          *ksnd_zc_recv;            /* enable ZC receive (for
+						* Chelsio TOE) */
+	int          *ksnd_zc_recv_min_nfrags; /* minimum # of fragments to
+						* enable ZC receive */
 } ksock_tunables_t;
 
 typedef struct {
@@ -141,55 +152,67 @@ typedef struct {
 #define SOCKNAL_CONND_RESV     1
 
 typedef struct {
-	int			ksnd_init;	/* initialisation state */
-	int			ksnd_nnets;	/* # networks set up */
-	struct list_head		ksnd_nets;	/* list of nets */
-	/* stabilize peer/conn ops */
-	rwlock_t		ksnd_global_lock;
-	/* hash table of all my known peers */
-	struct list_head		*ksnd_peers;
-	int			ksnd_peer_hash_size; /* size of ksnd_peers */
-
-	int			ksnd_nthreads;	/* # live threads */
-	int			ksnd_shuttingdown; /* tell threads to exit */
-	/* schedulers information */
-	struct ksock_sched_info	**ksnd_sched_info;
-
-	atomic_t      ksnd_nactive_txs;    /* #active txs */
-
-	struct list_head	ksnd_deathrow_conns; /* conns to close: reaper_lock*/
-	struct list_head	ksnd_zombie_conns;   /* conns to free: reaper_lock */
-	struct list_head	ksnd_enomem_conns;   /* conns to retry: reaper_lock*/
-	wait_queue_head_t       ksnd_reaper_waitq;   /* reaper sleeps here */
-	unsigned long	ksnd_reaper_waketime;/* when reaper will wake */
-	spinlock_t	  ksnd_reaper_lock;	/* serialise */
-
-	int	       ksnd_enomem_tx;      /* test ENOMEM sender */
-	int	       ksnd_stall_tx;       /* test sluggish sender */
-	int	       ksnd_stall_rx;       /* test sluggish receiver */
-
-	struct list_head	ksnd_connd_connreqs; /* incoming connection requests */
-	struct list_head	ksnd_connd_routes;   /* routes waiting to be connected */
-	wait_queue_head_t       ksnd_connd_waitq;    /* connds sleep here */
-	int	       ksnd_connd_connecting;/* # connds connecting */
-	/** time stamp of the last failed connecting attempt */
-	long	      ksnd_connd_failed_stamp;
-	/** # starting connd */
-	unsigned	  ksnd_connd_starting;
-	/** time stamp of the last starting connd */
-	long	      ksnd_connd_starting_stamp;
-	/** # running connd */
-	unsigned	  ksnd_connd_running;
-	spinlock_t	  ksnd_connd_lock;	/* serialise */
-
-	struct list_head	  ksnd_idle_noop_txs;	/* list head for freed noop tx */
-	spinlock_t	  ksnd_tx_lock;		/* serialise, g_lock unsafe */
+	int                     ksnd_init;              /* initialisation state
+							 */
+	int                     ksnd_nnets;             /* # networks set up */
+	struct list_head        ksnd_nets;              /* list of nets */
+	rwlock_t                ksnd_global_lock;       /* stabilize peer/conn
+							 * ops */
+	struct list_head        *ksnd_peers;            /* hash table of all my
+							 * known peers */
+	int                     ksnd_peer_hash_size;    /* size of ksnd_peers */
+
+	int                     ksnd_nthreads;          /* # live threads */
+	int                     ksnd_shuttingdown;      /* tell threads to exit
+							 */
+	struct ksock_sched_info **ksnd_sched_info;      /* schedulers info */
+
+	atomic_t                ksnd_nactive_txs;       /* #active txs */
+
+	struct list_head        ksnd_deathrow_conns;    /* conns to close:
+							 * reaper_lock*/
+	struct list_head        ksnd_zombie_conns;      /* conns to free:
+							 * reaper_lock */
+	struct list_head        ksnd_enomem_conns;      /* conns to retry:
+							 * reaper_lock*/
+	wait_queue_head_t       ksnd_reaper_waitq;      /* reaper sleeps here */
+	unsigned long	        ksnd_reaper_waketime;   /* when reaper will wake
+							 */
+	spinlock_t              ksnd_reaper_lock;       /* serialise */
+
+	int                     ksnd_enomem_tx;         /* test ENOMEM sender */
+	int                     ksnd_stall_tx;          /* test sluggish sender
+							 */
+	int                     ksnd_stall_rx;          /* test sluggish
+							 * receiver */
+
+	struct list_head        ksnd_connd_connreqs;    /* incoming connection
+							 * requests */
+	struct list_head        ksnd_connd_routes;      /* routes waiting to be
+							 * connected */
+	wait_queue_head_t       ksnd_connd_waitq;       /* connds sleep here */
+	int                     ksnd_connd_connecting;  /* # connds connecting
+							 */
+	long                    ksnd_connd_failed_stamp;/* time stamp of the
+							 * last failed
+							 * connecting attempt */
+	unsigned                ksnd_connd_starting;    /* # starting connd */
+	long                    ksnd_connd_starting_stamp;/* time stamp of the
+							   * last starting connd
+							   */
+	unsigned                ksnd_connd_running;     /* # running connd */
+	spinlock_t              ksnd_connd_lock;        /* serialise */
+
+	struct list_head        ksnd_idle_noop_txs;     /* list head for freed
+							 * noop tx */
+	spinlock_t              ksnd_tx_lock;           /* serialise, g_lock
+							 * unsafe */
 
 } ksock_nal_data_t;
 
-#define SOCKNAL_INIT_NOTHING    0
-#define SOCKNAL_INIT_DATA       1
-#define SOCKNAL_INIT_ALL	2
+#define SOCKNAL_INIT_NOTHING 0
+#define SOCKNAL_INIT_DATA    1
+#define SOCKNAL_INIT_ALL     2
 
 /* A packet just assembled for transmission is represented by 1 or more
  * struct iovec fragments (the first frag contains the portals header),
@@ -200,43 +223,45 @@ typedef struct {
  * received into either struct iovec or lnet_kiov_t fragments, depending on
  * what the header matched or whether the message needs forwarding. */
 
-struct ksock_conn;			      /* forward ref */
-struct ksock_peer;			      /* forward ref */
-struct ksock_route;			     /* forward ref */
-struct ksock_proto;			     /* forward ref */
+struct ksock_conn;  /* forward ref */
+struct ksock_peer;  /* forward ref */
+struct ksock_route; /* forward ref */
+struct ksock_proto; /* forward ref */
 
-typedef struct				  /* transmit packet */
+typedef struct                             /* transmit packet */
 {
-	struct list_head     tx_list;	/* queue on conn for transmission etc */
-	struct list_head     tx_zc_list;     /* queue on peer for ZC request */
-	atomic_t   tx_refcount;    /* tx reference count */
-	int	    tx_nob;	 /* # packet bytes */
-	int	    tx_resid;       /* residual bytes */
-	int	    tx_niov;	/* # packet iovec frags */
-	struct kvec  *tx_iov;	 /* packet iovec frags */
-	int	    tx_nkiov;       /* # packet page frags */
-	unsigned short tx_zc_aborted;  /* aborted ZC request */
-	unsigned short tx_zc_capable:1; /* payload is large enough for ZC */
-	unsigned short tx_zc_checked:1; /* Have I checked if I should ZC? */
-	unsigned short tx_nonblk:1;    /* it's a non-blocking ACK */
-	lnet_kiov_t   *tx_kiov;	/* packet page frags */
-	struct ksock_conn  *tx_conn;	/* owning conn */
-	lnet_msg_t    *tx_lnetmsg;     /* lnet message for lnet_finalize() */
-	unsigned long     tx_deadline;    /* when (in jiffies) tx times out */
-	ksock_msg_t    tx_msg;	 /* socklnd message buffer */
-	int	    tx_desc_size;   /* size of this descriptor */
+	struct list_head  tx_list;         /* queue on conn for transmission etc
+					    */
+	struct list_head  tx_zc_list;      /* queue on peer for ZC request */
+	atomic_t          tx_refcount;     /* tx reference count */
+	int               tx_nob;          /* # packet bytes */
+	int               tx_resid;        /* residual bytes */
+	int               tx_niov;         /* # packet iovec frags */
+	struct kvec       *tx_iov;         /* packet iovec frags */
+	int               tx_nkiov;        /* # packet page frags */
+	unsigned short    tx_zc_aborted;   /* aborted ZC request */
+	unsigned short    tx_zc_capable:1; /* payload is large enough for ZC */
+	unsigned short    tx_zc_checked:1; /* Have I checked if I should ZC? */
+	unsigned short    tx_nonblk:1;     /* it's a non-blocking ACK */
+	lnet_kiov_t       *tx_kiov;        /* packet page frags */
+	struct ksock_conn *tx_conn;        /* owning conn */
+	lnet_msg_t        *tx_lnetmsg;     /* lnet message for lnet_finalize()
+					    */
+	unsigned long     tx_deadline;     /* when (in jiffies) tx times out */
+	ksock_msg_t       tx_msg;          /* socklnd message buffer */
+	int               tx_desc_size;    /* size of this descriptor */
 	union {
 		struct {
-			struct kvec iov;       /* virt hdr */
-			lnet_kiov_t  kiov[0];   /* paged payload */
-		}		  paged;
+			struct kvec iov;     /* virt hdr */
+			lnet_kiov_t kiov[0]; /* paged payload */
+		} paged;
 		struct {
-			struct kvec iov[1];    /* virt hdr + payload */
-		}		  virt;
-	}		       tx_frags;
+			struct kvec iov[1];  /* virt hdr + payload */
+		} virt;
+	} tx_frags;
 } ksock_tx_t;
 
-#define KSOCK_NOOP_TX_SIZE  ((int)offsetof(ksock_tx_t, tx_frags.paged.kiov[0]))
+#define KSOCK_NOOP_TX_SIZE ((int)offsetof(ksock_tx_t, tx_frags.paged.kiov[0]))
 
 /* network zero copy callback descriptor embedded in ksock_tx_t */
 
@@ -247,148 +272,189 @@ typedef union {
 	lnet_kiov_t      kiov[LNET_MAX_IOV];
 } ksock_rxiovspace_t;
 
-#define SOCKNAL_RX_KSM_HEADER   1	       /* reading ksock message header */
-#define SOCKNAL_RX_LNET_HEADER  2	       /* reading lnet message header */
-#define SOCKNAL_RX_PARSE	3	       /* Calling lnet_parse() */
-#define SOCKNAL_RX_PARSE_WAIT   4	       /* waiting to be told to read the body */
-#define SOCKNAL_RX_LNET_PAYLOAD 5	       /* reading lnet payload (to deliver here) */
-#define SOCKNAL_RX_SLOP	 6	       /* skipping body */
+#define SOCKNAL_RX_KSM_HEADER   1 /* reading ksock message header */
+#define SOCKNAL_RX_LNET_HEADER  2 /* reading lnet message header */
+#define SOCKNAL_RX_PARSE        3 /* Calling lnet_parse() */
+#define SOCKNAL_RX_PARSE_WAIT   4 /* waiting to be told to read the body */
+#define SOCKNAL_RX_LNET_PAYLOAD 5 /* reading lnet payload (to deliver here) */
+#define SOCKNAL_RX_SLOP         6 /* skipping body */
 
 typedef struct ksock_conn {
-	struct ksock_peer  *ksnc_peer;	 /* owning peer */
-	struct ksock_route *ksnc_route;	/* owning route */
-	struct list_head	  ksnc_list;	 /* stash on peer's conn list */
-	struct socket       *ksnc_sock;	 /* actual socket */
-	void	       *ksnc_saved_data_ready; /* socket's original data_ready() callback */
-	void	       *ksnc_saved_write_space; /* socket's original write_space() callback */
-	atomic_t	ksnc_conn_refcount; /* conn refcount */
-	atomic_t	ksnc_sock_refcount; /* sock refcount */
-	ksock_sched_t      *ksnc_scheduler;  /* who schedules this connection */
-	__u32	       ksnc_myipaddr;   /* my IP */
-	__u32	       ksnc_ipaddr;     /* peer's IP */
-	int		 ksnc_port;       /* peer's port */
-	signed int	  ksnc_type:3;     /* type of connection,
-					      * should be signed value */
-	unsigned int	    ksnc_closing:1;  /* being shut down */
-	unsigned int	    ksnc_flip:1;     /* flip or not, only for V2.x */
-	unsigned int	    ksnc_zc_capable:1; /* enable to ZC */
-	struct ksock_proto *ksnc_proto;      /* protocol for the connection */
+	struct ksock_peer  *ksnc_peer;        /* owning peer */
+	struct ksock_route *ksnc_route;       /* owning route */
+	struct list_head   ksnc_list;         /* stash on peer's conn list */
+	struct socket      *ksnc_sock;        /* actual socket */
+	void               *ksnc_saved_data_ready;  /* socket's original
+						     * data_ready() callback */
+	void               *ksnc_saved_write_space; /* socket's original
+						     * write_space() callback */
+	atomic_t           ksnc_conn_refcount;/* conn refcount */
+	atomic_t           ksnc_sock_refcount;/* sock refcount */
+	ksock_sched_t      *ksnc_scheduler;   /* who schedules this connection
+					       */
+	__u32              ksnc_myipaddr;     /* my IP */
+	__u32              ksnc_ipaddr;       /* peer's IP */
+	int                ksnc_port;         /* peer's port */
+	signed int         ksnc_type:3;       /* type of connection, should be
+					       * signed value */
+	unsigned int       ksnc_closing:1;    /* being shut down */
+	unsigned int       ksnc_flip:1;       /* flip or not, only for V2.x */
+	unsigned int       ksnc_zc_capable:1; /* enable to ZC */
+	struct ksock_proto *ksnc_proto;       /* protocol for the connection */
 
 	/* reader */
-	struct list_head  ksnc_rx_list;     /* where I enq waiting input or a forwarding descriptor */
-	unsigned long	    ksnc_rx_deadline; /* when (in jiffies) receive times out */
-	__u8		  ksnc_rx_started;  /* started receiving a message */
-	__u8		  ksnc_rx_ready;    /* data ready to read */
-	__u8		  ksnc_rx_scheduled;/* being progressed */
-	__u8		  ksnc_rx_state;    /* what is being read */
-	int		   ksnc_rx_nob_left; /* # bytes to next hdr/body */
-	int		   ksnc_rx_nob_wanted; /* bytes actually wanted */
-	int		   ksnc_rx_niov;     /* # iovec frags */
-	struct kvec 	 *ksnc_rx_iov;      /* the iovec frags */
-	int		   ksnc_rx_nkiov;    /* # page frags */
-	lnet_kiov_t	  *ksnc_rx_kiov;     /* the page frags */
-	ksock_rxiovspace_t    ksnc_rx_iov_space;/* space for frag descriptors */
-	__u32		 ksnc_rx_csum;     /* partial checksum for incoming data */
-	void		 *ksnc_cookie;      /* rx lnet_finalize passthru arg */
-	ksock_msg_t	   ksnc_msg;	 /* incoming message buffer:
-						 * V2.x message takes the
-						 * whole struct
-						 * V1.x message is a bare
-						 * lnet_hdr_t, it's stored in
-						 * ksnc_msg.ksm_u.lnetmsg */
+	struct list_head   ksnc_rx_list;      /* where I enq waiting input or a
+					       * forwarding descriptor */
+	unsigned long      ksnc_rx_deadline;  /* when (in jiffies) receive times
+					       * out */
+	__u8               ksnc_rx_started;   /* started receiving a message */
+	__u8               ksnc_rx_ready;     /* data ready to read */
+	__u8               ksnc_rx_scheduled; /* being progressed */
+	__u8               ksnc_rx_state;     /* what is being read */
+	int                ksnc_rx_nob_left;  /* # bytes to next hdr/body */
+	int                ksnc_rx_nob_wanted;/* bytes actually wanted */
+	int                ksnc_rx_niov;      /* # iovec frags */
+	struct kvec        *ksnc_rx_iov;      /* the iovec frags */
+	int                ksnc_rx_nkiov;     /* # page frags */
+	lnet_kiov_t        *ksnc_rx_kiov;     /* the page frags */
+	ksock_rxiovspace_t ksnc_rx_iov_space; /* space for frag descriptors */
+	__u32              ksnc_rx_csum;      /* partial checksum for incoming
+					       * data */
+	void               *ksnc_cookie;      /* rx lnet_finalize passthru arg
+					       */
+	ksock_msg_t        ksnc_msg;          /* incoming message buffer:
+					       * V2.x message takes the
+					       * whole struct
+					       * V1.x message is a bare
+					       * lnet_hdr_t, it's stored in
+					       * ksnc_msg.ksm_u.lnetmsg */
 
 	/* WRITER */
-	struct list_head	    ksnc_tx_list;     /* where I enq waiting for output space */
-	struct list_head	    ksnc_tx_queue;    /* packets waiting to be sent */
-	ksock_tx_t	   *ksnc_tx_carrier;  /* next TX that can carry a LNet message or ZC-ACK */
-	unsigned long	    ksnc_tx_deadline; /* when (in jiffies) tx times out */
-	int		   ksnc_tx_bufnob;     /* send buffer marker */
-	atomic_t	  ksnc_tx_nob;	/* # bytes queued */
-	int		   ksnc_tx_ready;      /* write space */
-	int		   ksnc_tx_scheduled;  /* being progressed */
-	unsigned long	    ksnc_tx_last_post;  /* time stamp of the last posted TX */
+	struct list_head   ksnc_tx_list;      /* where I enq waiting for output
+					       * space */
+	struct list_head   ksnc_tx_queue;     /* packets waiting to be sent */
+	ksock_tx_t         *ksnc_tx_carrier;  /* next TX that can carry a LNet
+					       * message or ZC-ACK */
+	unsigned long      ksnc_tx_deadline;  /* when (in jiffies) tx times out
+					       */
+	int                ksnc_tx_bufnob;    /* send buffer marker */
+	atomic_t           ksnc_tx_nob;       /* # bytes queued */
+	int		   ksnc_tx_ready;     /* write space */
+	int		   ksnc_tx_scheduled; /* being progressed */
+	unsigned long      ksnc_tx_last_post; /* time stamp of the last posted
+					       * TX */
 } ksock_conn_t;
 
 typedef struct ksock_route {
-	struct list_head	    ksnr_list;	/* chain on peer route list */
-	struct list_head	    ksnr_connd_list;  /* chain on ksnr_connd_routes */
-	struct ksock_peer    *ksnr_peer;	/* owning peer */
-	atomic_t	  ksnr_refcount;    /* # users */
-	unsigned long	    ksnr_timeout;     /* when (in jiffies) reconnection can happen next */
-	long	ksnr_retry_interval; /* how long between retries */
-	__u32		 ksnr_myipaddr;    /* my IP */
-	__u32		 ksnr_ipaddr;      /* IP address to connect to */
-	int		   ksnr_port;	/* port to connect to */
-	unsigned int	  ksnr_scheduled:1; /* scheduled for attention */
-	unsigned int	  ksnr_connecting:1;/* connection establishment in progress */
-	unsigned int	  ksnr_connected:4; /* connections established by type */
-	unsigned int	  ksnr_deleted:1;   /* been removed from peer? */
-	unsigned int	  ksnr_share_count; /* created explicitly? */
-	int		   ksnr_conn_count;  /* # conns established by this route */
+	struct list_head  ksnr_list;           /* chain on peer route list */
+	struct list_head  ksnr_connd_list;     /* chain on ksnr_connd_routes */
+	struct ksock_peer *ksnr_peer;          /* owning peer */
+	atomic_t          ksnr_refcount;       /* # users */
+	unsigned long     ksnr_timeout;        /* when (in jiffies) reconnection
+						* can happen next */
+	long              ksnr_retry_interval; /* how long between retries */
+	__u32             ksnr_myipaddr;       /* my IP */
+	__u32             ksnr_ipaddr;         /* IP address to connect to */
+	int               ksnr_port;           /* port to connect to */
+	unsigned int      ksnr_scheduled:1;    /* scheduled for attention */
+	unsigned int      ksnr_connecting:1;   /* connection establishment in
+						* progress */
+	unsigned int      ksnr_connected:4;    /* connections established by
+						* type */
+	unsigned int      ksnr_deleted:1;      /* been removed from peer? */
+	unsigned int      ksnr_share_count;    /* created explicitly? */
+	int               ksnr_conn_count;     /* # conns established by this
+						* route */
 } ksock_route_t;
 
-#define SOCKNAL_KEEPALIVE_PING	  1       /* cookie for keepalive ping */
+#define SOCKNAL_KEEPALIVE_PING 1 /* cookie for keepalive ping */
 
 typedef struct ksock_peer {
-	struct list_head	    ksnp_list;	/* stash on global peer list */
-	unsigned long	    ksnp_last_alive;  /* when (in jiffies) I was last alive */
-	lnet_process_id_t     ksnp_id;       /* who's on the other end(s) */
-	atomic_t	  ksnp_refcount; /* # users */
-	int		   ksnp_sharecount;  /* lconf usage counter */
-	int		   ksnp_closing;  /* being closed */
-	int		   ksnp_accepting;/* # passive connections pending */
-	int		   ksnp_error;    /* errno on closing last conn */
-	__u64		 ksnp_zc_next_cookie;/* ZC completion cookie */
-	__u64		 ksnp_incarnation;   /* latest known peer incarnation */
-	struct ksock_proto   *ksnp_proto;    /* latest known peer protocol */
-	struct list_head	    ksnp_conns;    /* all active connections */
-	struct list_head	    ksnp_routes;   /* routes */
-	struct list_head	    ksnp_tx_queue; /* waiting packets */
-	spinlock_t	      ksnp_lock;	/* serialize, g_lock unsafe */
-	struct list_head	    ksnp_zc_req_list;   /* zero copy requests wait for ACK  */
-	unsigned long	    ksnp_send_keepalive; /* time to send keepalive */
-	lnet_ni_t	    *ksnp_ni;       /* which network */
-	int		   ksnp_n_passive_ips; /* # of... */
-	__u32		 ksnp_passive_ips[LNET_MAX_INTERFACES]; /* preferred local interfaces */
+	struct list_head   ksnp_list;           /* stash on global peer list */
+	unsigned long      ksnp_last_alive;     /* when (in jiffies) I was last
+						 * alive */
+	lnet_process_id_t  ksnp_id;             /* who's on the other end(s) */
+	atomic_t           ksnp_refcount;       /* # users */
+	int                ksnp_sharecount;     /* lconf usage counter */
+	int                ksnp_closing;        /* being closed */
+	int                ksnp_accepting;      /* # passive connections pending
+						 */
+	int                ksnp_error;          /* errno on closing last conn */
+	__u64              ksnp_zc_next_cookie; /* ZC completion cookie */
+	__u64              ksnp_incarnation;    /* latest known peer incarnation
+						 */
+	struct ksock_proto *ksnp_proto;         /* latest known peer protocol */
+	struct list_head   ksnp_conns;          /* all active connections */
+	struct list_head   ksnp_routes;         /* routes */
+	struct list_head   ksnp_tx_queue;       /* waiting packets */
+	spinlock_t         ksnp_lock;           /* serialize, g_lock unsafe */
+	struct list_head   ksnp_zc_req_list;    /* zero copy requests wait for
+						 * ACK  */
+	unsigned long      ksnp_send_keepalive; /* time to send keepalive */
+	lnet_ni_t          *ksnp_ni;            /* which network */
+	int                ksnp_n_passive_ips;  /* # of... */
+
+	/* preferred local interfaces */
+	__u32              ksnp_passive_ips[LNET_MAX_INTERFACES];
 } ksock_peer_t;
 
 typedef struct ksock_connreq {
-	struct list_head	    ksncr_list;     /* stash on ksnd_connd_connreqs */
-	lnet_ni_t	    *ksncr_ni;       /* chosen NI */
-	struct socket	 *ksncr_sock;     /* accepted socket */
+	struct list_head ksncr_list;  /* stash on ksnd_connd_connreqs */
+	lnet_ni_t        *ksncr_ni;   /* chosen NI */
+	struct socket    *ksncr_sock; /* accepted socket */
 } ksock_connreq_t;
 
 extern ksock_nal_data_t ksocknal_data;
 extern ksock_tunables_t ksocknal_tunables;
 
-#define SOCKNAL_MATCH_NO	0	/* TX can't match type of connection */
-#define SOCKNAL_MATCH_YES       1	/* TX matches type of connection */
-#define SOCKNAL_MATCH_MAY       2	/* TX can be sent on the connection, but not preferred */
+#define SOCKNAL_MATCH_NO  0 /* TX can't match type of connection */
+#define SOCKNAL_MATCH_YES 1 /* TX matches type of connection */
+#define SOCKNAL_MATCH_MAY 2 /* TX can be sent on the connection, but not
+			     * preferred */
 
 typedef struct ksock_proto {
-	int	   pro_version;					      /* version number of protocol */
-	int	 (*pro_send_hello)(ksock_conn_t *, ksock_hello_msg_t *);     /* handshake function */
-	int	 (*pro_recv_hello)(ksock_conn_t *, ksock_hello_msg_t *, int);/* handshake function */
-	void	(*pro_pack)(ksock_tx_t *);				  /* message pack */
-	void	(*pro_unpack)(ksock_msg_t *);			       /* message unpack */
-	ksock_tx_t *(*pro_queue_tx_msg)(ksock_conn_t *, ksock_tx_t *);	  /* queue tx on the connection */
-	int	 (*pro_queue_tx_zcack)(ksock_conn_t *, ksock_tx_t *, __u64); /* queue ZC ack on the connection */
-	int	 (*pro_handle_zcreq)(ksock_conn_t *, __u64, int);	    /* handle ZC request */
-	int	 (*pro_handle_zcack)(ksock_conn_t *, __u64, __u64);	  /* handle ZC ACK */
-	int	 (*pro_match_tx)(ksock_conn_t *, ksock_tx_t *, int);	 /* msg type matches the connection type:
-										 * return value:
-										 *   return MATCH_NO  : no
-										 *   return MATCH_YES : matching type
-										 *   return MATCH_MAY : can be backup */
+	/* version number of protocol */
+	int        pro_version;
+
+	/* handshake function */
+	int        (*pro_send_hello)(ksock_conn_t *, ksock_hello_msg_t *);
+
+	/* handshake function */
+	int        (*pro_recv_hello)(ksock_conn_t *, ksock_hello_msg_t *, int);
+
+	/* message pack */
+	void       (*pro_pack)(ksock_tx_t *);
+
+	/* message unpack */
+	void       (*pro_unpack)(ksock_msg_t *);
+
+	/* queue tx on the connection */
+	ksock_tx_t *(*pro_queue_tx_msg)(ksock_conn_t *, ksock_tx_t *);
+
+	/* queue ZC ack on the connection */
+	int        (*pro_queue_tx_zcack)(ksock_conn_t *, ksock_tx_t *, __u64);
+
+	/* handle ZC request */
+	int        (*pro_handle_zcreq)(ksock_conn_t *, __u64, int);
+
+	/* handle ZC ACK */
+	int        (*pro_handle_zcack)(ksock_conn_t *, __u64, __u64);
+
+	/* msg type matches the connection type:
+	 * return value:
+	 *   return MATCH_NO  : no
+	 *   return MATCH_YES : matching type
+	 *   return MATCH_MAY : can be backup */
+	int        (*pro_match_tx)(ksock_conn_t *, ksock_tx_t *, int);
 } ksock_proto_t;
 
 extern ksock_proto_t ksocknal_protocol_v1x;
 extern ksock_proto_t ksocknal_protocol_v2x;
 extern ksock_proto_t ksocknal_protocol_v3x;
 
-#define KSOCK_PROTO_V1_MAJOR    LNET_PROTO_TCP_VERSION_MAJOR
-#define KSOCK_PROTO_V1_MINOR    LNET_PROTO_TCP_VERSION_MINOR
-#define KSOCK_PROTO_V1	  KSOCK_PROTO_V1_MAJOR
+#define KSOCK_PROTO_V1_MAJOR LNET_PROTO_TCP_VERSION_MAJOR
+#define KSOCK_PROTO_V1_MINOR LNET_PROTO_TCP_VERSION_MINOR
+#define KSOCK_PROTO_V1       KSOCK_PROTO_V1_MAJOR
 
 #ifndef CPU_MASK_NONE
 #define CPU_MASK_NONE   0UL
@@ -434,7 +500,7 @@ ksocknal_conn_decref(ksock_conn_t *conn)
 static inline int
 ksocknal_connsock_addref(ksock_conn_t *conn)
 {
-	int   rc = -ESHUTDOWN;
+	int rc = -ESHUTDOWN;
 
 	read_lock(&amp;ksocknal_data.ksnd_global_lock);
 	if (!conn-&gt;ksnc_closing) {
diff --git a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_cb.c b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_cb.c
index fa7ad883bda9..a1a4ac027fb5 100644
--- a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_cb.c
+++ b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_cb.c
@@ -75,13 +75,13 @@ ksocknal_alloc_tx_noop(__u64 cookie, int nonblk)
 		return NULL;
 	}
 
-	tx-&gt;tx_conn     = NULL;
-	tx-&gt;tx_lnetmsg  = NULL;
-	tx-&gt;tx_kiov     = NULL;
-	tx-&gt;tx_nkiov    = 0;
-	tx-&gt;tx_iov      = tx-&gt;tx_frags.virt.iov;
-	tx-&gt;tx_niov     = 1;
-	tx-&gt;tx_nonblk   = nonblk;
+	tx-&gt;tx_conn    = NULL;
+	tx-&gt;tx_lnetmsg = NULL;
+	tx-&gt;tx_kiov    = NULL;
+	tx-&gt;tx_nkiov   = 0;
+	tx-&gt;tx_iov     = tx-&gt;tx_frags.virt.iov;
+	tx-&gt;tx_niov    = 1;
+	tx-&gt;tx_nonblk  = nonblk;
 
 	socklnd_init_msg(&amp;tx-&gt;tx_msg, KSOCK_MSG_NOOP);
 	tx-&gt;tx_msg.ksm_zc_cookies[1] = cookie;
@@ -110,11 +110,11 @@ ksocknal_free_tx (ksock_tx_t *tx)
 static int
 ksocknal_send_iov (ksock_conn_t *conn, ksock_tx_t *tx)
 {
-	struct kvec  *iov = tx-&gt;tx_iov;
-	int    nob;
-	int    rc;
+	struct kvec *iov = tx-&gt;tx_iov;
+	int nob;
+	int rc;
 
-	LASSERT (tx-&gt;tx_niov &gt; 0);
+	LASSERT(tx-&gt;tx_niov &gt; 0);
 
 	/* Never touch tx-&gt;tx_iov inside ksocknal_lib_send_iov() */
 	rc = ksocknal_lib_send_iov(conn, tx);
@@ -128,7 +128,7 @@ ksocknal_send_iov (ksock_conn_t *conn, ksock_tx_t *tx)
 
 	/* "consume" iov */
 	do {
-		LASSERT (tx-&gt;tx_niov &gt; 0);
+		LASSERT(tx-&gt;tx_niov &gt; 0);
 
 		if (nob &lt; (int) iov-&gt;iov_len) {
 			iov-&gt;iov_base = (void *)((char *)iov-&gt;iov_base + nob);
@@ -147,12 +147,12 @@ ksocknal_send_iov (ksock_conn_t *conn, ksock_tx_t *tx)
 static int
 ksocknal_send_kiov (ksock_conn_t *conn, ksock_tx_t *tx)
 {
-	lnet_kiov_t    *kiov = tx-&gt;tx_kiov;
-	int     nob;
-	int     rc;
+	lnet_kiov_t *kiov = tx-&gt;tx_kiov;
+	int nob;
+	int rc;
 
-	LASSERT (tx-&gt;tx_niov == 0);
-	LASSERT (tx-&gt;tx_nkiov &gt; 0);
+	LASSERT(tx-&gt;tx_niov == 0);
+	LASSERT(tx-&gt;tx_nkiov &gt; 0);
 
 	/* Never touch tx-&gt;tx_kiov inside ksocknal_lib_send_kiov() */
 	rc = ksocknal_lib_send_kiov(conn, tx);
@@ -185,15 +185,15 @@ ksocknal_send_kiov (ksock_conn_t *conn, ksock_tx_t *tx)
 static int
 ksocknal_transmit (ksock_conn_t *conn, ksock_tx_t *tx)
 {
-	int      rc;
-	int      bufnob;
+	int rc;
+	int bufnob;
 
 	if (ksocknal_data.ksnd_stall_tx != 0) {
 		set_current_state(TASK_UNINTERRUPTIBLE);
 		schedule_timeout(cfs_time_seconds(ksocknal_data.ksnd_stall_tx));
 	}
 
-	LASSERT (tx-&gt;tx_resid != 0);
+	LASSERT(tx-&gt;tx_resid != 0);
 
 	rc = ksocknal_connsock_addref(conn);
 	if (rc != 0) {
@@ -252,10 +252,10 @@ static int
 ksocknal_recv_iov (ksock_conn_t *conn)
 {
 	struct kvec *iov = conn-&gt;ksnc_rx_iov;
-	int     nob;
-	int     rc;
+	int nob;
+	int rc;
 
-	LASSERT (conn-&gt;ksnc_rx_niov &gt; 0);
+	LASSERT(conn-&gt;ksnc_rx_niov &gt; 0);
 
 	/* Never touch conn-&gt;ksnc_rx_iov or change connection
 	 * status inside ksocknal_lib_recv_iov */
@@ -277,7 +277,7 @@ ksocknal_recv_iov (ksock_conn_t *conn)
 	conn-&gt;ksnc_rx_nob_left -= nob;
 
 	do {
-		LASSERT (conn-&gt;ksnc_rx_niov &gt; 0);
+		LASSERT(conn-&gt;ksnc_rx_niov &gt; 0);
 
 		if (nob &lt; (int)iov-&gt;iov_len) {
 			iov-&gt;iov_len -= nob;
@@ -296,10 +296,10 @@ ksocknal_recv_iov (ksock_conn_t *conn)
 static int
 ksocknal_recv_kiov (ksock_conn_t *conn)
 {
-	lnet_kiov_t   *kiov = conn-&gt;ksnc_rx_kiov;
-	int     nob;
-	int     rc;
-	LASSERT (conn-&gt;ksnc_rx_nkiov &gt; 0);
+	lnet_kiov_t *kiov = conn-&gt;ksnc_rx_kiov;
+	int nob;
+	int rc;
+	LASSERT(conn-&gt;ksnc_rx_nkiov &gt; 0);
 
 	/* Never touch conn-&gt;ksnc_rx_kiov or change connection
 	 * status inside ksocknal_lib_recv_iov */
@@ -321,7 +321,7 @@ ksocknal_recv_kiov (ksock_conn_t *conn)
 	conn-&gt;ksnc_rx_nob_left -= nob;
 
 	do {
-		LASSERT (conn-&gt;ksnc_rx_nkiov &gt; 0);
+		LASSERT(conn-&gt;ksnc_rx_nkiov &gt; 0);
 
 		if (nob &lt; (int) kiov-&gt;kiov_len) {
 			kiov-&gt;kiov_offset += nob;
@@ -343,7 +343,7 @@ ksocknal_receive (ksock_conn_t *conn)
 	/* Return 1 on success, 0 on EOF, &lt; 0 on error.
 	 * Caller checks ksnc_rx_nob_wanted to determine
 	 * progress/completion. */
-	int     rc;
+	int rc;
 
 	if (ksocknal_data.ksnd_stall_rx != 0) {
 		set_current_state(TASK_UNINTERRUPTIBLE);
@@ -388,8 +388,8 @@ ksocknal_receive (ksock_conn_t *conn)
 void
 ksocknal_tx_done (lnet_ni_t *ni, ksock_tx_t *tx)
 {
-	lnet_msg_t  *lnetmsg = tx-&gt;tx_lnetmsg;
-	int	  rc = (tx-&gt;tx_resid == 0 &amp;&amp; !tx-&gt;tx_zc_aborted) ? 0 : -EIO;
+	lnet_msg_t *lnetmsg = tx-&gt;tx_lnetmsg;
+	int rc = (tx-&gt;tx_resid == 0 &amp;&amp; !tx-&gt;tx_zc_aborted) ? 0 : -EIO;
 
 	LASSERT(ni != NULL || tx-&gt;tx_conn != NULL);
 
@@ -410,7 +410,7 @@ ksocknal_txlist_done (lnet_ni_t *ni, struct list_head *txlist, int error)
 	ksock_tx_t *tx;
 
 	while (!list_empty (txlist)) {
-		tx = list_entry (txlist-&gt;next, ksock_tx_t, tx_list);
+		tx = list_entry(txlist-&gt;next, ksock_tx_t, tx_list);
 
 		if (error &amp;&amp; tx-&gt;tx_lnetmsg != NULL) {
 			CNETERR("Deleting packet type %d len %d %s-&gt;%s\n",
@@ -422,18 +422,18 @@ ksocknal_txlist_done (lnet_ni_t *ni, struct list_head *txlist, int error)
 			CNETERR("Deleting noop packet\n");
 		}
 
-		list_del (&amp;tx-&gt;tx_list);
+		list_del(&amp;tx-&gt;tx_list);
 
-		LASSERT (atomic_read(&amp;tx-&gt;tx_refcount) == 1);
-		ksocknal_tx_done (ni, tx);
+		LASSERT(atomic_read(&amp;tx-&gt;tx_refcount) == 1);
+		ksocknal_tx_done(ni, tx);
 	}
 }
 
 static void
 ksocknal_check_zc_req(ksock_tx_t *tx)
 {
-	ksock_conn_t   *conn = tx-&gt;tx_conn;
-	ksock_peer_t   *peer = conn-&gt;ksnc_peer;
+	ksock_conn_t *conn = tx-&gt;tx_conn;
+	ksock_peer_t *peer = conn-&gt;ksnc_peer;
 
 	/* Set tx_msg.ksm_zc_cookies[0] to a unique non-zero cookie and add tx
 	 * to ksnp_zc_req_list if some fragment of this message should be sent
@@ -441,8 +441,8 @@ ksocknal_check_zc_req(ksock_tx_t *tx)
 	 * she has received this message to tell us we can signal completion.
 	 * tx_msg.ksm_zc_cookies[0] remains non-zero while tx is on
 	 * ksnp_zc_req_list. */
-	LASSERT (tx-&gt;tx_msg.ksm_type != KSOCK_MSG_NOOP);
-	LASSERT (tx-&gt;tx_zc_capable);
+	LASSERT(tx-&gt;tx_msg.ksm_type != KSOCK_MSG_NOOP);
+	LASSERT(tx-&gt;tx_zc_capable);
 
 	tx-&gt;tx_zc_checked = 1;
 
@@ -461,7 +461,7 @@ ksocknal_check_zc_req(ksock_tx_t *tx)
 	tx-&gt;tx_deadline =
 		cfs_time_shift(*ksocknal_tunables.ksnd_timeout);
 
-	LASSERT (tx-&gt;tx_msg.ksm_zc_cookies[0] == 0);
+	LASSERT(tx-&gt;tx_msg.ksm_zc_cookies[0] == 0);
 
 	tx-&gt;tx_msg.ksm_zc_cookies[0] = peer-&gt;ksnp_zc_next_cookie++;
 
@@ -476,7 +476,7 @@ ksocknal_check_zc_req(ksock_tx_t *tx)
 static void
 ksocknal_uncheck_zc_req(ksock_tx_t *tx)
 {
-	ksock_peer_t   *peer = tx-&gt;tx_conn-&gt;ksnc_peer;
+	ksock_peer_t *peer = tx-&gt;tx_conn-&gt;ksnc_peer;
 
 	LASSERT(tx-&gt;tx_msg.ksm_type != KSOCK_MSG_NOOP);
 	LASSERT(tx-&gt;tx_zc_capable);
@@ -502,14 +502,14 @@ ksocknal_uncheck_zc_req(ksock_tx_t *tx)
 static int
 ksocknal_process_transmit (ksock_conn_t *conn, ksock_tx_t *tx)
 {
-	int	    rc;
+	int rc;
 
 	if (tx-&gt;tx_zc_capable &amp;&amp; !tx-&gt;tx_zc_checked)
 		ksocknal_check_zc_req(tx);
 
 	rc = ksocknal_transmit (conn, tx);
 
-	CDEBUG (D_NET, "send(%d) %d\n", tx-&gt;tx_resid, rc);
+	CDEBUG(D_NET, "send(%d) %d\n", tx-&gt;tx_resid, rc);
 
 	if (tx-&gt;tx_resid == 0) {
 		/* Sent everything OK */
@@ -546,7 +546,7 @@ ksocknal_process_transmit (ksock_conn_t *conn, ksock_tx_t *tx)
 	}
 
 	/* Actual error */
-	LASSERT (rc &lt; 0);
+	LASSERT(rc &lt; 0);
 
 	if (!conn-&gt;ksnc_closing) {
 		switch (rc) {
@@ -582,9 +582,9 @@ ksocknal_launch_connection_locked (ksock_route_t *route)
 
 	/* called holding write lock on ksnd_global_lock */
 
-	LASSERT (!route-&gt;ksnr_scheduled);
-	LASSERT (!route-&gt;ksnr_connecting);
-	LASSERT ((ksocknal_route_mask() &amp; ~route-&gt;ksnr_connected) != 0);
+	LASSERT(!route-&gt;ksnr_scheduled);
+	LASSERT(!route-&gt;ksnr_connecting);
+	LASSERT((ksocknal_route_mask() &amp; ~route-&gt;ksnr_connected) != 0);
 
 	route-&gt;ksnr_scheduled = 1;	      /* scheduling conn for connd */
 	ksocknal_route_addref(route);	   /* extra ref for connd */
@@ -617,22 +617,22 @@ ksocknal_launch_all_connections_locked (ksock_peer_t *peer)
 ksock_conn_t *
 ksocknal_find_conn_locked(ksock_peer_t *peer, ksock_tx_t *tx, int nonblk)
 {
-	struct list_head       *tmp;
-	ksock_conn_t     *conn;
-	ksock_conn_t     *typed = NULL;
-	ksock_conn_t     *fallback = NULL;
-	int	       tnob     = 0;
-	int	       fnob     = 0;
+	struct list_head *tmp;
+	ksock_conn_t *conn;
+	ksock_conn_t *typed = NULL;
+	ksock_conn_t *fallback = NULL;
+	int tnob = 0;
+	int fnob = 0;
 
 	list_for_each (tmp, &amp;peer-&gt;ksnp_conns) {
 		ksock_conn_t *c  = list_entry(tmp, ksock_conn_t, ksnc_list);
-		int	   nob = atomic_read(&amp;c-&gt;ksnc_tx_nob) +
-				    c-&gt;ksnc_sock-&gt;sk-&gt;sk_wmem_queued;
-		int	   rc;
+		int nob = atomic_read(&amp;c-&gt;ksnc_tx_nob) +
+                                      c-&gt;ksnc_sock-&gt;sk-&gt;sk_wmem_queued;
+		int rc;
 
-		LASSERT (!c-&gt;ksnc_closing);
-		LASSERT (c-&gt;ksnc_proto != NULL &amp;&amp;
-			 c-&gt;ksnc_proto-&gt;pro_match_tx != NULL);
+		LASSERT(!c-&gt;ksnc_closing);
+		LASSERT(c-&gt;ksnc_proto != NULL &amp;&amp;
+			c-&gt;ksnc_proto-&gt;pro_match_tx != NULL);
 
 		rc = c-&gt;ksnc_proto-&gt;pro_match_tx(c, tx, nonblk);
 
@@ -656,7 +656,7 @@ ksocknal_find_conn_locked(ksock_peer_t *peer, ksock_tx_t *tx, int nonblk)
 			    (fnob == nob &amp;&amp; *ksocknal_tunables.ksnd_round_robin &amp;&amp;
 			     cfs_time_after(fallback-&gt;ksnc_tx_last_post, c-&gt;ksnc_tx_last_post))) {
 				fallback = c;
-				fnob     = nob;
+				fnob = nob;
 			}
 			break;
 		}
@@ -685,9 +685,9 @@ void
 ksocknal_queue_tx_locked (ksock_tx_t *tx, ksock_conn_t *conn)
 {
 	ksock_sched_t *sched = conn-&gt;ksnc_scheduler;
-	ksock_msg_t   *msg = &amp;tx-&gt;tx_msg;
-	ksock_tx_t    *ztx = NULL;
-	int	    bufnob = 0;
+	ksock_msg_t *msg = &amp;tx-&gt;tx_msg;
+	ksock_tx_t *ztx = NULL;
+	int bufnob = 0;
 
 	/* called holding global lock (read or irq-write) and caller may
 	 * not have dropped this lock between finding conn and calling me,
@@ -708,11 +708,11 @@ ksocknal_queue_tx_locked (ksock_tx_t *tx, ksock_conn_t *conn)
 	 *
 	 * We always expect at least 1 mapped fragment containing the
 	 * complete ksocknal message header. */
-	LASSERT (lnet_iov_nob (tx-&gt;tx_niov, tx-&gt;tx_iov) +
-		 lnet_kiov_nob(tx-&gt;tx_nkiov, tx-&gt;tx_kiov) ==
-		 (unsigned int)tx-&gt;tx_nob);
-	LASSERT (tx-&gt;tx_niov &gt;= 1);
-	LASSERT (tx-&gt;tx_resid == tx-&gt;tx_nob);
+	LASSERT(lnet_iov_nob (tx-&gt;tx_niov, tx-&gt;tx_iov) +
+		lnet_kiov_nob(tx-&gt;tx_nkiov, tx-&gt;tx_kiov) ==
+		(unsigned int)tx-&gt;tx_nob);
+	LASSERT(tx-&gt;tx_niov &gt;= 1);
+	LASSERT(tx-&gt;tx_resid == tx-&gt;tx_nob);
 
 	CDEBUG (D_NET, "Packet %p type %d, nob %d niov %d nkiov %d\n",
 		tx, (tx-&gt;tx_lnetmsg != NULL) ? tx-&gt;tx_lnetmsg-&gt;msg_hdr.type:
@@ -739,8 +739,8 @@ ksocknal_queue_tx_locked (ksock_tx_t *tx, ksock_conn_t *conn)
 	if (msg-&gt;ksm_type == KSOCK_MSG_NOOP) {
 		/* The packet is noop ZC ACK, try to piggyback the ack_cookie
 		 * on a normal packet so I don't need to send it */
-		LASSERT (msg-&gt;ksm_zc_cookies[1] != 0);
-		LASSERT (conn-&gt;ksnc_proto-&gt;pro_queue_tx_zcack != NULL);
+		LASSERT(msg-&gt;ksm_zc_cookies[1] != 0);
+		LASSERT(conn-&gt;ksnc_proto-&gt;pro_queue_tx_zcack != NULL);
 
 		if (conn-&gt;ksnc_proto-&gt;pro_queue_tx_zcack(conn, tx, 0))
 			ztx = tx; /* ZC ACK piggybacked on ztx release tx later */
@@ -748,8 +748,8 @@ ksocknal_queue_tx_locked (ksock_tx_t *tx, ksock_conn_t *conn)
 	} else {
 		/* It's a normal packet - can it piggback a noop zc-ack that
 		 * has been queued already? */
-		LASSERT (msg-&gt;ksm_zc_cookies[1] == 0);
-		LASSERT (conn-&gt;ksnc_proto-&gt;pro_queue_tx_msg != NULL);
+		LASSERT(msg-&gt;ksm_zc_cookies[1] == 0);
+		LASSERT(conn-&gt;ksnc_proto-&gt;pro_queue_tx_msg != NULL);
 
 		ztx = conn-&gt;ksnc_proto-&gt;pro_queue_tx_msg(conn, tx);
 		/* ztx will be released later */
@@ -777,14 +777,14 @@ ksocknal_queue_tx_locked (ksock_tx_t *tx, ksock_conn_t *conn)
 ksock_route_t *
 ksocknal_find_connectable_route_locked (ksock_peer_t *peer)
 {
-	unsigned long     now = cfs_time_current();
-	struct list_head    *tmp;
+	unsigned long now = cfs_time_current();
+	struct list_head *tmp;
 	ksock_route_t *route;
 
 	list_for_each (tmp, &amp;peer-&gt;ksnp_routes) {
 		route = list_entry (tmp, ksock_route_t, ksnr_list);
 
-		LASSERT (!route-&gt;ksnr_connecting || route-&gt;ksnr_scheduled);
+		LASSERT(!route-&gt;ksnr_connecting || route-&gt;ksnr_scheduled);
 
 		if (route-&gt;ksnr_scheduled)      /* connections being established */
 			continue;
@@ -813,13 +813,13 @@ ksocknal_find_connectable_route_locked (ksock_peer_t *peer)
 ksock_route_t *
 ksocknal_find_connecting_route_locked (ksock_peer_t *peer)
 {
-	struct list_head	*tmp;
-	ksock_route_t     *route;
+	struct list_head *tmp;
+	ksock_route_t *route;
 
 	list_for_each (tmp, &amp;peer-&gt;ksnp_routes) {
 		route = list_entry (tmp, ksock_route_t, ksnr_list);
 
-		LASSERT (!route-&gt;ksnr_connecting || route-&gt;ksnr_scheduled);
+		LASSERT(!route-&gt;ksnr_connecting || route-&gt;ksnr_scheduled);
 
 		if (route-&gt;ksnr_scheduled)
 			return route;
@@ -831,13 +831,13 @@ ksocknal_find_connecting_route_locked (ksock_peer_t *peer)
 int
 ksocknal_launch_packet (lnet_ni_t *ni, ksock_tx_t *tx, lnet_process_id_t id)
 {
-	ksock_peer_t     *peer;
-	ksock_conn_t     *conn;
-	rwlock_t     *g_lock;
-	int	       retry;
-	int	       rc;
+	ksock_peer_t *peer;
+	ksock_conn_t *conn;
+	rwlock_t *g_lock;
+	int retry;
+	int rc;
 
-	LASSERT (tx-&gt;tx_conn == NULL);
+	LASSERT(tx-&gt;tx_conn == NULL);
 
 	g_lock = &amp;ksocknal_data.ksnd_global_lock;
 
@@ -922,17 +922,17 @@ ksocknal_launch_packet (lnet_ni_t *ni, ksock_tx_t *tx, lnet_process_id_t id)
 int
 ksocknal_send(lnet_ni_t *ni, void *private, lnet_msg_t *lntmsg)
 {
-	int	       mpflag = 1;
-	int	       type = lntmsg-&gt;msg_type;
+	int mpflag = 1;
+	int type = lntmsg-&gt;msg_type;
 	lnet_process_id_t target = lntmsg-&gt;msg_target;
-	unsigned int      payload_niov = lntmsg-&gt;msg_niov;
-	struct kvec      *payload_iov = lntmsg-&gt;msg_iov;
-	lnet_kiov_t      *payload_kiov = lntmsg-&gt;msg_kiov;
-	unsigned int      payload_offset = lntmsg-&gt;msg_offset;
-	unsigned int      payload_nob = lntmsg-&gt;msg_len;
-	ksock_tx_t       *tx;
-	int	       desc_size;
-	int	       rc;
+	unsigned int payload_niov = lntmsg-&gt;msg_niov;
+	struct kvec *payload_iov = lntmsg-&gt;msg_iov;
+	lnet_kiov_t *payload_kiov = lntmsg-&gt;msg_kiov;
+	unsigned int payload_offset = lntmsg-&gt;msg_offset;
+	unsigned int payload_nob = lntmsg-&gt;msg_len;
+	ksock_tx_t *tx;
+	int desc_size;
+	int rc;
 
 	/* NB 'private' is different depending on what we're sending.
 	 * Just ignore it... */
@@ -940,8 +940,8 @@ ksocknal_send(lnet_ni_t *ni, void *private, lnet_msg_t *lntmsg)
 	CDEBUG(D_NET, "sending %u bytes in %d frags to %s\n",
 	       payload_nob, payload_niov, libcfs_id2str(target));
 
-	LASSERT (payload_nob == 0 || payload_niov &gt; 0);
-	LASSERT (payload_niov &lt;= LNET_MAX_IOV);
+	LASSERT(payload_nob == 0 || payload_niov &gt; 0);
+	LASSERT(payload_niov &lt;= LNET_MAX_IOV);
 	/* payload is either all vaddrs or all pages */
 	LASSERT (!(payload_kiov != NULL &amp;&amp; payload_iov != NULL));
 	LASSERT (!in_interrupt ());
@@ -1028,9 +1028,9 @@ ksocknal_new_packet (ksock_conn_t *conn, int nob_to_skip)
 {
 	static char ksocknal_slop_buffer[4096];
 
-	int	    nob;
-	unsigned int   niov;
-	int	    skipped;
+	int nob;
+	unsigned int niov;
+	int skipped;
 
 	LASSERT(conn-&gt;ksnc_proto != NULL);
 
@@ -1063,7 +1063,7 @@ ksocknal_new_packet (ksock_conn_t *conn, int nob_to_skip)
 
 			conn-&gt;ksnc_rx_iov = (struct kvec *)&amp;conn-&gt;ksnc_rx_iov_space;
 			conn-&gt;ksnc_rx_iov[0].iov_base = &amp;conn-&gt;ksnc_msg.ksm_u.lnetmsg;
-			conn-&gt;ksnc_rx_iov[0].iov_len  = sizeof (lnet_hdr_t);
+			conn-&gt;ksnc_rx_iov[0].iov_len = sizeof (lnet_hdr_t);
 			break;
 
 		default:
@@ -1108,18 +1108,18 @@ ksocknal_new_packet (ksock_conn_t *conn, int nob_to_skip)
 static int
 ksocknal_process_receive (ksock_conn_t *conn)
 {
-	lnet_hdr_t	*lhdr;
+	lnet_hdr_t *lhdr;
 	lnet_process_id_t *id;
-	int		rc;
+	int rc;
 
 	LASSERT (atomic_read(&amp;conn-&gt;ksnc_conn_refcount) &gt; 0);
 
 	/* NB: sched lock NOT held */
 	/* SOCKNAL_RX_LNET_HEADER is here for backward compatibility */
-	LASSERT (conn-&gt;ksnc_rx_state == SOCKNAL_RX_KSM_HEADER ||
-		 conn-&gt;ksnc_rx_state == SOCKNAL_RX_LNET_PAYLOAD ||
-		 conn-&gt;ksnc_rx_state == SOCKNAL_RX_LNET_HEADER ||
-		 conn-&gt;ksnc_rx_state == SOCKNAL_RX_SLOP);
+	LASSERT(conn-&gt;ksnc_rx_state == SOCKNAL_RX_KSM_HEADER ||
+		conn-&gt;ksnc_rx_state == SOCKNAL_RX_LNET_PAYLOAD ||
+		conn-&gt;ksnc_rx_state == SOCKNAL_RX_LNET_HEADER ||
+		conn-&gt;ksnc_rx_state == SOCKNAL_RX_SLOP);
  again:
 	if (conn-&gt;ksnc_rx_nob_wanted != 0) {
 		rc = ksocknal_receive(conn);
@@ -1229,7 +1229,7 @@ ksocknal_process_receive (ksock_conn_t *conn)
 		if ((conn-&gt;ksnc_peer-&gt;ksnp_id.pid &amp; LNET_PID_USERFLAG) != 0) {
 			/* Userspace peer */
 			lhdr = &amp;conn-&gt;ksnc_msg.ksm_u.lnetmsg.ksnm_hdr;
-			id   = &amp;conn-&gt;ksnc_peer-&gt;ksnp_id;
+			id = &amp;conn-&gt;ksnc_peer-&gt;ksnp_id;
 
 			/* Substitute process ID assigned at connection time */
 			lhdr-&gt;src_pid = cpu_to_le32(id-&gt;pid);
@@ -1277,7 +1277,7 @@ ksocknal_process_receive (ksock_conn_t *conn)
 			LASSERT(conn-&gt;ksnc_proto != &amp;ksocknal_protocol_v1x);
 
 			lhdr = &amp;conn-&gt;ksnc_msg.ksm_u.lnetmsg.ksnm_hdr;
-			id   = &amp;conn-&gt;ksnc_peer-&gt;ksnp_id;
+			id = &amp;conn-&gt;ksnc_peer-&gt;ksnp_id;
 
 			rc = conn-&gt;ksnc_proto-&gt;pro_handle_zcreq(conn,
 					conn-&gt;ksnc_msg.ksm_zc_cookies[0],
@@ -1305,7 +1305,7 @@ ksocknal_process_receive (ksock_conn_t *conn)
 	}
 
 	/* Not Reached */
-	LBUG ();
+	LBUG();
 	return -EINVAL;		       /* keep gcc happy */
 }
 
@@ -1314,15 +1314,15 @@ ksocknal_recv (lnet_ni_t *ni, void *private, lnet_msg_t *msg, int delayed,
 	       unsigned int niov, struct kvec *iov, lnet_kiov_t *kiov,
 	       unsigned int offset, unsigned int mlen, unsigned int rlen)
 {
-	ksock_conn_t  *conn = (ksock_conn_t *)private;
+	ksock_conn_t *conn = (ksock_conn_t *)private;
 	ksock_sched_t *sched = conn-&gt;ksnc_scheduler;
 
-	LASSERT (mlen &lt;= rlen);
-	LASSERT (niov &lt;= LNET_MAX_IOV);
+	LASSERT(mlen &lt;= rlen);
+	LASSERT(niov &lt;= LNET_MAX_IOV);
 
 	conn-&gt;ksnc_cookie = msg;
 	conn-&gt;ksnc_rx_nob_wanted = mlen;
-	conn-&gt;ksnc_rx_nob_left   = rlen;
+	conn-&gt;ksnc_rx_nob_left = rlen;
 
 	if (mlen == 0 || iov != NULL) {
 		conn-&gt;ksnc_rx_nkiov = 0;
@@ -1333,18 +1333,18 @@ ksocknal_recv (lnet_ni_t *ni, void *private, lnet_msg_t *msg, int delayed,
 					 niov, iov, offset, mlen);
 	} else {
 		conn-&gt;ksnc_rx_niov = 0;
-		conn-&gt;ksnc_rx_iov  = NULL;
+		conn-&gt;ksnc_rx_iov = NULL;
 		conn-&gt;ksnc_rx_kiov = conn-&gt;ksnc_rx_iov_space.kiov;
 		conn-&gt;ksnc_rx_nkiov =
 			lnet_extract_kiov(LNET_MAX_IOV, conn-&gt;ksnc_rx_kiov,
 					  niov, kiov, offset, mlen);
 	}
 
-	LASSERT (mlen ==
-		 lnet_iov_nob (conn-&gt;ksnc_rx_niov, conn-&gt;ksnc_rx_iov) +
-		 lnet_kiov_nob (conn-&gt;ksnc_rx_nkiov, conn-&gt;ksnc_rx_kiov));
+	LASSERT(mlen ==
+		lnet_iov_nob(conn-&gt;ksnc_rx_niov, conn-&gt;ksnc_rx_iov) +
+		lnet_kiov_nob(conn-&gt;ksnc_rx_nkiov, conn-&gt;ksnc_rx_kiov));
 
-	LASSERT (conn-&gt;ksnc_rx_scheduled);
+	LASSERT(conn-&gt;ksnc_rx_scheduled);
 
 	spin_lock_bh(&amp;sched-&gt;kss_lock);
 
@@ -1370,7 +1370,7 @@ ksocknal_recv (lnet_ni_t *ni, void *private, lnet_msg_t *msg, int delayed,
 static inline int
 ksocknal_sched_cansleep(ksock_sched_t *sched)
 {
-	int	   rc;
+	int rc;
 
 	spin_lock_bh(&amp;sched-&gt;kss_lock);
 
@@ -1384,13 +1384,13 @@ ksocknal_sched_cansleep(ksock_sched_t *sched)
 
 int ksocknal_scheduler(void *arg)
 {
-	struct ksock_sched_info	*info;
-	ksock_sched_t		*sched;
-	ksock_conn_t		*conn;
-	ksock_tx_t		*tx;
-	int			rc;
-	int			nloops = 0;
-	long			id = (long)arg;
+	struct ksock_sched_info *info;
+	ksock_sched_t *sched;
+	ksock_conn_t *conn;
+	ksock_tx_t *tx;
+	int rc;
+	int nloops = 0;
+	long id = (long)arg;
 
 	info = ksocknal_data.ksnd_sched_info[KSOCK_THREAD_CPT(id)];
 	sched = &amp;info-&gt;ksi_scheds[KSOCK_THREAD_SID(id)];
@@ -1455,7 +1455,7 @@ int ksocknal_scheduler(void *arg)
 		}
 
 		if (!list_empty (&amp;sched-&gt;kss_tx_conns)) {
-			LIST_HEAD    (zlist);
+			LIST_HEAD(zlist);
 
 			if (!list_empty(&amp;sched-&gt;kss_zombie_noop_txs)) {
 				list_add(&amp;zlist,
@@ -1513,9 +1513,9 @@ int ksocknal_scheduler(void *arg)
 				/* Do nothing; after a short timeout, this
 				 * conn will be reposted on kss_tx_conns. */
 			} else if (conn-&gt;ksnc_tx_ready &amp;&amp;
-				   !list_empty (&amp;conn-&gt;ksnc_tx_queue)) {
+				   !list_empty(&amp;conn-&gt;ksnc_tx_queue)) {
 				/* reschedule for tx */
-				list_add_tail (&amp;conn-&gt;ksnc_tx_list,
+				list_add_tail(&amp;conn-&gt;ksnc_tx_list,
 						   &amp;sched-&gt;kss_tx_conns);
 			} else {
 				conn-&gt;ksnc_tx_scheduled = 0;
@@ -1606,7 +1606,7 @@ void ksocknal_write_callback (ksock_conn_t *conn)
 static ksock_proto_t *
 ksocknal_parse_proto_version (ksock_hello_msg_t *hello)
 {
-	__u32   version = 0;
+	__u32 version = 0;
 
 	if (hello-&gt;kshm_magic == LNET_PROTO_MAGIC)
 		version = hello-&gt;kshm_version;
@@ -1634,8 +1634,8 @@ ksocknal_parse_proto_version (ksock_hello_msg_t *hello)
 	if (hello-&gt;kshm_magic == le32_to_cpu(LNET_PROTO_TCP_MAGIC)) {
 		lnet_magicversion_t *hmv = (lnet_magicversion_t *)hello;
 
-		CLASSERT (sizeof (lnet_magicversion_t) ==
-			  offsetof (ksock_hello_msg_t, kshm_src_nid));
+		CLASSERT(sizeof (lnet_magicversion_t) ==
+			 offsetof (ksock_hello_msg_t, kshm_src_nid));
 
 		if (hmv-&gt;version_major == cpu_to_le16 (KSOCK_PROTO_V1_MAJOR) &amp;&amp;
 		    hmv-&gt;version_minor == cpu_to_le16 (KSOCK_PROTO_V1_MINOR))
@@ -1650,19 +1650,19 @@ ksocknal_send_hello (lnet_ni_t *ni, ksock_conn_t *conn,
 		     lnet_nid_t peer_nid, ksock_hello_msg_t *hello)
 {
 	/* CAVEAT EMPTOR: this byte flips 'ipaddrs' */
-	ksock_net_t	 *net = (ksock_net_t *)ni-&gt;ni_data;
+	ksock_net_t *net = (ksock_net_t *)ni-&gt;ni_data;
 
-	LASSERT (hello-&gt;kshm_nips &lt;= LNET_MAX_INTERFACES);
+	LASSERT(hello-&gt;kshm_nips &lt;= LNET_MAX_INTERFACES);
 
 	/* rely on caller to hold a ref on socket so it wouldn't disappear */
-	LASSERT (conn-&gt;ksnc_proto != NULL);
+	LASSERT(conn-&gt;ksnc_proto != NULL);
 
-	hello-&gt;kshm_src_nid	 = ni-&gt;ni_nid;
-	hello-&gt;kshm_dst_nid	 = peer_nid;
-	hello-&gt;kshm_src_pid	 = the_lnet.ln_pid;
+	hello-&gt;kshm_src_nid = ni-&gt;ni_nid;
+	hello-&gt;kshm_dst_nid = peer_nid;
+	hello-&gt;kshm_src_pid = the_lnet.ln_pid;
 
 	hello-&gt;kshm_src_incarnation = net-&gt;ksnn_incarnation;
-	hello-&gt;kshm_ctype	   = conn-&gt;ksnc_type;
+	hello-&gt;kshm_ctype = conn-&gt;ksnc_type;
 
 	return conn-&gt;ksnc_proto-&gt;pro_send_hello(conn, hello);
 }
@@ -1693,16 +1693,16 @@ ksocknal_recv_hello (lnet_ni_t *ni, ksock_conn_t *conn,
 	 *	EALREADY   lost connection race
 	 *	EPROTO     protocol version mismatch
 	 */
-	struct socket	*sock = conn-&gt;ksnc_sock;
-	int		  active = (conn-&gt;ksnc_proto != NULL);
-	int		  timeout;
-	int		  proto_match;
-	int		  rc;
-	ksock_proto_t       *proto;
-	lnet_process_id_t    recv_id;
+	struct socket *sock = conn-&gt;ksnc_sock;
+	int active = (conn-&gt;ksnc_proto != NULL);
+	int timeout;
+	int proto_match;
+	int rc;
+	ksock_proto_t *proto;
+	lnet_process_id_t recv_id;
 
 	/* socket type set on active connections - not set on passive */
-	LASSERT (!active == !(conn-&gt;ksnc_type != SOCKLND_CONN_NONE));
+	LASSERT(!active == !(conn-&gt;ksnc_type != SOCKLND_CONN_NONE));
 
 	timeout = active ? *ksocknal_tunables.ksnd_timeout :
 			    lnet_acceptor_timeout();
@@ -1731,7 +1731,7 @@ ksocknal_recv_hello (lnet_ni_t *ni, ksock_conn_t *conn,
 	if (rc != 0) {
 		CERROR("Error %d reading HELLO from %pI4h\n",
 			rc, &amp;conn-&gt;ksnc_ipaddr);
-		LASSERT (rc &lt; 0);
+		LASSERT(rc &lt; 0);
 		return rc;
 	}
 
@@ -1765,7 +1765,7 @@ ksocknal_recv_hello (lnet_ni_t *ni, ksock_conn_t *conn,
 	if (rc != 0) {
 		CERROR("Error %d reading or checking hello from from %pI4h\n",
 		       rc, &amp;conn-&gt;ksnc_ipaddr);
-		LASSERT (rc &lt; 0);
+		LASSERT(rc &lt; 0);
 		return rc;
 	}
 
@@ -1830,22 +1830,22 @@ ksocknal_recv_hello (lnet_ni_t *ni, ksock_conn_t *conn,
 static int
 ksocknal_connect (ksock_route_t *route)
 {
-	LIST_HEAD    (zombies);
-	ksock_peer_t     *peer = route-&gt;ksnr_peer;
-	int	       type;
-	int	       wanted;
-	struct socket     *sock;
-	unsigned long	deadline;
-	int	       retry_later = 0;
-	int	       rc = 0;
+	LIST_HEAD(zombies);
+	ksock_peer_t *peer = route-&gt;ksnr_peer;
+	int type;
+	int wanted;
+	struct socket *sock;
+	unsigned long deadline;
+	int retry_later = 0;
+	int rc = 0;
 
 	deadline = cfs_time_add(cfs_time_current(),
 				cfs_time_seconds(*ksocknal_tunables.ksnd_timeout));
 
 	write_lock_bh(&amp;ksocknal_data.ksnd_global_lock);
 
-	LASSERT (route-&gt;ksnr_scheduled);
-	LASSERT (!route-&gt;ksnr_connecting);
+	LASSERT(route-&gt;ksnr_scheduled);
+	LASSERT(!route-&gt;ksnr_connecting);
 
 	route-&gt;ksnr_connecting = 1;
 
@@ -2101,7 +2101,7 @@ static ksock_route_t *
 ksocknal_connd_get_route_locked(signed long *timeout_p)
 {
 	ksock_route_t *route;
-	unsigned long     now;
+	unsigned long now;
 
 	now = cfs_time_current();
 
@@ -2124,13 +2124,13 @@ ksocknal_connd_get_route_locked(signed long *timeout_p)
 int
 ksocknal_connd (void *arg)
 {
-	spinlock_t    *connd_lock = &amp;ksocknal_data.ksnd_connd_lock;
-	ksock_connreq_t   *cr;
-	wait_queue_t     wait;
-	int		nloops = 0;
-	int		cons_retry = 0;
+	spinlock_t *connd_lock = &amp;ksocknal_data.ksnd_connd_lock;
+	ksock_connreq_t *cr;
+	wait_queue_t wait;
+	int nloops = 0;
+	int cons_retry = 0;
 
-	cfs_block_allsigs ();
+	cfs_block_allsigs();
 
 	init_waitqueue_entry(&amp;wait, current);
 
@@ -2144,7 +2144,7 @@ ksocknal_connd (void *arg)
 		ksock_route_t *route = NULL;
 		long sec = get_seconds();
 		long timeout = MAX_SCHEDULE_TIMEOUT;
-		int  dropped_lock = 0;
+		int dropped_lock = 0;
 
 		if (ksocknal_connd_check_stop(sec, &amp;timeout)) {
 			/* wakeup another one to check stop */
@@ -2236,15 +2236,15 @@ static ksock_conn_t *
 ksocknal_find_timed_out_conn (ksock_peer_t *peer)
 {
 	/* We're called with a shared lock on ksnd_global_lock */
-	ksock_conn_t      *conn;
-	struct list_head	*ctmp;
+	ksock_conn_t *conn;
+	struct list_head *ctmp;
 
 	list_for_each (ctmp, &amp;peer-&gt;ksnp_conns) {
-		int     error;
+		int error;
 		conn = list_entry (ctmp, ksock_conn_t, ksnc_list);
 
 		/* Don't need the {get,put}connsock dance to deref ksnc_sock */
-		LASSERT (!conn-&gt;ksnc_closing);
+		LASSERT(!conn-&gt;ksnc_closing);
 
 		/* SOCK_ERROR will reset error code of socket in
 		 * some platform (like Darwin8.x) */
@@ -2313,8 +2313,8 @@ ksocknal_find_timed_out_conn (ksock_peer_t *peer)
 static inline void
 ksocknal_flush_stale_txs(ksock_peer_t *peer)
 {
-	ksock_tx_t	*tx;
-	LIST_HEAD      (stale_txs);
+	ksock_tx_t *tx;
+	LIST_HEAD(stale_txs);
 
 	write_lock_bh(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -2338,9 +2338,9 @@ ksocknal_flush_stale_txs(ksock_peer_t *peer)
 static int
 ksocknal_send_keepalive_locked(ksock_peer_t *peer)
 {
-	ksock_sched_t  *sched;
-	ksock_conn_t   *conn;
-	ksock_tx_t     *tx;
+	ksock_sched_t *sched;
+	ksock_conn_t *conn;
+	ksock_tx_t *tx;
 
 	if (list_empty(&amp;peer-&gt;ksnp_conns)) /* last_alive will be updated by create_conn */
 		return 0;
@@ -2399,10 +2399,10 @@ ksocknal_send_keepalive_locked(ksock_peer_t *peer)
 static void
 ksocknal_check_peer_timeouts (int idx)
 {
-	struct list_head       *peers = &amp;ksocknal_data.ksnd_peers[idx];
-	ksock_peer_t     *peer;
-	ksock_conn_t     *conn;
-	ksock_tx_t       *tx;
+	struct list_head *peers = &amp;ksocknal_data.ksnd_peers[idx];
+	ksock_peer_t *peer;
+	ksock_conn_t *conn;
+	ksock_tx_t *tx;
 
  again:
 	/* NB. We expect to have a look at all the peers and not find any
@@ -2411,9 +2411,9 @@ ksocknal_check_peer_timeouts (int idx)
 	read_lock(&amp;ksocknal_data.ksnd_global_lock);
 
 	list_for_each_entry(peer, peers, ksnp_list) {
-		unsigned long  deadline = 0;
-		int	 resid = 0;
-		int	 n     = 0;
+		unsigned long deadline = 0;
+		int resid = 0;
+		int n = 0;
 
 		if (ksocknal_send_keepalive_locked(peer) != 0) {
 			read_unlock(&amp;ksocknal_data.ksnd_global_lock);
@@ -2476,8 +2476,8 @@ ksocknal_check_peer_timeouts (int idx)
 		tx = list_entry(peer-&gt;ksnp_zc_req_list.next,
 				    ksock_tx_t, tx_zc_list);
 		deadline = tx-&gt;tx_deadline;
-		resid    = tx-&gt;tx_resid;
-		conn     = tx-&gt;tx_conn;
+		resid = tx-&gt;tx_resid;
+		conn = tx-&gt;tx_conn;
 		ksocknal_conn_addref(conn);
 
 		spin_unlock(&amp;peer-&gt;ksnp_lock);
@@ -2499,17 +2499,17 @@ ksocknal_check_peer_timeouts (int idx)
 int
 ksocknal_reaper (void *arg)
 {
-	wait_queue_t     wait;
-	ksock_conn_t      *conn;
-	ksock_sched_t     *sched;
-	struct list_head	 enomem_conns;
-	int		nenomem_conns;
-	long     timeout;
-	int		i;
-	int		peer_index = 0;
-	unsigned long	 deadline = cfs_time_current();
-
-	cfs_block_allsigs ();
+	wait_queue_t wait;
+	ksock_conn_t *conn;
+	ksock_sched_t *sched;
+	struct list_head enomem_conns;
+	int nenomem_conns;
+	long timeout;
+	int i;
+	int peer_index = 0;
+	unsigned long deadline = cfs_time_current();
+
+	cfs_block_allsigs();
 
 	INIT_LIST_HEAD(&amp;enomem_conns);
 	init_waitqueue_entry(&amp;wait, current);
@@ -2580,7 +2580,7 @@ ksocknal_reaper (void *arg)
 					       cfs_time_current())) &lt;= 0) {
 			const int n = 4;
 			const int p = 1;
-			int       chunk = ksocknal_data.ksnd_peer_hash_size;
+			int chunk = ksocknal_data.ksnd_peer_hash_size;
 
 			/* Time to check for timeouts on a few more peers: I do
 			 * checks every 'p' seconds on a proportion of the peer
diff --git a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_lib-linux.c b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_lib-linux.c
index f5e8ab06070c..caeb3477da97 100644
--- a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_lib-linux.c
+++ b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_lib-linux.c
@@ -64,7 +64,7 @@ ksocknal_lib_get_conn_addrs(ksock_conn_t *conn)
 int
 ksocknal_lib_zc_capable(ksock_conn_t *conn)
 {
-	int  caps = conn-&gt;ksnc_sock-&gt;sk-&gt;sk_route_caps;
+	int caps = conn-&gt;ksnc_sock-&gt;sk-&gt;sk_route_caps;
 
 	if (conn-&gt;ksnc_proto == &amp;ksocknal_protocol_v1x)
 		return 0;
@@ -78,8 +78,8 @@ int
 ksocknal_lib_send_iov(ksock_conn_t *conn, ksock_tx_t *tx)
 {
 	struct socket *sock = conn-&gt;ksnc_sock;
-	int	    nob;
-	int	    rc;
+	int nob;
+	int rc;
 
 	if (*ksocknal_tunables.ksnd_enable_csum	&amp;&amp; /* checksum enabled */
 	    conn-&gt;ksnc_proto == &amp;ksocknal_protocol_v2x &amp;&amp; /* V2.x connection  */
@@ -92,15 +92,15 @@ ksocknal_lib_send_iov(ksock_conn_t *conn, ksock_tx_t *tx)
 
 	{
 #if SOCKNAL_SINGLE_FRAG_TX
-		struct kvec    scratch;
-		struct kvec   *scratchiov = &amp;scratch;
-		unsigned int    niov = 1;
+		struct kvec scratch;
+		struct kvec *scratchiov = &amp;scratch;
+		unsigned int niov = 1;
 #else
-		struct kvec   *scratchiov = conn-&gt;ksnc_scheduler-&gt;kss_scratch_iov;
-		unsigned int    niov = tx-&gt;tx_niov;
+		struct kvec *scratchiov = conn-&gt;ksnc_scheduler-&gt;kss_scratch_iov;
+		unsigned int niov = tx-&gt;tx_niov;
 #endif
 		struct msghdr msg = {.msg_flags = MSG_DONTWAIT};
-		int  i;
+		int i;
 
 		for (nob = i = 0; i &lt; niov; i++) {
 			scratchiov[i] = tx-&gt;tx_iov[i];
@@ -120,9 +120,9 @@ int
 ksocknal_lib_send_kiov(ksock_conn_t *conn, ksock_tx_t *tx)
 {
 	struct socket *sock = conn-&gt;ksnc_sock;
-	lnet_kiov_t   *kiov = tx-&gt;tx_kiov;
-	int	    rc;
-	int	    nob;
+	lnet_kiov_t *kiov = tx-&gt;tx_kiov;
+	int rc;
+	int nob;
 
 	/* Not NOOP message */
 	LASSERT(tx-&gt;tx_lnetmsg != NULL);
@@ -131,11 +131,11 @@ ksocknal_lib_send_kiov(ksock_conn_t *conn, ksock_tx_t *tx)
 	 * or leave them alone. */
 	if (tx-&gt;tx_msg.ksm_zc_cookies[0] != 0) {
 		/* Zero copy is enabled */
-		struct sock   *sk = sock-&gt;sk;
-		struct page   *page = kiov-&gt;kiov_page;
-		int	    offset = kiov-&gt;kiov_offset;
-		int	    fragsize = kiov-&gt;kiov_len;
-		int	    msgflg = MSG_DONTWAIT;
+		struct sock *sk = sock-&gt;sk;
+		struct page *page = kiov-&gt;kiov_page;
+		int offset = kiov-&gt;kiov_offset;
+		int fragsize = kiov-&gt;kiov_len;
+		int msgflg = MSG_DONTWAIT;
 
 		CDEBUG(D_NET, "page %p + offset %x for %d\n",
 			       page, offset, kiov-&gt;kiov_len);
@@ -153,18 +153,18 @@ ksocknal_lib_send_kiov(ksock_conn_t *conn, ksock_tx_t *tx)
 		}
 	} else {
 #if SOCKNAL_SINGLE_FRAG_TX || !SOCKNAL_RISK_KMAP_DEADLOCK
-		struct kvec  scratch;
+		struct kvec scratch;
 		struct kvec *scratchiov = &amp;scratch;
-		unsigned int  niov = 1;
+		unsigned int niov = 1;
 #else
 #ifdef CONFIG_HIGHMEM
 #warning "XXX risk of kmap deadlock on multiple frags..."
 #endif
 		struct kvec *scratchiov = conn-&gt;ksnc_scheduler-&gt;kss_scratch_iov;
-		unsigned int  niov = tx-&gt;tx_nkiov;
+		unsigned int niov = tx-&gt;tx_nkiov;
 #endif
 		struct msghdr msg = {.msg_flags = MSG_DONTWAIT};
-		int	   i;
+		int i;
 
 		for (nob = i = 0; i &lt; niov; i++) {
 			scratchiov[i].iov_base = kmap(kiov[i].kiov_page) +
@@ -187,7 +187,7 @@ ksocknal_lib_send_kiov(ksock_conn_t *conn, ksock_tx_t *tx)
 void
 ksocknal_lib_eager_ack(ksock_conn_t *conn)
 {
-	int	    opt = 1;
+	int opt = 1;
 	struct socket *sock = conn-&gt;ksnc_sock;
 
 	/* Remind the socket to ACK eagerly.  If I don't, the socket might
@@ -203,23 +203,23 @@ int
 ksocknal_lib_recv_iov(ksock_conn_t *conn)
 {
 #if SOCKNAL_SINGLE_FRAG_RX
-	struct kvec  scratch;
+	struct kvec scratch;
 	struct kvec *scratchiov = &amp;scratch;
-	unsigned int  niov = 1;
+	unsigned int niov = 1;
 #else
 	struct kvec *scratchiov = conn-&gt;ksnc_scheduler-&gt;kss_scratch_iov;
-	unsigned int  niov = conn-&gt;ksnc_rx_niov;
+	unsigned int niov = conn-&gt;ksnc_rx_niov;
 #endif
 	struct kvec *iov = conn-&gt;ksnc_rx_iov;
 	struct msghdr msg = {
-		.msg_flags      = 0
+		.msg_flags = 0
 	};
-	int	  nob;
-	int	  i;
-	int	  rc;
-	int	  fragnob;
-	int	  sum;
-	__u32	saved_csum;
+	int nob;
+	int i;
+	int rc;
+	int fragnob;
+	int sum;
+	__u32 saved_csum;
 
 	/* NB we can't trust socket ops to either consume our iovs
 	 * or leave them alone. */
@@ -271,9 +271,9 @@ static void *
 ksocknal_lib_kiov_vmap(lnet_kiov_t *kiov, int niov,
 		       struct kvec *iov, struct page **pages)
 {
-	void	     *addr;
-	int	       nob;
-	int	       i;
+	void *addr;
+	int nob;
+	int i;
 
 	if (!*ksocknal_tunables.ksnd_zc_recv || pages == NULL)
 		return NULL;
@@ -307,29 +307,29 @@ int
 ksocknal_lib_recv_kiov(ksock_conn_t *conn)
 {
 #if SOCKNAL_SINGLE_FRAG_RX || !SOCKNAL_RISK_KMAP_DEADLOCK
-	struct kvec   scratch;
-	struct kvec  *scratchiov = &amp;scratch;
-	struct page  **pages      = NULL;
-	unsigned int   niov       = 1;
+	struct kvec scratch;
+	struct kvec *scratchiov = &amp;scratch;
+	struct page **pages = NULL;
+	unsigned int niov = 1;
 #else
 #ifdef CONFIG_HIGHMEM
 #warning "XXX risk of kmap deadlock on multiple frags..."
 #endif
-	struct kvec  *scratchiov = conn-&gt;ksnc_scheduler-&gt;kss_scratch_iov;
-	struct page  **pages      = conn-&gt;ksnc_scheduler-&gt;kss_rx_scratch_pgs;
-	unsigned int   niov       = conn-&gt;ksnc_rx_nkiov;
+	struct kvec *scratchiov = conn-&gt;ksnc_scheduler-&gt;kss_scratch_iov;
+	struct page **pages = conn-&gt;ksnc_scheduler-&gt;kss_rx_scratch_pgs;
+	unsigned int niov = conn-&gt;ksnc_rx_nkiov;
 #endif
 	lnet_kiov_t   *kiov = conn-&gt;ksnc_rx_kiov;
 	struct msghdr msg = {
-		.msg_flags      = 0
+		.msg_flags = 0
 	};
-	int	  nob;
-	int	  i;
-	int	  rc;
-	void	*base;
-	void	*addr;
-	int	  sum;
-	int	  fragnob;
+	int nob;
+	int i;
+	int rc;
+	void *base;
+	void *addr;
+	int sum;
+	int fragnob;
 	int n;
 
 	/* NB we can't trust socket ops to either consume our iovs
@@ -357,10 +357,10 @@ ksocknal_lib_recv_kiov(ksock_conn_t *conn)
 		for (i = 0, sum = rc; sum &gt; 0; i++, sum -= fragnob) {
 			LASSERT(i &lt; niov);
 
-			/* Dang! have to kmap again because I have nowhere to stash the
-			 * mapped address.  But by doing it while the page is still
-			 * mapped, the kernel just bumps the map count and returns me
-			 * the address it stashed. */
+			/* Dang! have to kmap again because I have nowhere to
+                         * stash the mapped address.  But by doing it while the
+                         * page is still mapped, the kernel just bumps the map
+                         * count and returns me the address it stashed. */
 			base = kmap(kiov[i].kiov_page) + kiov[i].kiov_offset;
 			fragnob = kiov[i].kiov_len;
 			if (fragnob &gt; sum)
@@ -386,9 +386,9 @@ ksocknal_lib_recv_kiov(ksock_conn_t *conn)
 void
 ksocknal_lib_csum_tx(ksock_tx_t *tx)
 {
-	int	  i;
-	__u32	csum;
-	void	*base;
+	int i;
+	__u32 csum;
+	void *base;
 
 	LASSERT(tx-&gt;tx_iov[0].iov_base == &amp;tx-&gt;tx_msg);
 	LASSERT(tx-&gt;tx_conn != NULL);
@@ -426,8 +426,8 @@ int
 ksocknal_lib_get_conn_tunables(ksock_conn_t *conn, int *txmem, int *rxmem, int *nagle)
 {
 	struct socket *sock = conn-&gt;ksnc_sock;
-	int	    len;
-	int	    rc;
+	int len;
+	int rc;
 
 	rc = ksocknal_connsock_addref(conn);
 	if (rc != 0) {
@@ -456,13 +456,13 @@ ksocknal_lib_get_conn_tunables(ksock_conn_t *conn, int *txmem, int *rxmem, int *
 int
 ksocknal_lib_setup_sock(struct socket *sock)
 {
-	int	     rc;
-	int	     option;
-	int	     keep_idle;
-	int	     keep_intvl;
-	int	     keep_count;
-	int	     do_keepalive;
-	struct linger   linger;
+	int rc;
+	int option;
+	int keep_idle;
+	int keep_intvl;
+	int keep_count;
+	int do_keepalive;
+	struct linger linger;
 
 	sock-&gt;sk-&gt;sk_allocation = GFP_NOFS;
 
@@ -555,11 +555,11 @@ ksocknal_lib_setup_sock(struct socket *sock)
 void
 ksocknal_lib_push_conn(ksock_conn_t *conn)
 {
-	struct sock    *sk;
+	struct sock *sk;
 	struct tcp_sock *tp;
-	int	     nonagle;
-	int	     val = 1;
-	int	     rc;
+	int nonagle;
+	int val = 1;
+	int rc;
 
 	rc = ksocknal_connsock_addref(conn);
 	if (rc != 0)			    /* being shut down */
@@ -592,7 +592,7 @@ extern void ksocknal_write_callback(ksock_conn_t *conn);
 static void
 ksocknal_data_ready(struct sock *sk)
 {
-	ksock_conn_t  *conn;
+	ksock_conn_t *conn;
 
 	/* interleave correctly with closing sockets... */
 	LASSERT(!in_irq());
@@ -611,9 +611,9 @@ ksocknal_data_ready(struct sock *sk)
 static void
 ksocknal_write_space(struct sock *sk)
 {
-	ksock_conn_t  *conn;
-	int	    wspace;
-	int	    min_wpace;
+	ksock_conn_t *conn;
+	int wspace;
+	int min_wpace;
 
 	/* interleave correctly with closing sockets... */
 	LASSERT(!in_irq());
@@ -689,7 +689,7 @@ ksocknal_lib_reset_callback(struct socket *sock, ksock_conn_t *conn)
 int
 ksocknal_lib_memory_pressure(ksock_conn_t *conn)
 {
-	int	    rc = 0;
+	int rc = 0;
 	ksock_sched_t *sched;
 
 	sched = conn-&gt;ksnc_scheduler;
diff --git a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_modparams.c b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_modparams.c
index 86b88db1cf20..c3ac67698125 100644
--- a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_modparams.c
+++ b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_modparams.c
@@ -145,40 +145,37 @@ ksock_tunables_t ksocknal_tunables;
 
 int ksocknal_tunables_init(void)
 {
-
 	/* initialize ksocknal_tunables structure */
-	ksocknal_tunables.ksnd_timeout	    = &amp;sock_timeout;
-	ksocknal_tunables.ksnd_nscheds		  = &amp;nscheds;
-	ksocknal_tunables.ksnd_nconnds	    = &amp;nconnds;
-	ksocknal_tunables.ksnd_nconnds_max	= &amp;nconnds_max;
+	ksocknal_tunables.ksnd_timeout            = &amp;sock_timeout;
+	ksocknal_tunables.ksnd_nscheds            = &amp;nscheds;
+	ksocknal_tunables.ksnd_nconnds            = &amp;nconnds;
+	ksocknal_tunables.ksnd_nconnds_max        = &amp;nconnds_max;
 	ksocknal_tunables.ksnd_min_reconnectms    = &amp;min_reconnectms;
 	ksocknal_tunables.ksnd_max_reconnectms    = &amp;max_reconnectms;
-	ksocknal_tunables.ksnd_eager_ack	  = &amp;eager_ack;
-	ksocknal_tunables.ksnd_typed_conns	= &amp;typed_conns;
-	ksocknal_tunables.ksnd_min_bulk	   = &amp;min_bulk;
+	ksocknal_tunables.ksnd_eager_ack          = &amp;eager_ack;
+	ksocknal_tunables.ksnd_typed_conns        = &amp;typed_conns;
+	ksocknal_tunables.ksnd_min_bulk           = &amp;min_bulk;
 	ksocknal_tunables.ksnd_tx_buffer_size     = &amp;tx_buffer_size;
 	ksocknal_tunables.ksnd_rx_buffer_size     = &amp;rx_buffer_size;
-	ksocknal_tunables.ksnd_nagle	      = &amp;nagle;
-	ksocknal_tunables.ksnd_round_robin	= &amp;round_robin;
-	ksocknal_tunables.ksnd_keepalive	  = &amp;keepalive;
+	ksocknal_tunables.ksnd_nagle              = &amp;nagle;
+	ksocknal_tunables.ksnd_round_robin        = &amp;round_robin;
+	ksocknal_tunables.ksnd_keepalive          = &amp;keepalive;
 	ksocknal_tunables.ksnd_keepalive_idle     = &amp;keepalive_idle;
 	ksocknal_tunables.ksnd_keepalive_count    = &amp;keepalive_count;
 	ksocknal_tunables.ksnd_keepalive_intvl    = &amp;keepalive_intvl;
-	ksocknal_tunables.ksnd_credits	    = &amp;credits;
+	ksocknal_tunables.ksnd_credits            = &amp;credits;
 	ksocknal_tunables.ksnd_peertxcredits      = &amp;peer_credits;
 	ksocknal_tunables.ksnd_peerrtrcredits     = &amp;peer_buffer_credits;
-	ksocknal_tunables.ksnd_peertimeout	= &amp;peer_timeout;
-	ksocknal_tunables.ksnd_enable_csum	= &amp;enable_csum;
+	ksocknal_tunables.ksnd_peertimeout        = &amp;peer_timeout;
+	ksocknal_tunables.ksnd_enable_csum        = &amp;enable_csum;
 	ksocknal_tunables.ksnd_inject_csum_error  = &amp;inject_csum_error;
 	ksocknal_tunables.ksnd_nonblk_zcack       = &amp;nonblk_zcack;
 	ksocknal_tunables.ksnd_zc_min_payload     = &amp;zc_min_payload;
-	ksocknal_tunables.ksnd_zc_recv	    = &amp;zc_recv;
+	ksocknal_tunables.ksnd_zc_recv            = &amp;zc_recv;
 	ksocknal_tunables.ksnd_zc_recv_min_nfrags = &amp;zc_recv_min_nfrags;
 
-
-
 #if SOCKNAL_VERSION_DEBUG
-	ksocknal_tunables.ksnd_protocol	   = &amp;protocol;
+	ksocknal_tunables.ksnd_protocol           = &amp;protocol;
 #endif
 
 	if (*ksocknal_tunables.ksnd_zc_min_payload &lt; (2 &lt;&lt; 10))
diff --git a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_proto.c b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_proto.c
index 8596581f54ff..1938d6a9b3a4 100644
--- a/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_proto.c
+++ b/drivers/staging/lustre/lnet/klnds/socklnd/socklnd_proto.c
@@ -52,7 +52,7 @@ ksocknal_queue_tx_msg_v1(ksock_conn_t *conn, ksock_tx_t *tx_msg)
 void
 ksocknal_next_tx_carrier(ksock_conn_t *conn)
 {
-	ksock_tx_t     *tx = conn-&gt;ksnc_tx_carrier;
+	ksock_tx_t *tx = conn-&gt;ksnc_tx_carrier;
 
 	/* Called holding BH lock: conn-&gt;ksnc_scheduler-&gt;kss_lock */
 	LASSERT(!list_empty(&amp;conn-&gt;ksnc_tx_queue));
@@ -119,7 +119,7 @@ ksocknal_queue_tx_zcack_v2(ksock_conn_t *conn,
 static ksock_tx_t *
 ksocknal_queue_tx_msg_v2(ksock_conn_t *conn, ksock_tx_t *tx_msg)
 {
-	ksock_tx_t  *tx  = conn-&gt;ksnc_tx_carrier;
+	ksock_tx_t *tx  = conn-&gt;ksnc_tx_carrier;
 
 	/*
 	 * Enqueue tx_msg:
@@ -361,10 +361,10 @@ ksocknal_match_tx_v3(ksock_conn_t *conn, ksock_tx_t *tx, int nonblk)
 static int
 ksocknal_handle_zcreq(ksock_conn_t *c, __u64 cookie, int remote)
 {
-	ksock_peer_t   *peer = c-&gt;ksnc_peer;
-	ksock_conn_t   *conn;
-	ksock_tx_t     *tx;
-	int	     rc;
+	ksock_peer_t *peer = c-&gt;ksnc_peer;
+	ksock_conn_t *conn;
+	ksock_tx_t *tx;
+	int rc;
 
 	read_lock(&amp;ksocknal_data.ksnd_global_lock);
 
@@ -405,11 +405,11 @@ ksocknal_handle_zcreq(ksock_conn_t *c, __u64 cookie, int remote)
 static int
 ksocknal_handle_zcack(ksock_conn_t *conn, __u64 cookie1, __u64 cookie2)
 {
-	ksock_peer_t      *peer = conn-&gt;ksnc_peer;
-	ksock_tx_t	*tx;
-	ksock_tx_t	*tmp;
+	ksock_peer_t *peer = conn-&gt;ksnc_peer;
+	ksock_tx_t *tx;
+	ksock_tx_t *tmp;
 	LIST_HEAD(zlist);
-	int		count;
+	int count;
 
 	if (cookie1 == 0)
 		cookie1 = cookie2;
@@ -452,11 +452,11 @@ ksocknal_handle_zcack(ksock_conn_t *conn, __u64 cookie1, __u64 cookie2)
 static int
 ksocknal_send_hello_v1(ksock_conn_t *conn, ksock_hello_msg_t *hello)
 {
-	struct socket	*sock = conn-&gt;ksnc_sock;
-	lnet_hdr_t	  *hdr;
+	struct socket *sock = conn-&gt;ksnc_sock;
+	lnet_hdr_t *hdr;
 	lnet_magicversion_t *hmv;
-	int		  rc;
-	int		  i;
+	int rc;
+	int i;
 
 	CLASSERT(sizeof(lnet_magicversion_t) == offsetof(lnet_hdr_t, src_nid));
 
@@ -470,7 +470,7 @@ ksocknal_send_hello_v1(ksock_conn_t *conn, ksock_hello_msg_t *hello)
 
 	/* Re-organize V2.x message header to V1.x (lnet_hdr_t)
 	 * header and send out */
-	hmv-&gt;magic	 = cpu_to_le32 (LNET_PROTO_TCP_MAGIC);
+	hmv-&gt;magic         = cpu_to_le32 (LNET_PROTO_TCP_MAGIC);
 	hmv-&gt;version_major = cpu_to_le16 (KSOCK_PROTO_V1_MAJOR);
 	hmv-&gt;version_minor = cpu_to_le16 (KSOCK_PROTO_V1_MINOR);
 
@@ -488,9 +488,9 @@ ksocknal_send_hello_v1(ksock_conn_t *conn, ksock_hello_msg_t *hello)
 		LNET_UNLOCK();
 	}
 
-	hdr-&gt;src_nid	= cpu_to_le64 (hello-&gt;kshm_src_nid);
-	hdr-&gt;src_pid	= cpu_to_le32 (hello-&gt;kshm_src_pid);
-	hdr-&gt;type	   = cpu_to_le32 (LNET_MSG_HELLO);
+	hdr-&gt;src_nid = cpu_to_le64 (hello-&gt;kshm_src_nid);
+	hdr-&gt;src_pid = cpu_to_le32 (hello-&gt;kshm_src_pid);
+	hdr-&gt;type = cpu_to_le32 (LNET_MSG_HELLO);
 	hdr-&gt;payload_length = cpu_to_le32 (hello-&gt;kshm_nips * sizeof(__u32));
 	hdr-&gt;msg.hello.type = cpu_to_le32 (hello-&gt;kshm_ctype);
 	hdr-&gt;msg.hello.incarnation = cpu_to_le64 (hello-&gt;kshm_src_incarnation);
@@ -529,7 +529,7 @@ static int
 ksocknal_send_hello_v2(ksock_conn_t *conn, ksock_hello_msg_t *hello)
 {
 	struct socket *sock = conn-&gt;ksnc_sock;
-	int	     rc;
+	int rc;
 
 	hello-&gt;kshm_magic   = LNET_PROTO_MAGIC;
 	hello-&gt;kshm_version = conn-&gt;ksnc_proto-&gt;pro_version;
@@ -572,10 +572,10 @@ static int
 ksocknal_recv_hello_v1(ksock_conn_t *conn, ksock_hello_msg_t *hello,
 		       int timeout)
 {
-	struct socket	*sock = conn-&gt;ksnc_sock;
-	lnet_hdr_t	  *hdr;
-	int		  rc;
-	int		  i;
+	struct socket *sock = conn-&gt;ksnc_sock;
+	lnet_hdr_t *hdr;
+	int rc;
+	int i;
 
 	LIBCFS_ALLOC(hdr, sizeof(*hdr));
 	if (hdr == NULL) {
@@ -602,12 +602,12 @@ ksocknal_recv_hello_v1(ksock_conn_t *conn, ksock_hello_msg_t *hello,
 		goto out;
 	}
 
-	hello-&gt;kshm_src_nid	 = le64_to_cpu(hdr-&gt;src_nid);
-	hello-&gt;kshm_src_pid	 = le32_to_cpu(hdr-&gt;src_pid);
+	hello-&gt;kshm_src_nid         = le64_to_cpu(hdr-&gt;src_nid);
+	hello-&gt;kshm_src_pid         = le32_to_cpu(hdr-&gt;src_pid);
 	hello-&gt;kshm_src_incarnation = le64_to_cpu(hdr-&gt;msg.hello.incarnation);
-	hello-&gt;kshm_ctype	   = le32_to_cpu(hdr-&gt;msg.hello.type);
-	hello-&gt;kshm_nips	    = le32_to_cpu(hdr-&gt;payload_length) /
-					 sizeof(__u32);
+	hello-&gt;kshm_ctype           = le32_to_cpu(hdr-&gt;msg.hello.type);
+	hello-&gt;kshm_nips            = le32_to_cpu(hdr-&gt;payload_length) /
+						  sizeof(__u32);
 
 	if (hello-&gt;kshm_nips &gt; LNET_MAX_INTERFACES) {
 		CERROR("Bad nips %d from ip %pI4h\n",
@@ -647,9 +647,9 @@ ksocknal_recv_hello_v1(ksock_conn_t *conn, ksock_hello_msg_t *hello,
 static int
 ksocknal_recv_hello_v2(ksock_conn_t *conn, ksock_hello_msg_t *hello, int timeout)
 {
-	struct socket   *sock = conn-&gt;ksnc_sock;
-	int		rc;
-	int		i;
+	struct socket *sock = conn-&gt;ksnc_sock;
+	int rc;
+	int i;
 
 	if (hello-&gt;kshm_magic == LNET_PROTO_MAGIC)
 		conn-&gt;ksnc_flip = 0;
@@ -746,9 +746,9 @@ ksocknal_pack_msg_v2(ksock_tx_t *tx)
 static void
 ksocknal_unpack_msg_v1(ksock_msg_t *msg)
 {
-	msg-&gt;ksm_csum	   = 0;
-	msg-&gt;ksm_type	   = KSOCK_MSG_LNET;
-	msg-&gt;ksm_zc_cookies[0]  = msg-&gt;ksm_zc_cookies[1]  = 0;
+	msg-&gt;ksm_csum = 0;
+	msg-&gt;ksm_type = KSOCK_MSG_LNET;
+	msg-&gt;ksm_zc_cookies[0] = msg-&gt;ksm_zc_cookies[1]  = 0;
 }
 
 static void
@@ -758,40 +758,40 @@ ksocknal_unpack_msg_v2(ksock_msg_t *msg)
 }
 
 ksock_proto_t  ksocknal_protocol_v1x = {
-	.pro_version	    = KSOCK_PROTO_V1,
-	.pro_send_hello	 = ksocknal_send_hello_v1,
-	.pro_recv_hello	 = ksocknal_recv_hello_v1,
-	.pro_pack	       = ksocknal_pack_msg_v1,
-	.pro_unpack	     = ksocknal_unpack_msg_v1,
-	.pro_queue_tx_msg       = ksocknal_queue_tx_msg_v1,
-	.pro_handle_zcreq       = NULL,
-	.pro_handle_zcack       = NULL,
-	.pro_queue_tx_zcack     = NULL,
-	.pro_match_tx	   = ksocknal_match_tx
+	.pro_version        = KSOCK_PROTO_V1,
+	.pro_send_hello     = ksocknal_send_hello_v1,
+	.pro_recv_hello     = ksocknal_recv_hello_v1,
+	.pro_pack           = ksocknal_pack_msg_v1,
+	.pro_unpack         = ksocknal_unpack_msg_v1,
+	.pro_queue_tx_msg   = ksocknal_queue_tx_msg_v1,
+	.pro_handle_zcreq   = NULL,
+	.pro_handle_zcack   = NULL,
+	.pro_queue_tx_zcack = NULL,
+	.pro_match_tx       = ksocknal_match_tx
 };
 
 ksock_proto_t  ksocknal_protocol_v2x = {
-	.pro_version	    = KSOCK_PROTO_V2,
-	.pro_send_hello	 = ksocknal_send_hello_v2,
-	.pro_recv_hello	 = ksocknal_recv_hello_v2,
-	.pro_pack	       = ksocknal_pack_msg_v2,
-	.pro_unpack	     = ksocknal_unpack_msg_v2,
-	.pro_queue_tx_msg       = ksocknal_queue_tx_msg_v2,
-	.pro_queue_tx_zcack     = ksocknal_queue_tx_zcack_v2,
-	.pro_handle_zcreq       = ksocknal_handle_zcreq,
-	.pro_handle_zcack       = ksocknal_handle_zcack,
-	.pro_match_tx	   = ksocknal_match_tx
+	.pro_version        = KSOCK_PROTO_V2,
+	.pro_send_hello     = ksocknal_send_hello_v2,
+	.pro_recv_hello     = ksocknal_recv_hello_v2,
+	.pro_pack           = ksocknal_pack_msg_v2,
+	.pro_unpack         = ksocknal_unpack_msg_v2,
+	.pro_queue_tx_msg   = ksocknal_queue_tx_msg_v2,
+	.pro_queue_tx_zcack = ksocknal_queue_tx_zcack_v2,
+	.pro_handle_zcreq   = ksocknal_handle_zcreq,
+	.pro_handle_zcack   = ksocknal_handle_zcack,
+	.pro_match_tx       = ksocknal_match_tx
 };
 
 ksock_proto_t  ksocknal_protocol_v3x = {
-	.pro_version	    = KSOCK_PROTO_V3,
-	.pro_send_hello	 = ksocknal_send_hello_v2,
-	.pro_recv_hello	 = ksocknal_recv_hello_v2,
-	.pro_pack	       = ksocknal_pack_msg_v2,
-	.pro_unpack	     = ksocknal_unpack_msg_v2,
-	.pro_queue_tx_msg       = ksocknal_queue_tx_msg_v2,
-	.pro_queue_tx_zcack     = ksocknal_queue_tx_zcack_v3,
-	.pro_handle_zcreq       = ksocknal_handle_zcreq,
-	.pro_handle_zcack       = ksocknal_handle_zcack,
-	.pro_match_tx	   = ksocknal_match_tx_v3
+	.pro_version        = KSOCK_PROTO_V3,
+	.pro_send_hello     = ksocknal_send_hello_v2,
+	.pro_recv_hello     = ksocknal_recv_hello_v2,
+	.pro_pack           = ksocknal_pack_msg_v2,
+	.pro_unpack         = ksocknal_unpack_msg_v2,
+	.pro_queue_tx_msg   = ksocknal_queue_tx_msg_v2,
+	.pro_queue_tx_zcack = ksocknal_queue_tx_zcack_v3,
+	.pro_handle_zcreq   = ksocknal_handle_zcreq,
+	.pro_handle_zcack   = ksocknal_handle_zcack,
+	.pro_match_tx       = ksocknal_match_tx_v3
 };</pre><hr><pre>commit ec3d17c0ed2e8e8cf111e24aec6586190b5d9b7a
Author: Mike Shuey &lt;shuey@purdue.edu&gt;
Date:   Tue May 19 10:14:36 2015 -0400

    staging: lustre: lnet: o2iblnd: code cleanup - align whitespace
    
    Unify variable declarations to use a single whitespace.  Also line up
    declarations and comments in o2iblnd.h.
    
    Signed-off-by: Mike Shuey &lt;shuey@purdue.edu&gt;
    Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;

diff --git a/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.c b/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.c
index 3bad441de8dc..a57c5c35ee68 100644
--- a/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.c
+++ b/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.c
@@ -42,21 +42,21 @@
 #include &lt;asm/div64.h&gt;
 
 static lnd_t the_o2iblnd = {
-	.lnd_type       = O2IBLND,
-	.lnd_startup    = kiblnd_startup,
-	.lnd_shutdown   = kiblnd_shutdown,
-	.lnd_ctl	= kiblnd_ctl,
-	.lnd_query      = kiblnd_query,
-	.lnd_send       = kiblnd_send,
-	.lnd_recv       = kiblnd_recv,
+	.lnd_type     = O2IBLND,
+	.lnd_startup  = kiblnd_startup,
+	.lnd_shutdown = kiblnd_shutdown,
+	.lnd_ctl      = kiblnd_ctl,
+	.lnd_query    = kiblnd_query,
+	.lnd_send     = kiblnd_send,
+	.lnd_recv     = kiblnd_recv,
 };
 
-kib_data_t	      kiblnd_data;
+kib_data_t kiblnd_data;
 
 static __u32 kiblnd_cksum(void *ptr, int nob)
 {
-	char  *c  = ptr;
-	__u32  sum = 0;
+	char *c = ptr;
+	__u32 sum = 0;
 
 	while (nob-- &gt; 0)
 		sum = ((sum &lt;&lt; 1) | (sum &gt;&gt; 31)) + *c++;
@@ -138,10 +138,10 @@ static int kiblnd_msgtype2size(int type)
 
 static int kiblnd_unpack_rd(kib_msg_t *msg, int flip)
 {
-	kib_rdma_desc_t   *rd;
-	int		nob;
-	int		n;
-	int		i;
+	kib_rdma_desc_t *rd;
+	int nob;
+	int n;
+	int i;
 
 	LASSERT(msg-&gt;ibm_type == IBLND_MSG_GET_REQ ||
 		 msg-&gt;ibm_type == IBLND_MSG_PUT_ACK);
@@ -210,10 +210,10 @@ void kiblnd_pack_msg(lnet_ni_t *ni, kib_msg_t *msg, int version,
 int kiblnd_unpack_msg(kib_msg_t *msg, int nob)
 {
 	const int hdr_size = offsetof(kib_msg_t, ibm_u);
-	__u32     msg_cksum;
-	__u16     version;
-	int       msg_nob;
-	int       flip;
+	__u32 msg_cksum;
+	__u16 version;
+	int msg_nob;
+	int flip;
 
 	/* 6 bytes are enough to have received magic + version */
 	if (nob &lt; 6) {
@@ -320,10 +320,10 @@ int kiblnd_unpack_msg(kib_msg_t *msg, int nob)
 
 int kiblnd_create_peer(lnet_ni_t *ni, kib_peer_t **peerp, lnet_nid_t nid)
 {
-	kib_peer_t	*peer;
-	kib_net_t	*net = ni-&gt;ni_data;
-	int		cpt = lnet_cpt_of_nid(nid);
-	unsigned long   flags;
+	kib_peer_t *peer;
+	kib_net_t *net = ni-&gt;ni_data;
+	int cpt = lnet_cpt_of_nid(nid);
+	unsigned long flags;
 
 	LASSERT(net != NULL);
 	LASSERT(nid != LNET_NID_ANY);
@@ -385,9 +385,9 @@ kib_peer_t *kiblnd_find_peer_locked(lnet_nid_t nid)
 {
 	/* the caller is responsible for accounting the additional reference
 	 * that this creates */
-	struct list_head       *peer_list = kiblnd_nid2peerlist(nid);
-	struct list_head       *tmp;
-	kib_peer_t       *peer;
+	struct list_head *peer_list = kiblnd_nid2peerlist(nid);
+	struct list_head *tmp;
+	kib_peer_t *peer;
 
 	list_for_each(tmp, peer_list) {
 
@@ -422,10 +422,10 @@ void kiblnd_unlink_peer_locked(kib_peer_t *peer)
 static int kiblnd_get_peer_info(lnet_ni_t *ni, int index,
 				lnet_nid_t *nidp, int *count)
 {
-	kib_peer_t	    *peer;
-	struct list_head	    *ptmp;
-	int		    i;
-	unsigned long	  flags;
+	kib_peer_t *peer;
+	struct list_head *ptmp;
+	int i;
+	unsigned long flags;
 
 	read_lock_irqsave(&amp;kiblnd_data.kib_global_lock, flags);
 
@@ -459,9 +459,9 @@ static int kiblnd_get_peer_info(lnet_ni_t *ni, int index,
 
 static void kiblnd_del_peer_locked(kib_peer_t *peer)
 {
-	struct list_head	   *ctmp;
-	struct list_head	   *cnxt;
-	kib_conn_t	   *conn;
+	struct list_head *ctmp;
+	struct list_head *cnxt;
+	kib_conn_t *conn;
 
 	if (list_empty(&amp;peer-&gt;ibp_conns)) {
 		kiblnd_unlink_peer_locked(peer);
@@ -480,14 +480,14 @@ static void kiblnd_del_peer_locked(kib_peer_t *peer)
 static int kiblnd_del_peer(lnet_ni_t *ni, lnet_nid_t nid)
 {
 	LIST_HEAD(zombies);
-	struct list_head	    *ptmp;
-	struct list_head	    *pnxt;
-	kib_peer_t	    *peer;
-	int		    lo;
-	int		    hi;
-	int		    i;
-	unsigned long	  flags;
-	int		    rc = -ENOENT;
+	struct list_head *ptmp;
+	struct list_head *pnxt;
+	kib_peer_t *peer;
+	int lo;
+	int hi;
+	int i;
+	unsigned long flags;
+	int rc = -ENOENT;
 
 	write_lock_irqsave(&amp;kiblnd_data.kib_global_lock, flags);
 
@@ -532,12 +532,12 @@ static int kiblnd_del_peer(lnet_ni_t *ni, lnet_nid_t nid)
 
 static kib_conn_t *kiblnd_get_conn_by_idx(lnet_ni_t *ni, int index)
 {
-	kib_peer_t	    *peer;
-	struct list_head	    *ptmp;
-	kib_conn_t	    *conn;
-	struct list_head	    *ctmp;
-	int		    i;
-	unsigned long	  flags;
+	kib_peer_t *peer;
+	struct list_head *ptmp;
+	kib_conn_t *conn;
+	struct list_head *ctmp;
+	int i;
+	unsigned long flags;
 
 	read_lock_irqsave(&amp;kiblnd_data.kib_global_lock, flags);
 
@@ -593,7 +593,7 @@ int kiblnd_translate_mtu(int value)
 
 static void kiblnd_setup_mtu_locked(struct rdma_cm_id *cmid)
 {
-	int	   mtu;
+	int mtu;
 
 	/* XXX There is no path record for iWARP, set by netdev-&gt;change_mtu? */
 	if (cmid-&gt;route.path_rec == NULL)
@@ -607,11 +607,11 @@ static void kiblnd_setup_mtu_locked(struct rdma_cm_id *cmid)
 
 static int kiblnd_get_completion_vector(kib_conn_t *conn, int cpt)
 {
-	cpumask_t	*mask;
-	int		vectors;
-	int		off;
-	int		i;
-	lnet_nid_t	nid = conn-&gt;ibc_peer-&gt;ibp_nid;
+	cpumask_t *mask;
+	int vectors;
+	int off;
+	int i;
+	lnet_nid_t nid = conn-&gt;ibc_peer-&gt;ibp_nid;
 
 	vectors = conn-&gt;ibc_cmid-&gt;device-&gt;num_comp_vectors;
 	if (vectors &lt;= 1)
@@ -642,17 +642,17 @@ kib_conn_t *kiblnd_create_conn(kib_peer_t *peer, struct rdma_cm_id *cmid,
 	 * she must dispose of 'cmid'.  (Actually I'd block forever if I tried
 	 * to destroy 'cmid' here since I'm called from the CM which still has
 	 * its ref on 'cmid'). */
-	rwlock_t		*glock = &amp;kiblnd_data.kib_global_lock;
-	kib_net_t	      *net = peer-&gt;ibp_ni-&gt;ni_data;
-	kib_dev_t	      *dev;
+	rwlock_t *glock = &amp;kiblnd_data.kib_global_lock;
+	kib_net_t *net = peer-&gt;ibp_ni-&gt;ni_data;
+	kib_dev_t *dev;
 	struct ib_qp_init_attr *init_qp_attr;
-	struct kib_sched_info	*sched;
-	kib_conn_t		*conn;
-	struct ib_cq		*cq;
-	unsigned long		flags;
-	int			cpt;
-	int			rc;
-	int			i;
+	struct kib_sched_info *sched;
+	kib_conn_t *conn;
+	struct ib_cq *cq;
+	unsigned long flags;
+	int cpt;
+	int rc;
+	int i;
 
 	LASSERT(net != NULL);
 	LASSERT(!in_interrupt());
@@ -837,8 +837,8 @@ kib_conn_t *kiblnd_create_conn(kib_peer_t *peer, struct rdma_cm_id *cmid,
 void kiblnd_destroy_conn(kib_conn_t *conn)
 {
 	struct rdma_cm_id *cmid = conn-&gt;ibc_cmid;
-	kib_peer_t	*peer = conn-&gt;ibc_peer;
-	int		rc;
+	kib_peer_t *peer = conn-&gt;ibc_peer;
+	int rc;
 
 	LASSERT(!in_interrupt());
 	LASSERT(atomic_read(&amp;conn-&gt;ibc_refcount) == 0);
@@ -904,10 +904,10 @@ void kiblnd_destroy_conn(kib_conn_t *conn)
 
 int kiblnd_close_peer_conns_locked(kib_peer_t *peer, int why)
 {
-	kib_conn_t	     *conn;
-	struct list_head	     *ctmp;
-	struct list_head	     *cnxt;
-	int		     count = 0;
+	kib_conn_t *conn;
+	struct list_head *ctmp;
+	struct list_head *cnxt;
+	int count = 0;
 
 	list_for_each_safe(ctmp, cnxt, &amp;peer-&gt;ibp_conns) {
 		conn = list_entry(ctmp, kib_conn_t, ibc_list);
@@ -926,10 +926,10 @@ int kiblnd_close_peer_conns_locked(kib_peer_t *peer, int why)
 int kiblnd_close_stale_conns_locked(kib_peer_t *peer,
 				     int version, __u64 incarnation)
 {
-	kib_conn_t	     *conn;
-	struct list_head	     *ctmp;
-	struct list_head	     *cnxt;
-	int		     count = 0;
+	kib_conn_t *conn;
+	struct list_head *ctmp;
+	struct list_head *cnxt;
+	int count = 0;
 
 	list_for_each_safe(ctmp, cnxt, &amp;peer-&gt;ibp_conns) {
 		conn = list_entry(ctmp, kib_conn_t, ibc_list);
@@ -953,14 +953,14 @@ int kiblnd_close_stale_conns_locked(kib_peer_t *peer,
 
 static int kiblnd_close_matching_conns(lnet_ni_t *ni, lnet_nid_t nid)
 {
-	kib_peer_t	     *peer;
-	struct list_head	     *ptmp;
-	struct list_head	     *pnxt;
-	int		     lo;
-	int		     hi;
-	int		     i;
-	unsigned long	   flags;
-	int		     count = 0;
+	kib_peer_t *peer;
+	struct list_head *ptmp;
+	struct list_head *pnxt;
+	int lo;
+	int hi;
+	int i;
+	unsigned long flags;
+	int count = 0;
 
 	write_lock_irqsave(&amp;kiblnd_data.kib_global_lock, flags);
 
@@ -1001,17 +1001,17 @@ static int kiblnd_close_matching_conns(lnet_ni_t *ni, lnet_nid_t nid)
 int kiblnd_ctl(lnet_ni_t *ni, unsigned int cmd, void *arg)
 {
 	struct libcfs_ioctl_data *data = arg;
-	int		       rc = -EINVAL;
+	int rc = -EINVAL;
 
 	switch (cmd) {
 	case IOC_LIBCFS_GET_PEER: {
-		lnet_nid_t   nid = 0;
-		int	  count = 0;
+		lnet_nid_t nid = 0;
+		int count = 0;
 
 		rc = kiblnd_get_peer_info(ni, data-&gt;ioc_count,
 					  &amp;nid, &amp;count);
-		data-&gt;ioc_nid    = nid;
-		data-&gt;ioc_count  = count;
+		data-&gt;ioc_nid   = nid;
+		data-&gt;ioc_count = count;
 		break;
 	}
 
@@ -1053,11 +1053,11 @@ int kiblnd_ctl(lnet_ni_t *ni, unsigned int cmd, void *arg)
 
 void kiblnd_query(lnet_ni_t *ni, lnet_nid_t nid, unsigned long *when)
 {
-	unsigned long	last_alive = 0;
-	unsigned long	now = cfs_time_current();
-	rwlock_t	*glock = &amp;kiblnd_data.kib_global_lock;
-	kib_peer_t	*peer;
-	unsigned long	flags;
+	unsigned long last_alive = 0;
+	unsigned long now = cfs_time_current();
+	rwlock_t *glock = &amp;kiblnd_data.kib_global_lock;
+	kib_peer_t *peer;
+	unsigned long flags;
 
 	read_lock_irqsave(glock, flags);
 
@@ -1086,8 +1086,8 @@ void kiblnd_query(lnet_ni_t *ni, lnet_nid_t nid, unsigned long *when)
 
 void kiblnd_free_pages(kib_pages_t *p)
 {
-	int	npages = p-&gt;ibp_npages;
-	int	i;
+	int npages = p-&gt;ibp_npages;
+	int i;
 
 	for (i = 0; i &lt; npages; i++) {
 		if (p-&gt;ibp_pages[i] != NULL)
@@ -1099,8 +1099,8 @@ void kiblnd_free_pages(kib_pages_t *p)
 
 int kiblnd_alloc_pages(kib_pages_t **pp, int cpt, int npages)
 {
-	kib_pages_t	*p;
-	int		i;
+	kib_pages_t *p;
+	int i;
 
 	LIBCFS_CPT_ALLOC(p, lnet_cpt_table(), cpt,
 			 offsetof(kib_pages_t, ibp_pages[npages]));
@@ -1130,7 +1130,7 @@ int kiblnd_alloc_pages(kib_pages_t **pp, int cpt, int npages)
 void kiblnd_unmap_rx_descs(kib_conn_t *conn)
 {
 	kib_rx_t *rx;
-	int       i;
+	int i;
 
 	LASSERT(conn-&gt;ibc_rxs != NULL);
 	LASSERT(conn-&gt;ibc_hdev != NULL);
@@ -1153,14 +1153,13 @@ void kiblnd_unmap_rx_descs(kib_conn_t *conn)
 
 void kiblnd_map_rx_descs(kib_conn_t *conn)
 {
-	kib_rx_t       *rx;
-	struct page    *pg;
-	int	     pg_off;
-	int	     ipg;
-	int	     i;
+	kib_rx_t *rx;
+	struct page *pg;
+	int pg_off;
+	int ipg;
+	int i;
 
-	for (pg_off = ipg = i = 0;
-	     i &lt; IBLND_RX_MSGS(conn-&gt;ibc_version); i++) {
+	for (pg_off = ipg = i = 0; i &lt; IBLND_RX_MSGS(conn-&gt;ibc_version); i++) {
 		pg = conn-&gt;ibc_rx_pages-&gt;ibp_pages[ipg];
 		rx = &amp;conn-&gt;ibc_rxs[i];
 
@@ -1192,9 +1191,9 @@ void kiblnd_map_rx_descs(kib_conn_t *conn)
 
 static void kiblnd_unmap_tx_pool(kib_tx_pool_t *tpo)
 {
-	kib_hca_dev_t  *hdev = tpo-&gt;tpo_hdev;
-	kib_tx_t       *tx;
-	int	     i;
+	kib_hca_dev_t *hdev = tpo-&gt;tpo_hdev;
+	kib_tx_t *tx;
+	int i;
 
 	LASSERT(tpo-&gt;tpo_pool.po_allocated == 0);
 
@@ -1216,8 +1215,8 @@ static void kiblnd_unmap_tx_pool(kib_tx_pool_t *tpo)
 static kib_hca_dev_t *kiblnd_current_hdev(kib_dev_t *dev)
 {
 	kib_hca_dev_t *hdev;
-	unsigned long  flags;
-	int	    i = 0;
+	unsigned long flags;
+	int i = 0;
 
 	read_lock_irqsave(&amp;kiblnd_data.kib_global_lock, flags);
 	while (dev-&gt;ibd_failover) {
@@ -1240,15 +1239,15 @@ static kib_hca_dev_t *kiblnd_current_hdev(kib_dev_t *dev)
 
 static void kiblnd_map_tx_pool(kib_tx_pool_t *tpo)
 {
-	kib_pages_t    *txpgs = tpo-&gt;tpo_tx_pages;
-	kib_pool_t     *pool  = &amp;tpo-&gt;tpo_pool;
-	kib_net_t      *net   = pool-&gt;po_owner-&gt;ps_net;
-	kib_dev_t      *dev;
-	struct page    *page;
-	kib_tx_t       *tx;
-	int	     page_offset;
-	int	     ipage;
-	int	     i;
+	kib_pages_t *txpgs = tpo-&gt;tpo_tx_pages;
+	kib_pool_t *pool = &amp;tpo-&gt;tpo_pool;
+	kib_net_t *net = pool-&gt;po_owner-&gt;ps_net;
+	kib_dev_t *dev;
+	struct page *page;
+	kib_tx_t *tx;
+	int page_offset;
+	int ipage;
+	int i;
 
 	LASSERT(net != NULL);
 
@@ -1291,7 +1290,7 @@ static void kiblnd_map_tx_pool(kib_tx_pool_t *tpo)
 
 struct ib_mr *kiblnd_find_dma_mr(kib_hca_dev_t *hdev, __u64 addr, __u64 size)
 {
-	__u64   index;
+	__u64 index;
 
 	LASSERT(hdev-&gt;ibh_mrs[0] != NULL);
 
@@ -1311,7 +1310,7 @@ struct ib_mr *kiblnd_find_rd_dma_mr(kib_hca_dev_t *hdev, kib_rdma_desc_t *rd)
 {
 	struct ib_mr *prev_mr;
 	struct ib_mr *mr;
-	int	   i;
+	int i;
 
 	LASSERT(hdev-&gt;ibh_mrs[0] != NULL);
 
@@ -1382,18 +1381,18 @@ static int kiblnd_create_fmr_pool(kib_fmr_poolset_t *fps,
 				  kib_fmr_pool_t **pp_fpo)
 {
 	/* FMR pool for RDMA */
-	kib_dev_t	       *dev = fps-&gt;fps_net-&gt;ibn_dev;
-	kib_fmr_pool_t	  *fpo;
+	kib_dev_t *dev = fps-&gt;fps_net-&gt;ibn_dev;
+	kib_fmr_pool_t *fpo;
 	struct ib_fmr_pool_param param = {
 		.max_pages_per_fmr = LNET_MAX_PAYLOAD/PAGE_SIZE,
-		.page_shift	= PAGE_SHIFT,
-		.access	    = (IB_ACCESS_LOCAL_WRITE |
-				      IB_ACCESS_REMOTE_WRITE),
-		.pool_size	   = fps-&gt;fps_pool_size,
+		.page_shift        = PAGE_SHIFT,
+		.access            = (IB_ACCESS_LOCAL_WRITE |
+		                      IB_ACCESS_REMOTE_WRITE),
+		.pool_size         = fps-&gt;fps_pool_size,
 		.dirty_watermark   = fps-&gt;fps_flush_trigger,
 		.flush_function    = NULL,
-		.flush_arg	 = NULL,
-		.cache	     = !!*kiblnd_tunables.kib_fmr_cache};
+		.flush_arg         = NULL,
+		.cache             = !!*kiblnd_tunables.kib_fmr_cache};
 	int rc;
 
 	LIBCFS_CPT_ALLOC(fpo, lnet_cpt_table(), fps-&gt;fps_cpt, sizeof(*fpo));
@@ -1454,7 +1453,7 @@ static int kiblnd_init_fmr_poolset(kib_fmr_poolset_t *fps, int cpt,
 				   int flush_trigger)
 {
 	kib_fmr_pool_t *fpo;
-	int	     rc;
+	int rc;
 
 	memset(fps, 0, sizeof(kib_fmr_poolset_t));
 
@@ -1485,11 +1484,11 @@ static int kiblnd_fmr_pool_is_idle(kib_fmr_pool_t *fpo, unsigned long now)
 void kiblnd_fmr_pool_unmap(kib_fmr_t *fmr, int status)
 {
 	LIST_HEAD(zombies);
-	kib_fmr_pool_t    *fpo = fmr-&gt;fmr_pool;
+	kib_fmr_pool_t *fpo = fmr-&gt;fmr_pool;
 	kib_fmr_poolset_t *fps = fpo-&gt;fpo_owner;
-	unsigned long	 now = cfs_time_current();
-	kib_fmr_pool_t    *tmp;
-	int		rc;
+	unsigned long now = cfs_time_current();
+	kib_fmr_pool_t *tmp;
+	int rc;
 
 	rc = ib_fmr_pool_unmap(fmr-&gt;fmr_pfmr);
 	LASSERT(rc == 0);
@@ -1525,9 +1524,9 @@ int kiblnd_fmr_pool_map(kib_fmr_poolset_t *fps, __u64 *pages, int npages,
 			__u64 iov, kib_fmr_t *fmr)
 {
 	struct ib_pool_fmr *pfmr;
-	kib_fmr_pool_t     *fpo;
-	__u64	       version;
-	int		 rc;
+	kib_fmr_pool_t *fpo;
+	__u64 version;
+	int rc;
 
  again:
 	spin_lock(&amp;fps-&gt;fps_lock);
@@ -1658,13 +1657,13 @@ static int kiblnd_init_poolset(kib_poolset_t *ps, int cpt,
 			       kib_ps_node_init_t nd_init,
 			       kib_ps_node_fini_t nd_fini)
 {
-	kib_pool_t	*pool;
-	int		rc;
+	kib_pool_t *pool;
+	int rc;
 
 	memset(ps, 0, sizeof(kib_poolset_t));
 
-	ps-&gt;ps_cpt	    = cpt;
-	ps-&gt;ps_net	  = net;
+	ps-&gt;ps_cpt          = cpt;
+	ps-&gt;ps_net          = net;
 	ps-&gt;ps_pool_create  = po_create;
 	ps-&gt;ps_pool_destroy = po_destroy;
 	ps-&gt;ps_node_init    = nd_init;
@@ -1698,9 +1697,9 @@ static int kiblnd_pool_is_idle(kib_pool_t *pool, unsigned long now)
 void kiblnd_pool_free_node(kib_pool_t *pool, struct list_head *node)
 {
 	LIST_HEAD(zombies);
-	kib_poolset_t  *ps = pool-&gt;po_owner;
-	kib_pool_t     *tmp;
-	unsigned long      now = cfs_time_current();
+	kib_poolset_t *ps = pool-&gt;po_owner;
+	kib_pool_t *tmp;
+	unsigned long now = cfs_time_current();
 
 	spin_lock(&amp;ps-&gt;ps_lock);
 
@@ -1727,9 +1726,9 @@ void kiblnd_pool_free_node(kib_pool_t *pool, struct list_head *node)
 
 struct list_head *kiblnd_pool_alloc_node(kib_poolset_t *ps)
 {
-	struct list_head	    *node;
-	kib_pool_t	    *pool;
-	int		    rc;
+	struct list_head *node;
+	kib_pool_t *pool;
+	int rc;
 
  again:
 	spin_lock(&amp;ps-&gt;ps_lock);
@@ -1789,8 +1788,8 @@ struct list_head *kiblnd_pool_alloc_node(kib_poolset_t *ps)
 
 void kiblnd_pmr_pool_unmap(kib_phys_mr_t *pmr)
 {
-	kib_pmr_pool_t      *ppo = pmr-&gt;pmr_pool;
-	struct ib_mr	*mr  = pmr-&gt;pmr_mr;
+	kib_pmr_pool_t *ppo = pmr-&gt;pmr_pool;
+	struct ib_mr *mr = pmr-&gt;pmr_mr;
 
 	pmr-&gt;pmr_mr = NULL;
 	kiblnd_pool_free_node(&amp;ppo-&gt;ppo_pool, &amp;pmr-&gt;pmr_list);
@@ -1802,9 +1801,9 @@ int kiblnd_pmr_pool_map(kib_pmr_poolset_t *pps, kib_hca_dev_t *hdev,
 		    kib_rdma_desc_t *rd, __u64 *iova, kib_phys_mr_t **pp_pmr)
 {
 	kib_phys_mr_t *pmr;
-	struct list_head    *node;
-	int	    rc;
-	int	    i;
+	struct list_head *node;
+	int rc;
+	int i;
 
 	node = kiblnd_pool_alloc_node(&amp;pps-&gt;pps_poolset);
 	if (node == NULL) {
@@ -1846,7 +1845,7 @@ int kiblnd_pmr_pool_map(kib_pmr_poolset_t *pps, kib_hca_dev_t *hdev,
 static void kiblnd_destroy_pmr_pool(kib_pool_t *pool)
 {
 	kib_pmr_pool_t *ppo = container_of(pool, kib_pmr_pool_t, ppo_pool);
-	kib_phys_mr_t  *pmr;
+	kib_phys_mr_t *pmr;
 	kib_phys_mr_t *tmp;
 
 	LASSERT(pool-&gt;po_allocated == 0);
@@ -1881,10 +1880,10 @@ static inline int kiblnd_pmr_pool_size(int ncpts)
 static int kiblnd_create_pmr_pool(kib_poolset_t *ps, int size,
 				  kib_pool_t **pp_po)
 {
-	struct kib_pmr_pool	*ppo;
-	struct kib_pool		*pool;
-	kib_phys_mr_t		*pmr;
-	int			i;
+	struct kib_pmr_pool *ppo;
+	struct kib_pool *pool;
+	kib_phys_mr_t *pmr;
+	int i;
 
 	LIBCFS_CPT_ALLOC(ppo, lnet_cpt_table(),
 			 ps-&gt;ps_cpt, sizeof(kib_pmr_pool_t));
@@ -1923,8 +1922,8 @@ static int kiblnd_create_pmr_pool(kib_poolset_t *ps, int size,
 
 static void kiblnd_destroy_tx_pool(kib_pool_t *pool)
 {
-	kib_tx_pool_t  *tpo = container_of(pool, kib_tx_pool_t, tpo_pool);
-	int	     i;
+	kib_tx_pool_t *tpo = container_of(pool, kib_tx_pool_t, tpo_pool);
+	int i;
 
 	LASSERT(pool-&gt;po_allocated == 0);
 
@@ -1979,9 +1978,9 @@ static int kiblnd_tx_pool_size(int ncpts)
 static int kiblnd_create_tx_pool(kib_poolset_t *ps, int size,
 				 kib_pool_t **pp_po)
 {
-	int	    i;
-	int	    npg;
-	kib_pool_t    *pool;
+	int i;
+	int npg;
+	kib_pool_t *pool;
 	kib_tx_pool_t *tpo;
 
 	LIBCFS_CPT_ALLOC(tpo, lnet_cpt_table(), ps-&gt;ps_cpt, sizeof(*tpo));
@@ -2064,19 +2063,19 @@ static void kiblnd_tx_init(kib_pool_t *pool, struct list_head *node)
 {
 	kib_tx_poolset_t *tps = container_of(pool-&gt;po_owner, kib_tx_poolset_t,
 					     tps_poolset);
-	kib_tx_t	 *tx  = list_entry(node, kib_tx_t, tx_list);
+	kib_tx_t *tx  = list_entry(node, kib_tx_t, tx_list);
 
 	tx-&gt;tx_cookie = tps-&gt;tps_next_tx_cookie++;
 }
 
 static void kiblnd_net_fini_pools(kib_net_t *net)
 {
-	int	i;
+	int i;
 
 	cfs_cpt_for_each(i, lnet_cpt_table()) {
-		kib_tx_poolset_t	*tps;
-		kib_fmr_poolset_t	*fps;
-		kib_pmr_poolset_t	*pps;
+		kib_tx_poolset_t *tps;
+		kib_fmr_poolset_t *fps;
+		kib_pmr_poolset_t *pps;
 
 		if (net-&gt;ibn_tx_ps != NULL) {
 			tps = net-&gt;ibn_tx_ps[i];
@@ -2112,16 +2111,15 @@ static void kiblnd_net_fini_pools(kib_net_t *net)
 
 static int kiblnd_net_init_pools(kib_net_t *net, __u32 *cpts, int ncpts)
 {
-	unsigned long	flags;
-	int		cpt;
-	int		rc;
-	int		i;
+	unsigned long flags;
+	int cpt;
+	int rc;
+	int i;
 
 	read_lock_irqsave(&amp;kiblnd_data.kib_global_lock, flags);
 	if (*kiblnd_tunables.kib_map_on_demand == 0 &amp;&amp;
 	    net-&gt;ibn_dev-&gt;ibd_hdev-&gt;ibh_nmrs == 1) {
-		read_unlock_irqrestore(&amp;kiblnd_data.kib_global_lock,
-					   flags);
+		read_unlock_irqrestore(&amp;kiblnd_data.kib_global_lock, flags);
 		goto create_tx_pool;
 	}
 
@@ -2241,7 +2239,7 @@ static int kiblnd_net_init_pools(kib_net_t *net, __u32 *cpts, int ncpts)
 static int kiblnd_hdev_get_attr(kib_hca_dev_t *hdev)
 {
 	struct ib_device_attr *attr;
-	int		    rc;
+	int rc;
 
 	/* It's safe to assume a HCA can handle a page size
 	 * matching that of the native system */
@@ -2284,7 +2282,7 @@ static int kiblnd_hdev_get_attr(kib_hca_dev_t *hdev)
 
 static void kiblnd_hdev_cleanup_mrs(kib_hca_dev_t *hdev)
 {
-	int     i;
+	int i;
 
 	if (hdev-&gt;ibh_nmrs == 0 || hdev-&gt;ibh_mrs == NULL)
 		return;
@@ -2317,12 +2315,11 @@ void kiblnd_hdev_destroy(kib_hca_dev_t *hdev)
 static int kiblnd_hdev_setup_mrs(kib_hca_dev_t *hdev)
 {
 	struct ib_mr *mr;
-	int	   i;
-	int	   rc;
-	__u64	 mm_size;
-	__u64	 mr_size;
-	int	   acflags = IB_ACCESS_LOCAL_WRITE |
-				IB_ACCESS_REMOTE_WRITE;
+	int i;
+	int rc;
+	__u64 mm_size;
+	__u64 mr_size;
+	int acflags = IB_ACCESS_LOCAL_WRITE | IB_ACCESS_REMOTE_WRITE;
 
 	rc = kiblnd_hdev_get_attr(hdev);
 	if (rc != 0)
@@ -2371,11 +2368,11 @@ static int kiblnd_hdev_setup_mrs(kib_hca_dev_t *hdev)
 
 	for (i = 0; i &lt; hdev-&gt;ibh_nmrs; i++) {
 		struct ib_phys_buf ipb;
-		__u64	      iova;
+		__u64 iova;
 
 		ipb.size = hdev-&gt;ibh_mr_size;
 		ipb.addr = i * mr_size;
-		iova     = ipb.addr;
+		iova = ipb.addr;
 
 		mr = ib_reg_phys_mr(hdev-&gt;ibh_pd, &amp;ipb, 1, acflags, &amp;iova);
 		if (IS_ERR(mr)) {
@@ -2406,10 +2403,10 @@ static int kiblnd_dummy_callback(struct rdma_cm_id *cmid,
 
 static int kiblnd_dev_need_failover(kib_dev_t *dev)
 {
-	struct rdma_cm_id  *cmid;
-	struct sockaddr_in  srcaddr;
-	struct sockaddr_in  dstaddr;
-	int		 rc;
+	struct rdma_cm_id *cmid;
+	struct sockaddr_in srcaddr;
+	struct sockaddr_in dstaddr;
+	int rc;
 
 	if (dev-&gt;ibd_hdev == NULL || /* initializing */
 	    dev-&gt;ibd_hdev-&gt;ibh_cmid == NULL || /* listener is dead */
@@ -2435,7 +2432,7 @@ static int kiblnd_dev_need_failover(kib_dev_t *dev)
 	}
 
 	memset(&amp;srcaddr, 0, sizeof(srcaddr));
-	srcaddr.sin_family      = AF_INET;
+	srcaddr.sin_family = AF_INET;
 	srcaddr.sin_addr.s_addr = (__force u32)htonl(dev-&gt;ibd_ifip);
 
 	memset(&amp;dstaddr, 0, sizeof(dstaddr));
@@ -2464,15 +2461,15 @@ int kiblnd_dev_failover(kib_dev_t *dev)
 	LIST_HEAD(zombie_tpo);
 	LIST_HEAD(zombie_ppo);
 	LIST_HEAD(zombie_fpo);
-	struct rdma_cm_id  *cmid  = NULL;
-	kib_hca_dev_t      *hdev  = NULL;
-	kib_hca_dev_t      *old;
-	struct ib_pd       *pd;
-	kib_net_t	  *net;
-	struct sockaddr_in  addr;
-	unsigned long       flags;
-	int		 rc = 0;
-	int		    i;
+	struct rdma_cm_id *cmid  = NULL;
+	kib_hca_dev_t *hdev  = NULL;
+	kib_hca_dev_t *old;
+	struct ib_pd *pd;
+	kib_net_t *net;
+	struct sockaddr_in addr;
+	unsigned long flags;
+	int rc = 0;
+	int i;
 
 	LASSERT(*kiblnd_tunables.kib_dev_failover &gt; 1 ||
 		 dev-&gt;ibd_can_failover ||
@@ -2614,11 +2611,11 @@ void kiblnd_destroy_dev(kib_dev_t *dev)
 static kib_dev_t *kiblnd_create_dev(char *ifname)
 {
 	struct net_device *netdev;
-	kib_dev_t	 *dev;
-	__u32	      netmask;
-	__u32	      ip;
-	int		up;
-	int		rc;
+	kib_dev_t *dev;
+	__u32 netmask;
+	__u32 ip;
+	int up;
+	int rc;
 
 	rc = libcfs_ipif_query(ifname, &amp;up, &amp;ip, &amp;netmask);
 	if (rc != 0) {
@@ -2665,8 +2662,8 @@ static kib_dev_t *kiblnd_create_dev(char *ifname)
 
 static void kiblnd_base_shutdown(void)
 {
-	struct kib_sched_info	*sched;
-	int			i;
+	struct kib_sched_info *sched;
+	int i;
 
 	LASSERT(list_empty(&amp;kiblnd_data.kib_devs));
 
@@ -2732,10 +2729,10 @@ static void kiblnd_base_shutdown(void)
 
 void kiblnd_shutdown(lnet_ni_t *ni)
 {
-	kib_net_t	*net = ni-&gt;ni_data;
-	rwlock_t     *g_lock = &amp;kiblnd_data.kib_global_lock;
-	int	       i;
-	unsigned long     flags;
+	kib_net_t *net = ni-&gt;ni_data;
+	rwlock_t *g_lock = &amp;kiblnd_data.kib_global_lock;
+	int i;
+	unsigned long flags;
 
 	LASSERT(kiblnd_data.kib_init == IBLND_INIT_ALL);
 
@@ -2804,9 +2801,9 @@ void kiblnd_shutdown(lnet_ni_t *ni)
 
 static int kiblnd_base_startup(void)
 {
-	struct kib_sched_info	*sched;
-	int			rc;
-	int			i;
+	struct kib_sched_info *sched;
+	int rc;
+	int i;
 
 	LASSERT(kiblnd_data.kib_init == IBLND_INIT_NOTHING);
 
@@ -2821,8 +2818,7 @@ static int kiblnd_base_startup(void)
 
 	kiblnd_data.kib_peer_hash_size = IBLND_PEER_HASH_SIZE;
 	LIBCFS_ALLOC(kiblnd_data.kib_peers,
-		     sizeof(struct list_head) *
-			    kiblnd_data.kib_peer_hash_size);
+		     sizeof(struct list_head) * kiblnd_data.kib_peer_hash_size);
 	if (kiblnd_data.kib_peers == NULL)
 		goto failed;
 	for (i = 0; i &lt; kiblnd_data.kib_peer_hash_size; i++)
@@ -2840,7 +2836,7 @@ static int kiblnd_base_startup(void)
 		goto failed;
 
 	cfs_percpt_for_each(sched, i, kiblnd_data.kib_scheds) {
-		int	nthrs;
+		int nthrs;
 
 		spin_lock_init(&amp;sched-&gt;ibs_lock);
 		INIT_LIST_HEAD(&amp;sched-&gt;ibs_conns);
@@ -2893,9 +2889,9 @@ static int kiblnd_base_startup(void)
 
 static int kiblnd_start_schedulers(struct kib_sched_info *sched)
 {
-	int	rc = 0;
-	int	nthrs;
-	int	i;
+	int rc = 0;
+	int nthrs;
+	int i;
 
 	if (sched-&gt;ibs_nthreads == 0) {
 		if (*kiblnd_tunables.kib_nscheds &gt; 0) {
@@ -2913,8 +2909,8 @@ static int kiblnd_start_schedulers(struct kib_sched_info *sched)
 	}
 
 	for (i = 0; i &lt; nthrs; i++) {
-		long	id;
-		char	name[20];
+		long id;
+		char name[20];
 
 		id = KIB_THREAD_ID(sched-&gt;ibs_cpt, sched-&gt;ibs_nthreads + i);
 		snprintf(name, sizeof(name), "kiblnd_sd_%02ld_%02ld",
@@ -2935,9 +2931,9 @@ static int kiblnd_start_schedulers(struct kib_sched_info *sched)
 static int kiblnd_dev_start_threads(kib_dev_t *dev, int newdev, __u32 *cpts,
 				    int ncpts)
 {
-	int	cpt;
-	int	rc;
-	int	i;
+	int cpt;
+	int rc;
+	int i;
 
 	for (i = 0; i &lt; ncpts; i++) {
 		struct kib_sched_info *sched;
@@ -2960,10 +2956,10 @@ static int kiblnd_dev_start_threads(kib_dev_t *dev, int newdev, __u32 *cpts,
 
 static kib_dev_t *kiblnd_dev_search(char *ifname)
 {
-	kib_dev_t	*alias = NULL;
-	kib_dev_t	*dev;
-	char		*colon;
-	char		*colon2;
+	kib_dev_t *alias = NULL;
+	kib_dev_t *dev;
+	char *colon;
+	char *colon2;
 
 	colon = strchr(ifname, ':');
 	list_for_each_entry(dev, &amp;kiblnd_data.kib_devs, ibd_list) {
@@ -2992,13 +2988,13 @@ static kib_dev_t *kiblnd_dev_search(char *ifname)
 
 int kiblnd_startup(lnet_ni_t *ni)
 {
-	char		     *ifname;
-	kib_dev_t		*ibdev = NULL;
-	kib_net_t		*net;
-	struct timeval	    tv;
-	unsigned long	     flags;
-	int		       rc;
-	int			  newdev;
+	char *ifname;
+	kib_dev_t *ibdev = NULL;
+	kib_net_t *net;
+	struct timeval tv;
+	unsigned long flags;
+	int rc;
+	int newdev;
 
 	LASSERT(ni-&gt;ni_lnd == &amp;the_o2iblnd);
 
@@ -3091,7 +3087,7 @@ static void __exit kiblnd_module_fini(void)
 
 static int __init kiblnd_module_init(void)
 {
-	int    rc;
+	int rc;
 
 	CLASSERT(sizeof(kib_msg_t) &lt;= IBLND_MSG_SIZE);
 	CLASSERT(offsetof(kib_msg_t,
diff --git a/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.h b/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.h
index cd664d025f41..7f52c6963427 100644
--- a/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.h
+++ b/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd.h
@@ -80,42 +80,47 @@
 #define IBLND_N_SCHED_HIGH		4
 
 typedef struct {
-	int	      *kib_dev_failover;     /* HCA failover */
-	unsigned int     *kib_service;	  /* IB service number */
-	int	      *kib_min_reconnect_interval; /* first failed connection retry... */
-	int	      *kib_max_reconnect_interval; /* ...exponentially increasing to this */
-	int	      *kib_cksum;	    /* checksum kib_msg_t? */
-	int	      *kib_timeout;	  /* comms timeout (seconds) */
-	int	      *kib_keepalive;	/* keepalive timeout (seconds) */
-	int	      *kib_ntx;	      /* # tx descs */
-	int	      *kib_credits;	  /* # concurrent sends */
-	int	      *kib_peertxcredits;    /* # concurrent sends to 1 peer */
-	int	      *kib_peerrtrcredits;   /* # per-peer router buffer credits */
-	int	      *kib_peercredits_hiw;  /* # when eagerly to return credits */
-	int	      *kib_peertimeout;      /* seconds to consider peer dead */
-	char	    **kib_default_ipif;     /* default IPoIB interface */
-	int	      *kib_retry_count;
-	int	      *kib_rnr_retry_count;
-	int	      *kib_concurrent_sends; /* send work queue sizing */
-	int		 *kib_ib_mtu;		/* IB MTU */
-	int	      *kib_map_on_demand;    /* map-on-demand if RD has more fragments
-						 * than this value, 0 disable map-on-demand */
-	int	      *kib_pmr_pool_size;    /* # physical MR in pool */
-	int	      *kib_fmr_pool_size;    /* # FMRs in pool */
-	int	      *kib_fmr_flush_trigger; /* When to trigger FMR flush */
-	int	      *kib_fmr_cache;	/* enable FMR pool cache? */
-	int	      *kib_require_priv_port;/* accept only privileged ports */
-	int	      *kib_use_priv_port;    /* use privileged port for active connect */
-	/* # threads on each CPT */
-	int		 *kib_nscheds;
+	int          *kib_dev_failover;      /* HCA failover */
+	unsigned int *kib_service;           /* IB service number */
+	int          *kib_min_reconnect_interval; /* first failed connection
+						   * retry... */
+	int          *kib_max_reconnect_interval; /* ...exponentially increasing
+						   * to this */
+	int          *kib_cksum;             /* checksum kib_msg_t? */
+	int          *kib_timeout;           /* comms timeout (seconds) */
+	int          *kib_keepalive;         /* keepalive timeout (seconds) */
+	int          *kib_ntx;               /* # tx descs */
+	int          *kib_credits;           /* # concurrent sends */
+	int          *kib_peertxcredits;     /* # concurrent sends to 1 peer */
+	int          *kib_peerrtrcredits;    /* # per-peer router buffer
+					      * credits */
+	int          *kib_peercredits_hiw;   /* # when eagerly to return
+					      * credits */
+	int          *kib_peertimeout;       /* seconds to consider peer dead */
+	char         **kib_default_ipif;     /* default IPoIB interface */
+	int          *kib_retry_count;
+	int          *kib_rnr_retry_count;
+	int          *kib_concurrent_sends;  /* send work queue sizing */
+	int          *kib_ib_mtu;            /* IB MTU */
+	int          *kib_map_on_demand;     /* map-on-demand if RD has more
+					      * fragments than this value, 0
+					      * disable map-on-demand */
+	int          *kib_pmr_pool_size;     /* # physical MR in pool */
+	int          *kib_fmr_pool_size;     /* # FMRs in pool */
+	int          *kib_fmr_flush_trigger; /* When to trigger FMR flush */
+	int          *kib_fmr_cache;         /* enable FMR pool cache? */
+	int          *kib_require_priv_port; /* accept only privileged ports */
+	int          *kib_use_priv_port;     /* use privileged port for active
+					      * connect */
+	int          *kib_nscheds;           /* # threads on each CPT */
 } kib_tunables_t;
 
 extern kib_tunables_t  kiblnd_tunables;
 
-#define IBLND_MSG_QUEUE_SIZE_V1      8	  /* V1 only : # messages/RDMAs in-flight */
-#define IBLND_CREDIT_HIGHWATER_V1    7	  /* V1 only : when eagerly to return credits */
+#define IBLND_MSG_QUEUE_SIZE_V1   8 /* V1 only : # messages/RDMAs in-flight */
+#define IBLND_CREDIT_HIGHWATER_V1 7 /* V1 only : when eagerly to return credits */
 
-#define IBLND_CREDITS_DEFAULT	8	  /* default # of peer credits */
+#define IBLND_CREDITS_DEFAULT     8 /* default # of peer credits */
 #define IBLND_CREDITS_MAX	  ((typeof(((kib_msg_t*) 0)-&gt;ibm_credits)) - 1)  /* Max # of peer credits */
 
 #define IBLND_MSG_QUEUE_SIZE(v)    ((v) == IBLND_MSG_VERSION_1 ? \
@@ -186,34 +191,36 @@ struct kib_hca_dev;
 #endif
 
 typedef struct {
-	struct list_head	   ibd_list;	  /* chain on kib_devs */
-	struct list_head	   ibd_fail_list;     /* chain on kib_failed_devs */
-	__u32		ibd_ifip;	  /* IPoIB interface IP */
-	/** IPoIB interface name */
-	char		 ibd_ifname[KIB_IFNAME_SIZE];
-	int		  ibd_nnets;	 /* # nets extant */
-
-	unsigned long	   ibd_next_failover;
-	int		  ibd_failed_failover; /* # failover failures */
-	unsigned int	 ibd_failover;      /* failover in progress */
-	unsigned int	 ibd_can_failover;  /* IPoIB interface is a bonding master */
-	struct list_head	   ibd_nets;
-	struct kib_hca_dev  *ibd_hdev;
+	struct list_head   ibd_list;            /* chain on kib_devs */
+	struct list_head   ibd_fail_list;       /* chain on kib_failed_devs */
+	__u32              ibd_ifip;            /* IPoIB interface IP */
+
+	/* IPoIB interface name */
+	char               ibd_ifname[KIB_IFNAME_SIZE];
+	int                ibd_nnets;           /* # nets extant */
+
+	unsigned long      ibd_next_failover;
+	int                ibd_failed_failover; /* # failover failures */
+	unsigned int       ibd_failover;        /* failover in progress */
+	unsigned int       ibd_can_failover;    /* IPoIB interface is a bonding
+						 * master */
+	struct list_head   ibd_nets;
+	struct kib_hca_dev *ibd_hdev;
 } kib_dev_t;
 
 typedef struct kib_hca_dev {
-	struct rdma_cm_id   *ibh_cmid;	  /* listener cmid */
-	struct ib_device    *ibh_ibdev;	 /* IB device */
-	int		  ibh_page_shift;    /* page shift of current HCA */
-	int		  ibh_page_size;     /* page size of current HCA */
-	__u64		ibh_page_mask;     /* page mask of current HCA */
-	int		  ibh_mr_shift;      /* bits shift of max MR size */
-	__u64		ibh_mr_size;       /* size of MR */
-	int		  ibh_nmrs;	  /* # of global MRs */
-	struct ib_mr       **ibh_mrs;	   /* global MR */
-	struct ib_pd	*ibh_pd;	    /* PD */
-	kib_dev_t	   *ibh_dev;	   /* owner */
-	atomic_t	 ibh_ref;	   /* refcount */
+	struct rdma_cm_id  *ibh_cmid;           /* listener cmid */
+	struct ib_device   *ibh_ibdev;          /* IB device */
+	int                ibh_page_shift;      /* page shift of current HCA */
+	int                ibh_page_size;       /* page size of current HCA */
+	__u64              ibh_page_mask;       /* page mask of current HCA */
+	int                ibh_mr_shift;        /* bits shift of max MR size */
+	__u64              ibh_mr_size;         /* size of MR */
+	int                ibh_nmrs;            /* # of global MRs */
+	struct ib_mr       **ibh_mrs;           /* global MR */
+	struct ib_pd       *ibh_pd;             /* PD */
+	kib_dev_t          *ibh_dev;            /* owner */
+	atomic_t           ibh_ref;             /* refcount */
 } kib_hca_dev_t;
 
 /** # of seconds to keep pool alive */
@@ -222,19 +229,19 @@ typedef struct kib_hca_dev {
 #define IBLND_POOL_RETRY	1
 
 typedef struct {
-	int		     ibp_npages;	     /* # pages */
-	struct page	    *ibp_pages[0];	   /* page array */
+	int                ibp_npages;          /* # pages */
+	struct page        *ibp_pages[0];       /* page array */
 } kib_pages_t;
 
 struct kib_pmr_pool;
 
 typedef struct {
-	struct list_head	      pmr_list;	       /* chain node */
-	struct ib_phys_buf     *pmr_ipb;		/* physical buffer */
-	struct ib_mr	   *pmr_mr;		 /* IB MR */
-	struct kib_pmr_pool    *pmr_pool;	       /* owner of this MR */
-	__u64		   pmr_iova;	       /* Virtual I/O address */
-	int		     pmr_refcount;	   /* reference count */
+	struct list_head    pmr_list;           /* chain node */
+	struct ib_phys_buf  *pmr_ipb;           /* physical buffer */
+	struct ib_mr        *pmr_mr;            /* IB MR */
+	struct kib_pmr_pool *pmr_pool;          /* owner of this MR */
+	__u64               pmr_iova;           /* Virtual I/O address */
+	int                 pmr_refcount;       /* reference count */
 } kib_phys_mr_t;
 
 struct kib_pool;
@@ -251,97 +258,99 @@ struct kib_net;
 #define IBLND_POOL_NAME_LEN     32
 
 typedef struct kib_poolset {
-	spinlock_t		ps_lock;		/* serialize */
-	struct kib_net	 *ps_net;		 /* network it belongs to */
-	char		    ps_name[IBLND_POOL_NAME_LEN]; /* pool set name */
-	struct list_head	      ps_pool_list;	   /* list of pools */
-	struct list_head	      ps_failed_pool_list;    /* failed pool list */
-	unsigned long	      ps_next_retry;	  /* time stamp for retry if failed to allocate */
-	int		     ps_increasing;	  /* is allocating new pool */
-	int		     ps_pool_size;	   /* new pool size */
-	int			ps_cpt;			/* CPT id */
-
-	kib_ps_pool_create_t    ps_pool_create;	 /* create a new pool */
-	kib_ps_pool_destroy_t   ps_pool_destroy;	/* destroy a pool */
-	kib_ps_node_init_t      ps_node_init;	   /* initialize new allocated node */
-	kib_ps_node_fini_t      ps_node_fini;	   /* finalize node */
+	spinlock_t            ps_lock;            /* serialize */
+	struct kib_net        *ps_net;            /* network it belongs to */
+	char                  ps_name[IBLND_POOL_NAME_LEN]; /* pool set name */
+	struct list_head      ps_pool_list;       /* list of pools */
+	struct list_head      ps_failed_pool_list;/* failed pool list */
+	unsigned long         ps_next_retry;      /* time stamp for retry if
+						   * failed to allocate */
+	int                   ps_increasing;      /* is allocating new pool */
+	int                   ps_pool_size;       /* new pool size */
+	int                   ps_cpt;             /* CPT id */
+
+	kib_ps_pool_create_t  ps_pool_create;     /* create a new pool */
+	kib_ps_pool_destroy_t ps_pool_destroy;    /* destroy a pool */
+	kib_ps_node_init_t    ps_node_init;       /* initialize new allocated
+						   * node */
+	kib_ps_node_fini_t    ps_node_fini;       /* finalize node */
 } kib_poolset_t;
 
 typedef struct kib_pool {
-	struct list_head	      po_list;		/* chain on pool list */
-	struct list_head	      po_free_list;	   /* pre-allocated node */
-	kib_poolset_t	  *po_owner;	       /* pool_set of this pool */
-	unsigned long	      po_deadline;	    /* deadline of this pool */
-	int		     po_allocated;	   /* # of elements in use */
-	int		     po_failed;	      /* pool is created on failed HCA */
-	int		     po_size;		/* # of pre-allocated elements */
+	struct list_head      po_list;         /* chain on pool list */
+	struct list_head      po_free_list;    /* pre-allocated node */
+	kib_poolset_t         *po_owner;       /* pool_set of this pool */
+	unsigned long         po_deadline;     /* deadline of this pool */
+	int                   po_allocated;    /* # of elements in use */
+	int                   po_failed;       /* pool is created on failed
+						* HCA */
+	int                   po_size;         /* # of pre-allocated elements */
 } kib_pool_t;
 
 typedef struct {
-	kib_poolset_t	   tps_poolset;	    /* pool-set */
-	__u64		   tps_next_tx_cookie;     /* cookie of TX */
+	kib_poolset_t         tps_poolset;        /* pool-set */
+	__u64                 tps_next_tx_cookie; /* cookie of TX */
 } kib_tx_poolset_t;
 
 typedef struct {
-	kib_pool_t	      tpo_pool;	       /* pool */
-	struct kib_hca_dev     *tpo_hdev;	       /* device for this pool */
-	struct kib_tx	  *tpo_tx_descs;	   /* all the tx descriptors */
-	kib_pages_t	    *tpo_tx_pages;	   /* premapped tx msg pages */
+	kib_pool_t            tpo_pool;           /* pool */
+	struct kib_hca_dev    *tpo_hdev;          /* device for this pool */
+	struct kib_tx         *tpo_tx_descs;      /* all the tx descriptors */
+	kib_pages_t           *tpo_tx_pages;      /* premapped tx msg pages */
 } kib_tx_pool_t;
 
 typedef struct {
-	kib_poolset_t	   pps_poolset;	    /* pool-set */
+	kib_poolset_t         pps_poolset;        /* pool-set */
 } kib_pmr_poolset_t;
 
 typedef struct kib_pmr_pool {
-	struct kib_hca_dev     *ppo_hdev;	       /* device for this pool */
-	kib_pool_t	      ppo_pool;	       /* pool */
+	struct kib_hca_dev    *ppo_hdev;          /* device for this pool */
+	kib_pool_t            ppo_pool;           /* pool */
 } kib_pmr_pool_t;
 
 typedef struct {
-	spinlock_t		fps_lock;		/* serialize */
-	struct kib_net	 *fps_net;		/* IB network */
-	struct list_head	      fps_pool_list;	  /* FMR pool list */
-	struct list_head	      fps_failed_pool_list;   /* FMR pool list */
-	__u64		   fps_version;	    /* validity stamp */
-	int			fps_cpt;		/* CPT id */
-	int			fps_pool_size;
-	int			fps_flush_trigger;
-	/* is allocating new pool */
-	int			fps_increasing;
-	/* time stamp for retry if failed to allocate */
-	unsigned long		fps_next_retry;
+	spinlock_t            fps_lock;            /* serialize */
+	struct kib_net        *fps_net;            /* IB network */
+	struct list_head      fps_pool_list;       /* FMR pool list */
+	struct list_head      fps_failed_pool_list;/* FMR pool list */
+	__u64                 fps_version;         /* validity stamp */
+	int                   fps_cpt;             /* CPT id */
+	int                   fps_pool_size;
+	int                   fps_flush_trigger;
+	int                   fps_increasing;      /* is allocating new pool */
+	unsigned long         fps_next_retry;      /* time stamp for retry if
+						    * failed to allocate */
 } kib_fmr_poolset_t;
 
 typedef struct {
-	struct list_head	      fpo_list;	       /* chain on pool list */
-	struct kib_hca_dev     *fpo_hdev;	       /* device for this pool */
-	kib_fmr_poolset_t      *fpo_owner;	      /* owner of this pool */
-	struct ib_fmr_pool     *fpo_fmr_pool;	   /* IB FMR pool */
-	unsigned long	      fpo_deadline;	   /* deadline of this pool */
-	int		     fpo_failed;	     /* fmr pool is failed */
-	int		     fpo_map_count;	  /* # of mapped FMR */
+	struct list_head      fpo_list;            /* chain on pool list */
+	struct kib_hca_dev    *fpo_hdev;           /* device for this pool */
+	kib_fmr_poolset_t     *fpo_owner;          /* owner of this pool */
+	struct ib_fmr_pool    *fpo_fmr_pool;       /* IB FMR pool */
+	unsigned long         fpo_deadline;        /* deadline of this pool */
+	int                   fpo_failed;          /* fmr pool is failed */
+	int                   fpo_map_count;       /* # of mapped FMR */
 } kib_fmr_pool_t;
 
 typedef struct {
-	struct ib_pool_fmr     *fmr_pfmr;	       /* IB pool fmr */
-	kib_fmr_pool_t	 *fmr_pool;	       /* pool of FMR */
+	struct ib_pool_fmr    *fmr_pfmr;           /* IB pool fmr */
+	kib_fmr_pool_t        *fmr_pool;           /* pool of FMR */
 } kib_fmr_t;
 
 typedef struct kib_net {
-	struct list_head	   ibn_list;	  /* chain on kib_dev_t::ibd_nets */
-	__u64		ibn_incarnation;   /* my epoch */
-	int		  ibn_init;	  /* initialisation state */
-	int		  ibn_shutdown;      /* shutting down? */
+	struct list_head      ibn_list;       /* chain on kib_dev_t::ibd_nets */
+	__u64                 ibn_incarnation;/* my epoch */
+	int                   ibn_init;       /* initialisation state */
+	int                   ibn_shutdown;   /* shutting down? */
 
-	atomic_t		ibn_npeers;	/* # peers extant */
-	atomic_t		ibn_nconns;	/* # connections extant */
+	atomic_t              ibn_npeers;     /* # peers extant */
+	atomic_t              ibn_nconns;     /* # connections extant */
 
-	kib_tx_poolset_t	**ibn_tx_ps;	/* tx pool-set */
-	kib_fmr_poolset_t	**ibn_fmr_ps;	/* fmr pool-set */
-	kib_pmr_poolset_t	**ibn_pmr_ps;	/* pmr pool-set */
+	kib_tx_poolset_t      **ibn_tx_ps;    /* tx pool-set */
+	kib_fmr_poolset_t     **ibn_fmr_ps;   /* fmr pool-set */
+	kib_pmr_poolset_t     **ibn_pmr_ps;   /* pmr pool-set */
 
-	kib_dev_t		*ibn_dev;	/* underlying IB device */
+	kib_dev_t             *ibn_dev;       /* underlying IB device */
 } kib_net_t;
 
 #define KIB_THREAD_SHIFT		16
@@ -350,51 +359,45 @@ typedef struct kib_net {
 #define KIB_THREAD_TID(id)		((id) &amp; ((1UL &lt;&lt; KIB_THREAD_SHIFT) - 1))
 
 struct kib_sched_info {
-	/* serialise */
-	spinlock_t		ibs_lock;
-	/* schedulers sleep here */
-	wait_queue_head_t		ibs_waitq;
-	/* conns to check for rx completions */
-	struct list_head		ibs_conns;
-	/* number of scheduler threads */
-	int			ibs_nthreads;
-	/* max allowed scheduler threads */
-	int			ibs_nthreads_max;
-	int			ibs_cpt;	/* CPT id */
+	spinlock_t         ibs_lock;     /* serialise */
+	wait_queue_head_t  ibs_waitq;    /* schedulers sleep here */
+	struct list_head   ibs_conns;    /* conns to check for rx completions */
+	int                ibs_nthreads; /* number of scheduler threads */
+	int                ibs_nthreads_max; /* max allowed scheduler threads */
+	int                ibs_cpt;      /* CPT id */
 };
 
 typedef struct {
-	int			kib_init;	/* initialisation state */
-	int			kib_shutdown;	/* shut down? */
-	struct list_head		kib_devs;	/* IB devices extant */
-	/* list head of failed devices */
-	struct list_head		kib_failed_devs;
-	/* schedulers sleep here */
-	wait_queue_head_t		kib_failover_waitq;
-	atomic_t		kib_nthreads;	/* # live threads */
-	/* stabilize net/dev/peer/conn ops */
-	rwlock_t		kib_global_lock;
-	/* hash table of all my known peers */
-	struct list_head		*kib_peers;
-	/* size of kib_peers */
-	int			kib_peer_hash_size;
-	/* the connd task (serialisation assertions) */
-	void			*kib_connd;
-	/* connections to setup/teardown */
-	struct list_head		kib_connd_conns;
-	/* connections with zero refcount */
-	struct list_head		kib_connd_zombies;
-	/* connection daemon sleeps here */
-	wait_queue_head_t		kib_connd_waitq;
-	spinlock_t		kib_connd_lock;	/* serialise */
-	struct ib_qp_attr	kib_error_qpa;	/* QP-&gt;ERROR */
-	/* percpt data for schedulers */
-	struct kib_sched_info	**kib_scheds;
+	int                   kib_init;           /* initialisation state */
+	int                   kib_shutdown;       /* shut down? */
+	struct list_head      kib_devs;           /* IB devices extant */
+	struct list_head      kib_failed_devs;    /* list head of failed
+						   * devices */
+	wait_queue_head_t     kib_failover_waitq; /* schedulers sleep here */
+	atomic_t              kib_nthreads;       /* # live threads */
+	rwlock_t              kib_global_lock;    /* stabilize net/dev/peer/conn
+						   * ops */
+	struct list_head      *kib_peers;         /* hash table of all my known
+						   * peers */
+	int                   kib_peer_hash_size; /* size of kib_peers */
+	void                  *kib_connd;         /* the connd task
+						   * (serialisation assertions)
+						   */
+	struct list_head      kib_connd_conns;    /* connections to
+						   * setup/teardown */
+	struct list_head      kib_connd_zombies;  /* connections with zero
+						   * refcount */
+	wait_queue_head_t     kib_connd_waitq;    /* connection daemon sleeps
+						   * here */
+	spinlock_t            kib_connd_lock;     /* serialise */
+	struct ib_qp_attr     kib_error_qpa;      /* QP-&gt;ERROR */
+	struct kib_sched_info **kib_scheds;       /* percpt data for schedulers
+						   */
 } kib_data_t;
 
-#define IBLND_INIT_NOTHING	 0
-#define IBLND_INIT_DATA	    1
-#define IBLND_INIT_ALL	     2
+#define IBLND_INIT_NOTHING 0
+#define IBLND_INIT_DATA    1
+#define IBLND_INIT_ALL     2
 
 /************************************************************************
  * IB Wire message format.
@@ -402,228 +405,243 @@ typedef struct {
  */
 
 typedef struct kib_connparams {
-	__u16	     ibcp_queue_depth;
-	__u16	     ibcp_max_frags;
-	__u32	     ibcp_max_msg_size;
+	__u16        ibcp_queue_depth;
+	__u16        ibcp_max_frags;
+	__u32        ibcp_max_msg_size;
 } WIRE_ATTR kib_connparams_t;
 
 typedef struct {
-	lnet_hdr_t	ibim_hdr;	     /* portals header */
-	char	      ibim_payload[0];      /* piggy-backed payload */
+	lnet_hdr_t   ibim_hdr;        /* portals header */
+	char         ibim_payload[0]; /* piggy-backed payload */
 } WIRE_ATTR kib_immediate_msg_t;
 
 typedef struct {
-	__u32	     rf_nob;	       /* # bytes this frag */
-	__u64	     rf_addr;	      /* CAVEAT EMPTOR: misaligned!! */
+	__u32        rf_nob;          /* # bytes this frag */
+	__u64        rf_addr;         /* CAVEAT EMPTOR: misaligned!! */
 } WIRE_ATTR kib_rdma_frag_t;
 
 typedef struct {
-	__u32	     rd_key;	       /* local/remote key */
-	__u32	     rd_nfrags;	    /* # fragments */
-	kib_rdma_frag_t   rd_frags[0];	  /* buffer frags */
+	__u32           rd_key;       /* local/remote key */
+	__u32           rd_nfrags;    /* # fragments */
+	kib_rdma_frag_t rd_frags[0];  /* buffer frags */
 } WIRE_ATTR kib_rdma_desc_t;
 
 typedef struct {
-	lnet_hdr_t	ibprm_hdr;	    /* portals header */
-	__u64	     ibprm_cookie;	 /* opaque completion cookie */
+	lnet_hdr_t      ibprm_hdr;    /* portals header */
+	__u64           ibprm_cookie; /* opaque completion cookie */
 } WIRE_ATTR kib_putreq_msg_t;
 
 typedef struct {
-	__u64	     ibpam_src_cookie;     /* reflected completion cookie */
-	__u64	     ibpam_dst_cookie;     /* opaque completion cookie */
-	kib_rdma_desc_t   ibpam_rd;	     /* sender's sink buffer */
+	__u64           ibpam_src_cookie; /* reflected completion cookie */
+	__u64           ibpam_dst_cookie; /* opaque completion cookie */
+	kib_rdma_desc_t ibpam_rd;         /* sender's sink buffer */
 } WIRE_ATTR kib_putack_msg_t;
 
 typedef struct {
-	lnet_hdr_t	ibgm_hdr;	     /* portals header */
-	__u64	     ibgm_cookie;	  /* opaque completion cookie */
-	kib_rdma_desc_t   ibgm_rd;	      /* rdma descriptor */
+	lnet_hdr_t      ibgm_hdr;     /* portals header */
+	__u64           ibgm_cookie;  /* opaque completion cookie */
+	kib_rdma_desc_t ibgm_rd;      /* rdma descriptor */
 } WIRE_ATTR kib_get_msg_t;
 
 typedef struct {
-	__u64	     ibcm_cookie;	  /* opaque completion cookie */
-	__s32	     ibcm_status;	  /* &lt; 0 failure: &gt;= 0 length */
+	__u64           ibcm_cookie;  /* opaque completion cookie */
+	__s32           ibcm_status;  /* &lt; 0 failure: &gt;= 0 length */
 } WIRE_ATTR kib_completion_msg_t;
 
 typedef struct {
 	/* First 2 fields fixed FOR ALL TIME */
-	__u32	     ibm_magic;	    /* I'm an ibnal message */
-	__u16	     ibm_version;	  /* this is my version number */
-
-	__u8	      ibm_type;	     /* msg type */
-	__u8	      ibm_credits;	  /* returned credits */
-	__u32	     ibm_nob;	      /* # bytes in whole message */
-	__u32	     ibm_cksum;	    /* checksum (0 == no checksum) */
-	__u64	     ibm_srcnid;	   /* sender's NID */
-	__u64	     ibm_srcstamp;	 /* sender's incarnation */
-	__u64	     ibm_dstnid;	   /* destination's NID */
-	__u64	     ibm_dststamp;	 /* destination's incarnation */
+	__u32           ibm_magic;    /* I'm an ibnal message */
+	__u16           ibm_version;  /* this is my version number */
+
+	__u8            ibm_type;     /* msg type */
+	__u8            ibm_credits;  /* returned credits */
+	__u32           ibm_nob;      /* # bytes in whole message */
+	__u32           ibm_cksum;    /* checksum (0 == no checksum) */
+	__u64           ibm_srcnid;   /* sender's NID */
+	__u64           ibm_srcstamp; /* sender's incarnation */
+	__u64           ibm_dstnid;   /* destination's NID */
+	__u64           ibm_dststamp; /* destination's incarnation */
 
 	union {
-		kib_connparams_t      connparams;
-		kib_immediate_msg_t   immediate;
-		kib_putreq_msg_t      putreq;
-		kib_putack_msg_t      putack;
-		kib_get_msg_t	 get;
-		kib_completion_msg_t  completion;
+		kib_connparams_t     connparams;
+		kib_immediate_msg_t  immediate;
+		kib_putreq_msg_t     putreq;
+		kib_putack_msg_t     putack;
+		kib_get_msg_t        get;
+		kib_completion_msg_t completion;
 	} WIRE_ATTR ibm_u;
 } WIRE_ATTR kib_msg_t;
 
-#define IBLND_MSG_MAGIC LNET_PROTO_IB_MAGIC	/* unique magic */
+#define IBLND_MSG_MAGIC     LNET_PROTO_IB_MAGIC /* unique magic */
 
-#define IBLND_MSG_VERSION_1	 0x11
-#define IBLND_MSG_VERSION_2	 0x12
-#define IBLND_MSG_VERSION	   IBLND_MSG_VERSION_2
+#define IBLND_MSG_VERSION_1 0x11
+#define IBLND_MSG_VERSION_2 0x12
+#define IBLND_MSG_VERSION   IBLND_MSG_VERSION_2
 
-#define IBLND_MSG_CONNREQ	   0xc0	/* connection request */
-#define IBLND_MSG_CONNACK	   0xc1	/* connection acknowledge */
-#define IBLND_MSG_NOOP	      0xd0	/* nothing (just credits) */
-#define IBLND_MSG_IMMEDIATE	 0xd1	/* immediate */
-#define IBLND_MSG_PUT_REQ	   0xd2	/* putreq (src-&gt;sink) */
-#define IBLND_MSG_PUT_NAK	   0xd3	/* completion (sink-&gt;src) */
-#define IBLND_MSG_PUT_ACK	   0xd4	/* putack (sink-&gt;src) */
-#define IBLND_MSG_PUT_DONE	  0xd5	/* completion (src-&gt;sink) */
-#define IBLND_MSG_GET_REQ	   0xd6	/* getreq (sink-&gt;src) */
-#define IBLND_MSG_GET_DONE	  0xd7	/* completion (src-&gt;sink: all OK) */
+#define IBLND_MSG_CONNREQ   0xc0	/* connection request */
+#define IBLND_MSG_CONNACK   0xc1	/* connection acknowledge */
+#define IBLND_MSG_NOOP      0xd0	/* nothing (just credits) */
+#define IBLND_MSG_IMMEDIATE 0xd1	/* immediate */
+#define IBLND_MSG_PUT_REQ   0xd2	/* putreq (src-&gt;sink) */
+#define IBLND_MSG_PUT_NAK   0xd3	/* completion (sink-&gt;src) */
+#define IBLND_MSG_PUT_ACK   0xd4	/* putack (sink-&gt;src) */
+#define IBLND_MSG_PUT_DONE  0xd5	/* completion (src-&gt;sink) */
+#define IBLND_MSG_GET_REQ   0xd6	/* getreq (sink-&gt;src) */
+#define IBLND_MSG_GET_DONE  0xd7	/* completion (src-&gt;sink: all OK) */
 
 typedef struct {
-	__u32	    ibr_magic;	     /* sender's magic */
-	__u16	    ibr_version;	   /* sender's version */
-	__u8	     ibr_why;	       /* reject reason */
-	__u8	     ibr_padding;	   /* padding */
-	__u64	    ibr_incarnation;       /* incarnation of peer */
-	kib_connparams_t ibr_cp;		/* connection parameters */
+	__u32            ibr_magic;       /* sender's magic */
+	__u16            ibr_version;     /* sender's version */
+	__u8             ibr_why;         /* reject reason */
+	__u8             ibr_padding;     /* padding */
+	__u64            ibr_incarnation; /* incarnation of peer */
+	kib_connparams_t ibr_cp;          /* connection parameters */
 } WIRE_ATTR kib_rej_t;
 
 /* connection rejection reasons */
-#define IBLND_REJECT_CONN_RACE       1	  /* You lost connection race */
-#define IBLND_REJECT_NO_RESOURCES    2	  /* Out of memory/conns etc */
-#define IBLND_REJECT_FATAL	   3	  /* Anything else */
-
-#define IBLND_REJECT_CONN_UNCOMPAT   4	  /* incompatible version peer */
-#define IBLND_REJECT_CONN_STALE      5	  /* stale peer */
-
-#define IBLND_REJECT_RDMA_FRAGS      6	  /* Fatal: peer's rdma frags can't match mine */
-#define IBLND_REJECT_MSG_QUEUE_SIZE  7	  /* Fatal: peer's msg queue size can't match mine */
+#define IBLND_REJECT_CONN_RACE      1 /* You lost connection race */
+#define IBLND_REJECT_NO_RESOURCES   2 /* Out of memory/conns etc */
+#define IBLND_REJECT_FATAL          3 /* Anything else */
+#define IBLND_REJECT_CONN_UNCOMPAT  4 /* incompatible version peer */
+#define IBLND_REJECT_CONN_STALE     5 /* stale peer */
+#define IBLND_REJECT_RDMA_FRAGS     6 /* Fatal: peer's rdma frags can't match
+				       * mine */
+#define IBLND_REJECT_MSG_QUEUE_SIZE 7 /* Fatal: peer's msg queue size can't
+				       * match mine */
 
 /***********************************************************************/
 
-typedef struct kib_rx			   /* receive message */
+typedef struct kib_rx                         /* receive message */
 {
-	struct list_head		rx_list;      /* queue for attention */
-	struct kib_conn	  *rx_conn;      /* owning conn */
-	int		       rx_nob;       /* # bytes received (-1 while posted) */
-	enum ib_wc_status	 rx_status;    /* completion status */
-	kib_msg_t		*rx_msg;       /* message buffer (host vaddr) */
-	__u64		     rx_msgaddr;   /* message buffer (I/O addr) */
-	DECLARE_PCI_UNMAP_ADDR   (rx_msgunmap); /* for dma_unmap_single() */
-	struct ib_recv_wr	 rx_wrq;       /* receive work item... */
-	struct ib_sge	     rx_sge;       /* ...and its memory */
+	struct list_head       rx_list;       /* queue for attention */
+	struct kib_conn        *rx_conn;      /* owning conn */
+	int                    rx_nob;        /* # bytes received (-1 while
+					       * posted) */
+	enum ib_wc_status      rx_status;     /* completion status */
+	kib_msg_t              *rx_msg;       /* message buffer (host vaddr) */
+	__u64                  rx_msgaddr;    /* message buffer (I/O addr) */
+	DECLARE_PCI_UNMAP_ADDR (rx_msgunmap); /* for dma_unmap_single() */
+	struct ib_recv_wr      rx_wrq;        /* receive work item... */
+	struct ib_sge          rx_sge;        /* ...and its memory */
 } kib_rx_t;
 
-#define IBLND_POSTRX_DONT_POST    0	     /* don't post */
-#define IBLND_POSTRX_NO_CREDIT    1	     /* post: no credits */
-#define IBLND_POSTRX_PEER_CREDIT  2	     /* post: give peer back 1 credit */
-#define IBLND_POSTRX_RSRVD_CREDIT 3	     /* post: give myself back 1 reserved credit */
+#define IBLND_POSTRX_DONT_POST    0   /* don't post */
+#define IBLND_POSTRX_NO_CREDIT    1   /* post: no credits */
+#define IBLND_POSTRX_PEER_CREDIT  2   /* post: give peer back 1 credit */
+#define IBLND_POSTRX_RSRVD_CREDIT 3   /* post: give myself back 1 reserved
+				       * credit */
 
-typedef struct kib_tx			   /* transmit message */
+typedef struct kib_tx                         /* transmit message */
 {
-	struct list_head		tx_list;      /* queue on idle_txs ibc_tx_queue etc. */
-	kib_tx_pool_t	    *tx_pool;      /* pool I'm from */
-	struct kib_conn	  *tx_conn;      /* owning conn */
-	short		     tx_sending;   /* # tx callbacks outstanding */
-	short		     tx_queued;    /* queued for sending */
-	short		     tx_waiting;   /* waiting for peer */
-	int		       tx_status;    /* LNET completion status */
-	unsigned long	     tx_deadline;  /* completion deadline */
-	__u64		     tx_cookie;    /* completion cookie */
-	lnet_msg_t	       *tx_lntmsg[2]; /* lnet msgs to finalize on completion */
-	kib_msg_t		*tx_msg;       /* message buffer (host vaddr) */
-	__u64		     tx_msgaddr;   /* message buffer (I/O addr) */
-	DECLARE_PCI_UNMAP_ADDR   (tx_msgunmap); /* for dma_unmap_single() */
-	int		       tx_nwrq;      /* # send work items */
-	struct ib_send_wr	*tx_wrq;       /* send work items... */
-	struct ib_sge	    *tx_sge;       /* ...and their memory */
-	kib_rdma_desc_t	  *tx_rd;	/* rdma descriptor */
-	int		       tx_nfrags;    /* # entries in... */
-	struct scatterlist       *tx_frags;     /* dma_map_sg descriptor */
-	__u64		    *tx_pages;     /* rdma phys page addrs */
+	struct list_head       tx_list;       /* queue on idle_txs ibc_tx_queue
+					       * etc. */
+	kib_tx_pool_t          *tx_pool;      /* pool I'm from */
+	struct kib_conn        *tx_conn;      /* owning conn */
+	short                  tx_sending;    /* # tx callbacks outstanding */
+	short                  tx_queued;     /* queued for sending */
+	short                  tx_waiting;    /* waiting for peer */
+	int                    tx_status;     /* LNET completion status */
+	unsigned long          tx_deadline;   /* completion deadline */
+	__u64                  tx_cookie;     /* completion cookie */
+	lnet_msg_t             *tx_lntmsg[2]; /* lnet msgs to finalize on
+					       * completion */
+	kib_msg_t              *tx_msg;       /* message buffer (host vaddr) */
+	__u64                  tx_msgaddr;    /* message buffer (I/O addr) */
+	DECLARE_PCI_UNMAP_ADDR (tx_msgunmap); /* for dma_unmap_single() */
+	int                    tx_nwrq;       /* # send work items */
+	struct ib_send_wr      *tx_wrq;       /* send work items... */
+	struct ib_sge          *tx_sge;       /* ...and their memory */
+	kib_rdma_desc_t        *tx_rd;        /* rdma descriptor */
+	int                    tx_nfrags;     /* # entries in... */
+	struct scatterlist     *tx_frags;     /* dma_map_sg descriptor */
+	__u64                  *tx_pages;     /* rdma phys page addrs */
 	union {
-		kib_phys_mr_t      *pmr;	/* MR for physical buffer */
-		kib_fmr_t	   fmr;	/* FMR */
-	}			 tx_u;
-	int		       tx_dmadir;    /* dma direction */
+		kib_phys_mr_t  *pmr;          /* MR for physical buffer */
+		kib_fmr_t      fmr;           /* FMR */
+	}                      tx_u;
+	int                    tx_dmadir;     /* dma direction */
 } kib_tx_t;
 
 typedef struct kib_connvars {
-	/* connection-in-progress variables */
-	kib_msg_t		 cv_msg;
+	kib_msg_t cv_msg; /* connection-in-progress variables */
 } kib_connvars_t;
 
 typedef struct kib_conn {
-	struct kib_sched_info *ibc_sched;	/* scheduler information */
-	struct kib_peer     *ibc_peer;	  /* owning peer */
-	kib_hca_dev_t       *ibc_hdev;	  /* HCA bound on */
-	struct list_head	   ibc_list;	  /* stash on peer's conn list */
-	struct list_head	   ibc_sched_list;    /* schedule for attention */
-	__u16		ibc_version;       /* version of connection */
-	__u64		ibc_incarnation;   /* which instance of the peer */
-	atomic_t	 ibc_refcount;      /* # users */
-	int		  ibc_state;	 /* what's happening */
-	int		  ibc_nsends_posted; /* # uncompleted sends */
-	int		  ibc_noops_posted;  /* # uncompleted NOOPs */
-	int		  ibc_credits;       /* # credits I have */
-	int		  ibc_outstanding_credits; /* # credits to return */
-	int		  ibc_reserved_credits;/* # ACK/DONE msg credits */
-	int		  ibc_comms_error;   /* set on comms error */
-	unsigned int	     ibc_nrx:16;	/* receive buffers owned */
-	unsigned int	     ibc_scheduled:1;   /* scheduled for attention */
-	unsigned int	     ibc_ready:1;       /* CQ callback fired */
-	/* time of last send */
-	unsigned long	ibc_last_send;
-	/** link chain for kiblnd_check_conns only */
-	struct list_head	   ibc_connd_list;
-	/** rxs completed before ESTABLISHED */
-	struct list_head	   ibc_early_rxs;
-	/** IBLND_MSG_NOOPs for IBLND_MSG_VERSION_1 */
-	struct list_head	   ibc_tx_noops;
-	struct list_head	   ibc_tx_queue;       /* sends that need a credit */
-	struct list_head	   ibc_tx_queue_nocred;/* sends that don't need a credit */
-	struct list_head	   ibc_tx_queue_rsrvd; /* sends that need to reserve an ACK/DONE msg */
-	struct list_head	   ibc_active_txs;     /* active tx awaiting completion */
-	spinlock_t	     ibc_lock;		 /* serialise */
-	kib_rx_t	    *ibc_rxs;	    /* the rx descs */
-	kib_pages_t	 *ibc_rx_pages;       /* premapped rx msg pages */
-
-	struct rdma_cm_id   *ibc_cmid;	   /* CM id */
-	struct ib_cq	*ibc_cq;	     /* completion queue */
-
-	kib_connvars_t      *ibc_connvars;       /* in-progress connection state */
+	struct kib_sched_info *ibc_sched;           /* scheduler information */
+	struct kib_peer       *ibc_peer;            /* owning peer */
+	kib_hca_dev_t         *ibc_hdev;            /* HCA bound on */
+	struct list_head      ibc_list;             /* stash on peer's conn
+						     * list */
+	struct list_head      ibc_sched_list;       /* schedule for attention */
+	__u16                 ibc_version;          /* version of connection */
+	__u64                 ibc_incarnation;      /* which instance of the
+						     * peer */
+	atomic_t              ibc_refcount;         /* # users */
+	int                   ibc_state;            /* what's happening */
+	int                   ibc_nsends_posted;    /* # uncompleted sends */
+	int                   ibc_noops_posted;     /* # uncompleted NOOPs */
+	int                   ibc_credits;          /* # credits I have */
+	int                   ibc_outstanding_credits; /* # credits to return */
+	int                   ibc_reserved_credits; /* # ACK/DONE msg credits */
+	int                   ibc_comms_error;      /* set on comms error */
+	unsigned int          ibc_nrx:16;           /* receive buffers owned */
+	unsigned int          ibc_scheduled:1;      /* scheduled for attention
+						     */
+	unsigned int          ibc_ready:1;          /* CQ callback fired */
+	unsigned long         ibc_last_send;        /* time of last send */
+	struct list_head      ibc_connd_list;       /* link chain for
+						     * kiblnd_check_conns only
+						     */
+	struct list_head      ibc_early_rxs;        /* rxs completed before
+						     * ESTABLISHED */
+	struct list_head      ibc_tx_noops;         /* IBLND_MSG_NOOPs for
+						     * IBLND_MSG_VERSION_1 */
+	struct list_head      ibc_tx_queue;         /* sends that need a credit
+						     */
+	struct list_head      ibc_tx_queue_nocred;  /* sends that don't need a
+						     * credit */
+	struct list_head      ibc_tx_queue_rsrvd;   /* sends that need to
+						     * reserve an ACK/DONE msg
+						     */
+	struct list_head      ibc_active_txs;       /* active tx awaiting
+						     * completion */
+	spinlock_t            ibc_lock;             /* serialise */
+	kib_rx_t              *ibc_rxs;             /* the rx descs */
+	kib_pages_t           *ibc_rx_pages;        /* premapped rx msg pages */
+
+	struct rdma_cm_id     *ibc_cmid;            /* CM id */
+	struct ib_cq          *ibc_cq;              /* completion queue */
+
+	kib_connvars_t        *ibc_connvars;        /* in-progress connection
+						     * state */
 } kib_conn_t;
 
-#define IBLND_CONN_INIT	       0	 /* being initialised */
-#define IBLND_CONN_ACTIVE_CONNECT     1	 /* active sending req */
-#define IBLND_CONN_PASSIVE_WAIT       2	 /* passive waiting for rtu */
-#define IBLND_CONN_ESTABLISHED	3	 /* connection established */
-#define IBLND_CONN_CLOSING	    4	 /* being closed */
-#define IBLND_CONN_DISCONNECTED       5	 /* disconnected */
+#define IBLND_CONN_INIT           0	 /* being initialised */
+#define IBLND_CONN_ACTIVE_CONNECT 1	 /* active sending req */
+#define IBLND_CONN_PASSIVE_WAIT   2	 /* passive waiting for rtu */
+#define IBLND_CONN_ESTABLISHED    3	 /* connection established */
+#define IBLND_CONN_CLOSING        4	 /* being closed */
+#define IBLND_CONN_DISCONNECTED   5	 /* disconnected */
 
 typedef struct kib_peer {
-	struct list_head	   ibp_list;	   /* stash on global peer list */
-	lnet_nid_t	   ibp_nid;	    /* who's on the other end(s) */
-	lnet_ni_t	   *ibp_ni;	     /* LNet interface */
-	atomic_t	 ibp_refcount;       /* # users */
-	struct list_head	   ibp_conns;	  /* all active connections */
-	struct list_head	   ibp_tx_queue;       /* msgs waiting for a conn */
-	__u16		ibp_version;	/* version of peer */
-	__u64		ibp_incarnation;    /* incarnation of peer */
-	int		  ibp_connecting;     /* current active connection attempts */
-	int		  ibp_accepting;      /* current passive connection attempts */
-	int		  ibp_error;	  /* errno on closing this peer */
-	unsigned long	   ibp_last_alive;     /* when (in jiffies) I was last alive */
+	struct list_head ibp_list;        /* stash on global peer list */
+	lnet_nid_t       ibp_nid;         /* who's on the other end(s) */
+	lnet_ni_t        *ibp_ni;         /* LNet interface */
+	atomic_t         ibp_refcount;    /* # users */
+	struct list_head ibp_conns;       /* all active connections */
+	struct list_head ibp_tx_queue;    /* msgs waiting for a conn */
+	__u16            ibp_version;     /* version of peer */
+	__u64            ibp_incarnation; /* incarnation of peer */
+	int              ibp_connecting;  /* current active connection attempts
+					   */
+	int              ibp_accepting;   /* current passive connection attempts
+					   */
+	int              ibp_error;       /* errno on closing this peer */
+	unsigned long    ibp_last_alive;  /* when (in jiffies) I was last alive
+					   */
 } kib_peer_t;
 
-extern kib_data_t      kiblnd_data;
+extern kib_data_t kiblnd_data;
 
 extern void kiblnd_hdev_destroy(kib_hca_dev_t *hdev);
 
@@ -941,8 +959,8 @@ static inline unsigned int kiblnd_sg_dma_len(struct ib_device *dev,
  * right because OFED1.2 defines it as const, to use it we have to add
  * (void *) cast to overcome "const" */
 
-#define KIBLND_CONN_PARAM(e)	    ((e)-&gt;param.conn.private_data)
-#define KIBLND_CONN_PARAM_LEN(e)	((e)-&gt;param.conn.private_data_len)
+#define KIBLND_CONN_PARAM(e)     ((e)-&gt;param.conn.private_data)
+#define KIBLND_CONN_PARAM_LEN(e) ((e)-&gt;param.conn.private_data_len)
 
 
 struct ib_mr *kiblnd_find_rd_dma_mr(kib_hca_dev_t *hdev,
diff --git a/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_cb.c b/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_cb.c
index dbf3749831f9..477aa8b76f32 100644
--- a/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_cb.c
+++ b/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_cb.c
@@ -44,9 +44,9 @@ static void
 kiblnd_tx_done(lnet_ni_t *ni, kib_tx_t *tx)
 {
 	lnet_msg_t *lntmsg[2];
-	kib_net_t  *net = ni-&gt;ni_data;
-	int	 rc;
-	int	 i;
+	kib_net_t *net = ni-&gt;ni_data;
+	int rc;
+	int i;
 
 	LASSERT(net != NULL);
 	LASSERT(!in_interrupt());
@@ -102,10 +102,10 @@ kiblnd_txlist_done(lnet_ni_t *ni, struct list_head *txlist, int status)
 static kib_tx_t *
 kiblnd_get_idle_tx(lnet_ni_t *ni, lnet_nid_t target)
 {
-	kib_net_t		*net = (kib_net_t *)ni-&gt;ni_data;
-	struct list_head		*node;
-	kib_tx_t		*tx;
-	kib_tx_poolset_t	*tps;
+	kib_net_t *net = (kib_net_t *)ni-&gt;ni_data;
+	struct list_head *node;
+	kib_tx_t *tx;
+	kib_tx_poolset_t *tps;
 
 	tps = net-&gt;ibn_tx_ps[lnet_cpt_of_nid(target)];
 	node = kiblnd_pool_alloc_node(&amp;tps-&gt;tps_poolset);
@@ -130,9 +130,9 @@ kiblnd_get_idle_tx(lnet_ni_t *ni, lnet_nid_t target)
 static void
 kiblnd_drop_rx(kib_rx_t *rx)
 {
-	kib_conn_t		*conn	= rx-&gt;rx_conn;
-	struct kib_sched_info	*sched	= conn-&gt;ibc_sched;
-	unsigned long		flags;
+	kib_conn_t *conn = rx-&gt;rx_conn;
+	struct kib_sched_info *sched = conn-&gt;ibc_sched;
+	unsigned long flags;
 
 	spin_lock_irqsave(&amp;sched-&gt;ibs_lock, flags);
 	LASSERT(conn-&gt;ibc_nrx &gt; 0);
@@ -145,11 +145,11 @@ kiblnd_drop_rx(kib_rx_t *rx)
 int
 kiblnd_post_rx(kib_rx_t *rx, int credit)
 {
-	kib_conn_t	 *conn = rx-&gt;rx_conn;
-	kib_net_t	  *net = conn-&gt;ibc_peer-&gt;ibp_ni-&gt;ni_data;
-	struct ib_recv_wr  *bad_wrq = NULL;
-	struct ib_mr       *mr;
-	int		 rc;
+	kib_conn_t *conn = rx-&gt;rx_conn;
+	kib_net_t *net = conn-&gt;ibc_peer-&gt;ibp_ni-&gt;ni_data;
+	struct ib_recv_wr *bad_wrq = NULL;
+	struct ib_mr *mr;
+	int rc;
 
 	LASSERT(net != NULL);
 	LASSERT(!in_interrupt());
@@ -164,10 +164,10 @@ kiblnd_post_rx(kib_rx_t *rx, int credit)
 	rx-&gt;rx_sge.addr   = rx-&gt;rx_msgaddr;
 	rx-&gt;rx_sge.length = IBLND_MSG_SIZE;
 
-	rx-&gt;rx_wrq.next = NULL;
+	rx-&gt;rx_wrq.next    = NULL;
 	rx-&gt;rx_wrq.sg_list = &amp;rx-&gt;rx_sge;
 	rx-&gt;rx_wrq.num_sge = 1;
-	rx-&gt;rx_wrq.wr_id = kiblnd_ptr2wreqid(rx, IBLND_WID_RX);
+	rx-&gt;rx_wrq.wr_id   = kiblnd_ptr2wreqid(rx, IBLND_WID_RX);
 
 	LASSERT(conn-&gt;ibc_state &gt;= IBLND_CONN_INIT);
 	LASSERT(rx-&gt;rx_nob &gt;= 0);	      /* not posted */
@@ -212,7 +212,7 @@ kiblnd_post_rx(kib_rx_t *rx, int credit)
 static kib_tx_t *
 kiblnd_find_waiting_tx_locked(kib_conn_t *conn, int txtype, __u64 cookie)
 {
-	struct list_head   *tmp;
+	struct list_head *tmp;
 
 	list_for_each(tmp, &amp;conn-&gt;ibc_active_txs) {
 		kib_tx_t *tx = list_entry(tmp, kib_tx_t, tx_list);
@@ -237,9 +237,9 @@ kiblnd_find_waiting_tx_locked(kib_conn_t *conn, int txtype, __u64 cookie)
 static void
 kiblnd_handle_completion(kib_conn_t *conn, int txtype, int status, __u64 cookie)
 {
-	kib_tx_t    *tx;
-	lnet_ni_t   *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
-	int	  idle;
+	kib_tx_t *tx;
+	lnet_ni_t *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
+	int idle;
 
 	spin_lock(&amp;conn-&gt;ibc_lock);
 
@@ -276,8 +276,8 @@ kiblnd_handle_completion(kib_conn_t *conn, int txtype, int status, __u64 cookie)
 static void
 kiblnd_send_completion(kib_conn_t *conn, int type, int status, __u64 cookie)
 {
-	lnet_ni_t   *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
-	kib_tx_t    *tx = kiblnd_get_idle_tx(ni, conn-&gt;ibc_peer-&gt;ibp_nid);
+	lnet_ni_t *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
+	kib_tx_t *tx = kiblnd_get_idle_tx(ni, conn-&gt;ibc_peer-&gt;ibp_nid);
 
 	if (tx == NULL) {
 		CERROR("Can't get tx for completion %x for %s\n",
@@ -295,14 +295,14 @@ kiblnd_send_completion(kib_conn_t *conn, int type, int status, __u64 cookie)
 static void
 kiblnd_handle_rx(kib_rx_t *rx)
 {
-	kib_msg_t    *msg = rx-&gt;rx_msg;
-	kib_conn_t   *conn = rx-&gt;rx_conn;
-	lnet_ni_t    *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
-	int	   credits = msg-&gt;ibm_credits;
-	kib_tx_t     *tx;
-	int	   rc = 0;
-	int	   rc2;
-	int	   post_credit;
+	kib_msg_t *msg = rx-&gt;rx_msg;
+	kib_conn_t *conn = rx-&gt;rx_conn;
+	lnet_ni_t *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
+	int credits = msg-&gt;ibm_credits;
+	kib_tx_t *tx;
+	int rc = 0;
+	int rc2;
+	int post_credit;
 
 	LASSERT(conn-&gt;ibc_state &gt;= IBLND_CONN_ESTABLISHED);
 
@@ -456,12 +456,12 @@ kiblnd_handle_rx(kib_rx_t *rx)
 static void
 kiblnd_rx_complete(kib_rx_t *rx, int status, int nob)
 {
-	kib_msg_t    *msg = rx-&gt;rx_msg;
-	kib_conn_t   *conn = rx-&gt;rx_conn;
-	lnet_ni_t    *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
-	kib_net_t    *net = ni-&gt;ni_data;
-	int	   rc;
-	int	   err = -EIO;
+	kib_msg_t *msg = rx-&gt;rx_msg;
+	kib_conn_t *conn = rx-&gt;rx_conn;
+	lnet_ni_t *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
+	kib_net_t *net = ni-&gt;ni_data;
+	int rc;
+	int err = -EIO;
 
 	LASSERT(net != NULL);
 	LASSERT(rx-&gt;rx_nob &lt; 0);	       /* was posted */
@@ -502,8 +502,8 @@ kiblnd_rx_complete(kib_rx_t *rx, int status, int nob)
 	/* racing with connection establishment/teardown! */
 
 	if (conn-&gt;ibc_state &lt; IBLND_CONN_ESTABLISHED) {
-		rwlock_t  *g_lock = &amp;kiblnd_data.kib_global_lock;
-		unsigned long  flags;
+		rwlock_t *g_lock = &amp;kiblnd_data.kib_global_lock;
+		unsigned long flags;
 
 		write_lock_irqsave(g_lock, flags);
 		/* must check holding global lock to eliminate race */
@@ -550,19 +550,19 @@ kiblnd_kvaddr_to_page(unsigned long vaddr)
 static int
 kiblnd_fmr_map_tx(kib_net_t *net, kib_tx_t *tx, kib_rdma_desc_t *rd, int nob)
 {
-	kib_hca_dev_t		*hdev;
-	__u64			*pages = tx-&gt;tx_pages;
-	kib_fmr_poolset_t	*fps;
-	int			npages;
-	int			size;
-	int			cpt;
-	int			rc;
-	int			i;
+	kib_hca_dev_t *hdev;
+	__u64 *pages = tx-&gt;tx_pages;
+	kib_fmr_poolset_t *fps;
+	int npages;
+	int size;
+	int cpt;
+	int rc;
+	int i;
 
 	LASSERT(tx-&gt;tx_pool != NULL);
 	LASSERT(tx-&gt;tx_pool-&gt;tpo_pool.po_owner != NULL);
 
-	hdev  = tx-&gt;tx_pool-&gt;tpo_hdev;
+	hdev = tx-&gt;tx_pool-&gt;tpo_hdev;
 
 	for (i = 0, npages = 0; i &lt; rd-&gt;rd_nfrags; i++) {
 		for (size = 0; size &lt;  rd-&gt;rd_frags[i].rf_nob;
@@ -586,7 +586,7 @@ kiblnd_fmr_map_tx(kib_net_t *net, kib_tx_t *tx, kib_rdma_desc_t *rd, int nob)
 	rd-&gt;rd_key = (rd != tx-&gt;tx_rd) ? tx-&gt;tx_u.fmr.fmr_pfmr-&gt;fmr-&gt;rkey :
 					 tx-&gt;tx_u.fmr.fmr_pfmr-&gt;fmr-&gt;lkey;
 	rd-&gt;rd_frags[0].rf_addr &amp;= ~hdev-&gt;ibh_page_mask;
-	rd-&gt;rd_frags[0].rf_nob   = nob;
+	rd-&gt;rd_frags[0].rf_nob = nob;
 	rd-&gt;rd_nfrags = 1;
 
 	return 0;
@@ -595,11 +595,11 @@ kiblnd_fmr_map_tx(kib_net_t *net, kib_tx_t *tx, kib_rdma_desc_t *rd, int nob)
 static int
 kiblnd_pmr_map_tx(kib_net_t *net, kib_tx_t *tx, kib_rdma_desc_t *rd, int nob)
 {
-	kib_hca_dev_t		*hdev;
-	kib_pmr_poolset_t	*pps;
-	__u64			iova;
-	int			cpt;
-	int			rc;
+	kib_hca_dev_t *hdev;
+	kib_pmr_poolset_t *pps;
+	__u64 iova;
+	int cpt;
+	int rc;
 
 	LASSERT(tx-&gt;tx_pool != NULL);
 	LASSERT(tx-&gt;tx_pool-&gt;tpo_pool.po_owner != NULL);
@@ -623,7 +623,7 @@ kiblnd_pmr_map_tx(kib_net_t *net, kib_tx_t *tx, kib_rdma_desc_t *rd, int nob)
 					 tx-&gt;tx_u.pmr-&gt;pmr_mr-&gt;lkey;
 	rd-&gt;rd_nfrags = 1;
 	rd-&gt;rd_frags[0].rf_addr = iova;
-	rd-&gt;rd_frags[0].rf_nob  = nob;
+	rd-&gt;rd_frags[0].rf_nob = nob;
 
 	return 0;
 }
@@ -631,7 +631,7 @@ kiblnd_pmr_map_tx(kib_net_t *net, kib_tx_t *tx, kib_rdma_desc_t *rd, int nob)
 void
 kiblnd_unmap_tx(lnet_ni_t *ni, kib_tx_t *tx)
 {
-	kib_net_t  *net = ni-&gt;ni_data;
+	kib_net_t *net = ni-&gt;ni_data;
 
 	LASSERT(net != NULL);
 
@@ -655,20 +655,19 @@ int
 kiblnd_map_tx(lnet_ni_t *ni, kib_tx_t *tx,
 	      kib_rdma_desc_t *rd, int nfrags)
 {
-	kib_hca_dev_t      *hdev  = tx-&gt;tx_pool-&gt;tpo_hdev;
-	kib_net_t	  *net   = ni-&gt;ni_data;
-	struct ib_mr       *mr    = NULL;
-	__u32	       nob;
-	int		 i;
+	kib_hca_dev_t *hdev = tx-&gt;tx_pool-&gt;tpo_hdev;
+	kib_net_t *net = ni-&gt;ni_data;
+	struct ib_mr *mr    = NULL;
+	__u32 nob;
+	int i;
 
 	/* If rd is not tx_rd, it's going to get sent to a peer and I'm the
 	 * RDMA sink */
 	tx-&gt;tx_dmadir = (rd != tx-&gt;tx_rd) ? DMA_FROM_DEVICE : DMA_TO_DEVICE;
 	tx-&gt;tx_nfrags = nfrags;
 
-	rd-&gt;rd_nfrags =
-		kiblnd_dma_map_sg(hdev-&gt;ibh_ibdev,
-				  tx-&gt;tx_frags, tx-&gt;tx_nfrags, tx-&gt;tx_dmadir);
+	rd-&gt;rd_nfrags = kiblnd_dma_map_sg(hdev-&gt;ibh_ibdev, tx-&gt;tx_frags,
+					  tx-&gt;tx_nfrags, tx-&gt;tx_dmadir);
 
 	for (i = 0, nob = 0; i &lt; rd-&gt;rd_nfrags; i++) {
 		rd-&gt;rd_frags[i].rf_nob  = kiblnd_sg_dma_len(
@@ -699,12 +698,12 @@ static int
 kiblnd_setup_rd_iov(lnet_ni_t *ni, kib_tx_t *tx, kib_rdma_desc_t *rd,
 		    unsigned int niov, struct kvec *iov, int offset, int nob)
 {
-	kib_net_t	  *net = ni-&gt;ni_data;
-	struct page	*page;
+	kib_net_t *net = ni-&gt;ni_data;
+	struct page *page;
 	struct scatterlist *sg;
-	unsigned long       vaddr;
-	int		 fragnob;
-	int		 page_offset;
+	unsigned long vaddr;
+	int fragnob;
+	int page_offset;
 
 	LASSERT(nob &gt; 0);
 	LASSERT(niov &gt; 0);
@@ -752,9 +751,9 @@ static int
 kiblnd_setup_rd_kiov(lnet_ni_t *ni, kib_tx_t *tx, kib_rdma_desc_t *rd,
 		      int nkiov, lnet_kiov_t *kiov, int offset, int nob)
 {
-	kib_net_t	  *net = ni-&gt;ni_data;
+	kib_net_t *net = ni-&gt;ni_data;
 	struct scatterlist *sg;
-	int		 fragnob;
+	int fragnob;
 
 	CDEBUG(D_NET, "niov %d offset %d nob %d\n", nkiov, offset, nob);
 
@@ -793,11 +792,11 @@ kiblnd_post_tx_locked(kib_conn_t *conn, kib_tx_t *tx, int credit)
 	__releases(conn-&gt;ibc_lock)
 	__acquires(conn-&gt;ibc_lock)
 {
-	kib_msg_t	 *msg = tx-&gt;tx_msg;
-	kib_peer_t	*peer = conn-&gt;ibc_peer;
-	int		ver = conn-&gt;ibc_version;
-	int		rc;
-	int		done;
+	kib_msg_t *msg = tx-&gt;tx_msg;
+	kib_peer_t *peer = conn-&gt;ibc_peer;
+	int ver = conn-&gt;ibc_version;
+	int rc;
+	int done;
 	struct ib_send_wr *bad_wrq;
 
 	LASSERT(tx-&gt;tx_queued);
@@ -878,8 +877,7 @@ kiblnd_post_tx_locked(kib_conn_t *conn, kib_tx_t *tx, int credit)
 		/* close_conn will launch failover */
 		rc = -ENETDOWN;
 	} else {
-		rc = ib_post_send(conn-&gt;ibc_cmid-&gt;qp,
-				  tx-&gt;tx_wrq, &amp;bad_wrq);
+		rc = ib_post_send(conn-&gt;ibc_cmid-&gt;qp, tx-&gt;tx_wrq, &amp;bad_wrq);
 	}
 
 	conn-&gt;ibc_last_send = jiffies;
@@ -925,9 +923,9 @@ kiblnd_post_tx_locked(kib_conn_t *conn, kib_tx_t *tx, int credit)
 void
 kiblnd_check_sends(kib_conn_t *conn)
 {
-	int	ver = conn-&gt;ibc_version;
+	int ver = conn-&gt;ibc_version;
 	lnet_ni_t *ni = conn-&gt;ibc_peer-&gt;ibp_ni;
-	kib_tx_t  *tx;
+	kib_tx_t *tx;
 
 	/* Don't send anything until after the connection is established */
 	if (conn-&gt;ibc_state &lt; IBLND_CONN_ESTABLISHED) {
@@ -997,9 +995,9 @@ kiblnd_check_sends(kib_conn_t *conn)
 static void
 kiblnd_tx_complete(kib_tx_t *tx, int status)
 {
-	int	   failed = (status != IB_WC_SUCCESS);
-	kib_conn_t   *conn = tx-&gt;tx_conn;
-	int	   idle;
+	int failed = (status != IB_WC_SUCCESS);
+	kib_conn_t *conn = tx-&gt;tx_conn;
+	int idle;
 
 	LASSERT(tx-&gt;tx_sending &gt; 0);
 
@@ -1051,11 +1049,11 @@ kiblnd_tx_complete(kib_tx_t *tx, int status)
 void
 kiblnd_init_tx_msg(lnet_ni_t *ni, kib_tx_t *tx, int type, int body_nob)
 {
-	kib_hca_dev_t     *hdev = tx-&gt;tx_pool-&gt;tpo_hdev;
-	struct ib_sge     *sge = &amp;tx-&gt;tx_sge[tx-&gt;tx_nwrq];
+	kib_hca_dev_t *hdev = tx-&gt;tx_pool-&gt;tpo_hdev;
+	struct ib_sge *sge = &amp;tx-&gt;tx_sge[tx-&gt;tx_nwrq];
 	struct ib_send_wr *wrq = &amp;tx-&gt;tx_wrq[tx-&gt;tx_nwrq];
-	int		nob = offsetof(kib_msg_t, ibm_u) + body_nob;
-	struct ib_mr      *mr;
+	int nob = offsetof(kib_msg_t, ibm_u) + body_nob;
+	struct ib_mr *mr;
 
 	LASSERT(tx-&gt;tx_nwrq &gt;= 0);
 	LASSERT(tx-&gt;tx_nwrq &lt; IBLND_MAX_RDMA_FRAGS + 1);
@@ -1086,14 +1084,14 @@ int
 kiblnd_init_rdma(kib_conn_t *conn, kib_tx_t *tx, int type,
 		  int resid, kib_rdma_desc_t *dstrd, __u64 dstcookie)
 {
-	kib_msg_t	 *ibmsg = tx-&gt;tx_msg;
-	kib_rdma_desc_t   *srcrd = tx-&gt;tx_rd;
-	struct ib_sge     *sge = &amp;tx-&gt;tx_sge[0];
+	kib_msg_t *ibmsg = tx-&gt;tx_msg;
+	kib_rdma_desc_t *srcrd = tx-&gt;tx_rd;
+	struct ib_sge *sge = &amp;tx-&gt;tx_sge[0];
 	struct ib_send_wr *wrq = &amp;tx-&gt;tx_wrq[0];
-	int		rc  = resid;
-	int		srcidx;
-	int		dstidx;
-	int		wrknob;
+	int rc  = resid;
+	int srcidx;
+	int dstidx;
+	int wrknob;
 
 	LASSERT(!in_interrupt());
 	LASSERT(tx-&gt;tx_nwrq == 0);
@@ -1144,7 +1142,7 @@ kiblnd_init_rdma(kib_conn_t *conn, kib_tx_t *tx, int type,
 		wrq-&gt;send_flags = 0;
 
 		wrq-&gt;wr.rdma.remote_addr = kiblnd_rd_frag_addr(dstrd, dstidx);
-		wrq-&gt;wr.rdma.rkey	= kiblnd_rd_frag_key(dstrd, dstidx);
+		wrq-&gt;wr.rdma.rkey        = kiblnd_rd_frag_key(dstrd, dstidx);
 
 		srcidx = kiblnd_rd_consume_frag(srcrd, srcidx, wrknob);
 		dstidx = kiblnd_rd_consume_frag(dstrd, dstidx, wrknob);
@@ -1170,7 +1168,7 @@ kiblnd_init_rdma(kib_conn_t *conn, kib_tx_t *tx, int type,
 void
 kiblnd_queue_tx_locked(kib_tx_t *tx, kib_conn_t *conn)
 {
-	struct list_head   *q;
+	struct list_head *q;
 
 	LASSERT(tx-&gt;tx_nwrq &gt; 0);	      /* work items set up */
 	LASSERT(!tx-&gt;tx_queued);	       /* not queued for sending already */
@@ -1271,11 +1269,11 @@ static void
 kiblnd_connect_peer(kib_peer_t *peer)
 {
 	struct rdma_cm_id *cmid;
-	kib_dev_t	 *dev;
-	kib_net_t	 *net = peer-&gt;ibp_ni-&gt;ni_data;
+	kib_dev_t *dev;
+	kib_net_t *net = peer-&gt;ibp_ni-&gt;ni_data;
 	struct sockaddr_in srcaddr;
 	struct sockaddr_in dstaddr;
-	int		rc;
+	int rc;
 
 	LASSERT(net != NULL);
 	LASSERT(peer-&gt;ibp_connecting &gt; 0);
@@ -1335,12 +1333,12 @@ kiblnd_connect_peer(kib_peer_t *peer)
 void
 kiblnd_launch_tx(lnet_ni_t *ni, kib_tx_t *tx, lnet_nid_t nid)
 {
-	kib_peer_t	*peer;
-	kib_peer_t	*peer2;
-	kib_conn_t	*conn;
-	rwlock_t	*g_lock = &amp;kiblnd_data.kib_global_lock;
-	unsigned long      flags;
-	int		rc;
+	kib_peer_t *peer;
+	kib_peer_t *peer2;
+	kib_conn_t *conn;
+	rwlock_t *g_lock = &amp;kiblnd_data.kib_global_lock;
+	unsigned long flags;
+	int rc;
 
 	/* If I get here, I've committed to send, so I complete the tx with
 	 * failure on any problems */
@@ -1456,20 +1454,20 @@ kiblnd_launch_tx(lnet_ni_t *ni, kib_tx_t *tx, lnet_nid_t nid)
 int
 kiblnd_send(lnet_ni_t *ni, void *private, lnet_msg_t *lntmsg)
 {
-	lnet_hdr_t       *hdr = &amp;lntmsg-&gt;msg_hdr;
-	int	       type = lntmsg-&gt;msg_type;
+	lnet_hdr_t *hdr = &amp;lntmsg-&gt;msg_hdr;
+	int type = lntmsg-&gt;msg_type;
 	lnet_process_id_t target = lntmsg-&gt;msg_target;
-	int	       target_is_router = lntmsg-&gt;msg_target_is_router;
-	int	       routing = lntmsg-&gt;msg_routing;
-	unsigned int      payload_niov = lntmsg-&gt;msg_niov;
-	struct kvec      *payload_iov = lntmsg-&gt;msg_iov;
-	lnet_kiov_t      *payload_kiov = lntmsg-&gt;msg_kiov;
-	unsigned int      payload_offset = lntmsg-&gt;msg_offset;
-	unsigned int      payload_nob = lntmsg-&gt;msg_len;
-	kib_msg_t	*ibmsg;
-	kib_tx_t	 *tx;
-	int	       nob;
-	int	       rc;
+	int target_is_router = lntmsg-&gt;msg_target_is_router;
+	int routing = lntmsg-&gt;msg_routing;
+	unsigned int payload_niov = lntmsg-&gt;msg_niov;
+	struct kvec *payload_iov = lntmsg-&gt;msg_iov;
+	lnet_kiov_t *payload_kiov = lntmsg-&gt;msg_kiov;
+	unsigned int payload_offset = lntmsg-&gt;msg_offset;
+	unsigned int payload_nob = lntmsg-&gt;msg_len;
+	kib_msg_t *ibmsg;
+	kib_tx_t *tx;
+	int nob;
+	int rc;
 
 	/* NB 'private' is different depending on what we're sending.... */
 
@@ -1628,13 +1626,13 @@ static void
 kiblnd_reply(lnet_ni_t *ni, kib_rx_t *rx, lnet_msg_t *lntmsg)
 {
 	lnet_process_id_t target = lntmsg-&gt;msg_target;
-	unsigned int      niov = lntmsg-&gt;msg_niov;
-	struct kvec      *iov = lntmsg-&gt;msg_iov;
-	lnet_kiov_t      *kiov = lntmsg-&gt;msg_kiov;
-	unsigned int      offset = lntmsg-&gt;msg_offset;
-	unsigned int      nob = lntmsg-&gt;msg_len;
-	kib_tx_t	 *tx;
-	int	       rc;
+	unsigned int niov = lntmsg-&gt;msg_niov;
+	struct kvec *iov = lntmsg-&gt;msg_iov;
+	lnet_kiov_t *kiov = lntmsg-&gt;msg_kiov;
+	unsigned int offset = lntmsg-&gt;msg_offset;
+	unsigned int nob = lntmsg-&gt;msg_len;
+	kib_tx_t *tx;
+	int rc;
 
 	tx = kiblnd_get_idle_tx(ni, rx-&gt;rx_conn-&gt;ibc_peer-&gt;ibp_nid);
 	if (tx == NULL) {
@@ -1691,14 +1689,14 @@ kiblnd_recv(lnet_ni_t *ni, void *private, lnet_msg_t *lntmsg, int delayed,
 	     unsigned int niov, struct kvec *iov, lnet_kiov_t *kiov,
 	     unsigned int offset, unsigned int mlen, unsigned int rlen)
 {
-	kib_rx_t    *rx = private;
-	kib_msg_t   *rxmsg = rx-&gt;rx_msg;
-	kib_conn_t  *conn = rx-&gt;rx_conn;
-	kib_tx_t    *tx;
-	kib_msg_t   *txmsg;
-	int	  nob;
-	int	  post_credit = IBLND_POSTRX_PEER_CREDIT;
-	int	  rc = 0;
+	kib_rx_t *rx = private;
+	kib_msg_t *rxmsg = rx-&gt;rx_msg;
+	kib_conn_t *conn = rx-&gt;rx_conn;
+	kib_tx_t *tx;
+	kib_msg_t *txmsg;
+	int nob;
+	int post_credit = IBLND_POSTRX_PEER_CREDIT;
+	int rc = 0;
 
 	LASSERT(mlen &lt;= rlen);
 	LASSERT(!in_interrupt());
@@ -1828,8 +1826,8 @@ kiblnd_peer_alive(kib_peer_t *peer)
 static void
 kiblnd_peer_notify(kib_peer_t *peer)
 {
-	int	   error = 0;
-	unsigned long    last_alive = 0;
+	int error = 0;
+	unsigned long last_alive = 0;
 	unsigned long flags;
 
 	read_lock_irqsave(&amp;kiblnd_data.kib_global_lock, flags);
@@ -1860,9 +1858,9 @@ kiblnd_close_conn_locked(kib_conn_t *conn, int error)
 	 * connection to be finished off by the connd.  Otherwise the connd is
 	 * already dealing with it (either to set it up or tear it down).
 	 * Caller holds kib_global_lock exclusively in irq context */
-	kib_peer_t       *peer = conn-&gt;ibc_peer;
-	kib_dev_t	*dev;
-	unsigned long     flags;
+	kib_peer_t *peer = conn-&gt;ibc_peer;
+	kib_dev_t *dev;
+	unsigned long flags;
 
 	LASSERT(error != 0 || conn-&gt;ibc_state &gt;= IBLND_CONN_ESTABLISHED);
 
@@ -1934,8 +1932,8 @@ kiblnd_close_conn(kib_conn_t *conn, int error)
 static void
 kiblnd_handle_early_rxs(kib_conn_t *conn)
 {
-	unsigned long    flags;
-	kib_rx_t	*rx;
+	unsigned long flags;
+	kib_rx_t *rx;
 	kib_rx_t *tmp;
 
 	LASSERT(!in_interrupt());
@@ -1957,9 +1955,9 @@ static void
 kiblnd_abort_txs(kib_conn_t *conn, struct list_head *txs)
 {
 	LIST_HEAD(zombies);
-	struct list_head	  *tmp;
-	struct list_head	  *nxt;
-	kib_tx_t	    *tx;
+	struct list_head *tmp;
+	struct list_head *nxt;
+	kib_tx_t *tx;
 
 	spin_lock(&amp;conn-&gt;ibc_lock);
 
@@ -2018,7 +2016,7 @@ void
 kiblnd_peer_connect_failed(kib_peer_t *peer, int active, int error)
 {
 	LIST_HEAD(zombies);
-	unsigned long     flags;
+	unsigned long flags;
 
 	LASSERT(error != 0);
 	LASSERT(!in_interrupt());
@@ -2071,12 +2069,12 @@ kiblnd_peer_connect_failed(kib_peer_t *peer, int active, int error)
 void
 kiblnd_connreq_done(kib_conn_t *conn, int status)
 {
-	kib_peer_t	*peer = conn-&gt;ibc_peer;
-	kib_tx_t	  *tx;
+	kib_peer_t *peer = conn-&gt;ibc_peer;
+	kib_tx_t *tx;
 	kib_tx_t *tmp;
-	struct list_head	 txs;
-	unsigned long      flags;
-	int		active;
+	struct list_head txs;
+	unsigned long flags;
+	int active;
 
 	active = (conn-&gt;ibc_state == IBLND_CONN_ACTIVE_CONNECT);
 
@@ -2166,7 +2164,7 @@ kiblnd_connreq_done(kib_conn_t *conn, int status)
 static void
 kiblnd_reject(struct rdma_cm_id *cmid, kib_rej_t *rej)
 {
-	int	  rc;
+	int rc;
 
 	rc = rdma_reject(cmid, rej, sizeof(*rej));
 
@@ -2177,22 +2175,22 @@ kiblnd_reject(struct rdma_cm_id *cmid, kib_rej_t *rej)
 static int
 kiblnd_passive_connect(struct rdma_cm_id *cmid, void *priv, int priv_nob)
 {
-	rwlock_t		*g_lock = &amp;kiblnd_data.kib_global_lock;
-	kib_msg_t	     *reqmsg = priv;
-	kib_msg_t	     *ackmsg;
-	kib_dev_t	     *ibdev;
-	kib_peer_t	    *peer;
-	kib_peer_t	    *peer2;
-	kib_conn_t	    *conn;
-	lnet_ni_t	     *ni  = NULL;
-	kib_net_t	     *net = NULL;
-	lnet_nid_t	     nid;
+	rwlock_t *g_lock = &amp;kiblnd_data.kib_global_lock;
+	kib_msg_t *reqmsg = priv;
+	kib_msg_t *ackmsg;
+	kib_dev_t *ibdev;
+	kib_peer_t *peer;
+	kib_peer_t *peer2;
+	kib_conn_t *conn;
+	lnet_ni_t *ni  = NULL;
+	kib_net_t *net = NULL;
+	lnet_nid_t nid;
 	struct rdma_conn_param cp;
-	kib_rej_t	      rej;
-	int		    version = IBLND_MSG_VERSION;
-	unsigned long	  flags;
-	int		    rc;
-	struct sockaddr_in    *peer_addr;
+	kib_rej_t rej;
+	int version = IBLND_MSG_VERSION;
+	unsigned long flags;
+	int rc;
+	struct sockaddr_in *peer_addr;
 	LASSERT(!in_interrupt());
 
 	/* cmid inherits 'context' from the corresponding listener id */
@@ -2200,8 +2198,8 @@ kiblnd_passive_connect(struct rdma_cm_id *cmid, void *priv, int priv_nob)
 	LASSERT(ibdev != NULL);
 
 	memset(&amp;rej, 0, sizeof(rej));
-	rej.ibr_magic		= IBLND_MSG_MAGIC;
-	rej.ibr_why		  = IBLND_REJECT_FATAL;
+	rej.ibr_magic = IBLND_MSG_MAGIC;
+	rej.ibr_why = IBLND_REJECT_FATAL;
 	rej.ibr_cp.ibcp_max_msg_size = IBLND_MSG_SIZE;
 
 	peer_addr = (struct sockaddr_in *)&amp;(cmid-&gt;route.addr.dst_addr);
@@ -2243,7 +2241,7 @@ kiblnd_passive_connect(struct rdma_cm_id *cmid, void *priv, int priv_nob)
 	}
 
 	nid = reqmsg-&gt;ibm_srcnid;
-	ni  = lnet_net2ni(LNET_NIDNET(reqmsg-&gt;ibm_dstnid));
+	ni = lnet_net2ni(LNET_NIDNET(reqmsg-&gt;ibm_dstnid));
 
 	if (ni != NULL) {
 		net = (kib_net_t *)ni-&gt;ni_data;
@@ -2394,7 +2392,7 @@ kiblnd_passive_connect(struct rdma_cm_id *cmid, void *priv, int priv_nob)
 	 * CM callback doesn't destroy cmid. */
 
 	conn-&gt;ibc_incarnation      = reqmsg-&gt;ibm_srcstamp;
-	conn-&gt;ibc_credits	  = IBLND_MSG_QUEUE_SIZE(version);
+	conn-&gt;ibc_credits          = IBLND_MSG_QUEUE_SIZE(version);
 	conn-&gt;ibc_reserved_credits = IBLND_MSG_QUEUE_SIZE(version);
 	LASSERT(conn-&gt;ibc_credits + conn-&gt;ibc_reserved_credits + IBLND_OOB_MSGS(version)
 		 &lt;= IBLND_RX_MSGS(version));
@@ -2412,12 +2410,12 @@ kiblnd_passive_connect(struct rdma_cm_id *cmid, void *priv, int priv_nob)
 
 	memset(&amp;cp, 0, sizeof(cp));
 	cp.private_data	= ackmsg;
-	cp.private_data_len    = ackmsg-&gt;ibm_nob;
+	cp.private_data_len = ackmsg-&gt;ibm_nob;
 	cp.responder_resources = 0;	     /* No atomic ops or RDMA reads */
-	cp.initiator_depth     = 0;
+	cp.initiator_depth = 0;
 	cp.flow_control	= 1;
-	cp.retry_count	 = *kiblnd_tunables.kib_retry_count;
-	cp.rnr_retry_count     = *kiblnd_tunables.kib_rnr_retry_count;
+	cp.retry_count = *kiblnd_tunables.kib_retry_count;
+	cp.rnr_retry_count = *kiblnd_tunables.kib_rnr_retry_count;
 
 	CDEBUG(D_NET, "Accept %s\n", libcfs_nid2str(nid));
 
@@ -2439,7 +2437,7 @@ kiblnd_passive_connect(struct rdma_cm_id *cmid, void *priv, int priv_nob)
 	if (ni != NULL)
 		lnet_ni_decref(ni);
 
-	rej.ibr_version = version;
+	rej.ibr_version             = version;
 	rej.ibr_cp.ibcp_queue_depth = IBLND_MSG_QUEUE_SIZE(version);
 	rej.ibr_cp.ibcp_max_frags   = IBLND_RDMA_FRAGS(version);
 	kiblnd_reject(cmid, &amp;rej);
@@ -2451,10 +2449,10 @@ static void
 kiblnd_reconnect(kib_conn_t *conn, int version,
 		  __u64 incarnation, int why, kib_connparams_t *cp)
 {
-	kib_peer_t    *peer = conn-&gt;ibc_peer;
-	char	  *reason;
-	int	    retry = 0;
-	unsigned long  flags;
+	kib_peer_t *peer = conn-&gt;ibc_peer;
+	char *reason;
+	int retry = 0;
+	unsigned long flags;
 
 	LASSERT(conn-&gt;ibc_state == IBLND_CONN_ACTIVE_CONNECT);
 	LASSERT(peer-&gt;ibp_connecting &gt; 0);     /* 'conn' at least */
@@ -2513,7 +2511,7 @@ kiblnd_reconnect(kib_conn_t *conn, int version,
 static void
 kiblnd_rejected(kib_conn_t *conn, int reason, void *priv, int priv_nob)
 {
-	kib_peer_t    *peer = conn-&gt;ibc_peer;
+	kib_peer_t *peer = conn-&gt;ibc_peer;
 
 	LASSERT(!in_interrupt());
 	LASSERT(conn-&gt;ibc_state == IBLND_CONN_ACTIVE_CONNECT);
@@ -2532,10 +2530,10 @@ kiblnd_rejected(kib_conn_t *conn, int reason, void *priv, int priv_nob)
 
 	case IB_CM_REJ_CONSUMER_DEFINED:
 		if (priv_nob &gt;= offsetof(kib_rej_t, ibr_padding)) {
-			kib_rej_t	*rej	 = priv;
-			kib_connparams_t *cp	  = NULL;
-			int	       flip	= 0;
-			__u64	     incarnation = -1;
+			kib_rej_t *rej = priv;
+			kib_connparams_t *cp = NULL;
+			int flip = 0;
+			__u64 incarnation = -1;
 
 			/* NB. default incarnation is -1 because:
 			 * a) V1 will ignore dst incarnation in connreq.
@@ -2652,13 +2650,13 @@ kiblnd_rejected(kib_conn_t *conn, int reason, void *priv, int priv_nob)
 static void
 kiblnd_check_connreply(kib_conn_t *conn, void *priv, int priv_nob)
 {
-	kib_peer_t    *peer = conn-&gt;ibc_peer;
-	lnet_ni_t     *ni   = peer-&gt;ibp_ni;
-	kib_net_t     *net  = ni-&gt;ni_data;
-	kib_msg_t     *msg  = priv;
-	int	    ver  = conn-&gt;ibc_version;
-	int	    rc   = kiblnd_unpack_msg(msg, priv_nob);
-	unsigned long  flags;
+	kib_peer_t *peer = conn-&gt;ibc_peer;
+	lnet_ni_t *ni = peer-&gt;ibp_ni;
+	kib_net_t *net = ni-&gt;ni_data;
+	kib_msg_t *msg = priv;
+	int ver = conn-&gt;ibc_version;
+	int rc = kiblnd_unpack_msg(msg, priv_nob);
+	unsigned long flags;
 
 	LASSERT(net != NULL);
 
@@ -2726,8 +2724,8 @@ kiblnd_check_connreply(kib_conn_t *conn, void *priv, int priv_nob)
 		goto failed;
 	}
 
-	conn-&gt;ibc_incarnation      = msg-&gt;ibm_srcstamp;
-	conn-&gt;ibc_credits	  =
+	conn-&gt;ibc_incarnation = msg-&gt;ibm_srcstamp;
+	conn-&gt;ibc_credits =
 	conn-&gt;ibc_reserved_credits = IBLND_MSG_QUEUE_SIZE(ver);
 	LASSERT(conn-&gt;ibc_credits + conn-&gt;ibc_reserved_credits + IBLND_OOB_MSGS(ver)
 		 &lt;= IBLND_RX_MSGS(ver));
@@ -2749,20 +2747,20 @@ kiblnd_check_connreply(kib_conn_t *conn, void *priv, int priv_nob)
 static int
 kiblnd_active_connect(struct rdma_cm_id *cmid)
 {
-	kib_peer_t	      *peer = (kib_peer_t *)cmid-&gt;context;
-	kib_conn_t	      *conn;
-	kib_msg_t	       *msg;
-	struct rdma_conn_param   cp;
-	int		      version;
-	__u64		    incarnation;
-	unsigned long	    flags;
-	int		      rc;
+	kib_peer_t *peer = (kib_peer_t *)cmid-&gt;context;
+	kib_conn_t *conn;
+	kib_msg_t *msg;
+	struct rdma_conn_param cp;
+	int version;
+	__u64 incarnation;
+	unsigned long flags;
+	int rc;
 
 	read_lock_irqsave(&amp;kiblnd_data.kib_global_lock, flags);
 
 	incarnation = peer-&gt;ibp_incarnation;
-	version     = (peer-&gt;ibp_version == 0) ? IBLND_MSG_VERSION :
-						 peer-&gt;ibp_version;
+	version = (peer-&gt;ibp_version == 0) ? IBLND_MSG_VERSION :
+					     peer-&gt;ibp_version;
 
 	read_unlock_irqrestore(&amp;kiblnd_data.kib_global_lock, flags);
 
@@ -2793,8 +2791,8 @@ kiblnd_active_connect(struct rdma_cm_id *cmid)
 	cp.private_data_len    = msg-&gt;ibm_nob;
 	cp.responder_resources = 0;	     /* No atomic ops or RDMA reads */
 	cp.initiator_depth     = 0;
-	cp.flow_control	= 1;
-	cp.retry_count	 = *kiblnd_tunables.kib_retry_count;
+	cp.flow_control        = 1;
+	cp.retry_count         = *kiblnd_tunables.kib_retry_count;
 	cp.rnr_retry_count     = *kiblnd_tunables.kib_rnr_retry_count;
 
 	LASSERT(cmid-&gt;context == (void *)conn);
@@ -2814,9 +2812,9 @@ kiblnd_active_connect(struct rdma_cm_id *cmid)
 int
 kiblnd_cm_callback(struct rdma_cm_id *cmid, struct rdma_cm_event *event)
 {
-	kib_peer_t  *peer;
-	kib_conn_t  *conn;
-	int	  rc;
+	kib_peer_t *peer;
+	kib_conn_t *conn;
+	int rc;
 
 	switch (event-&gt;event) {
 	default:
@@ -2983,8 +2981,8 @@ kiblnd_cm_callback(struct rdma_cm_id *cmid, struct rdma_cm_event *event)
 static int
 kiblnd_check_txs_locked(kib_conn_t *conn, struct list_head *txs)
 {
-	kib_tx_t	  *tx;
-	struct list_head	*ttmp;
+	kib_tx_t *tx;
+	struct list_head *ttmp;
 
 	list_for_each(ttmp, txs) {
 		tx = list_entry(ttmp, kib_tx_t, tx_list);
@@ -3022,13 +3020,13 @@ kiblnd_check_conns(int idx)
 {
 	LIST_HEAD(closes);
 	LIST_HEAD(checksends);
-	struct list_head    *peers = &amp;kiblnd_data.kib_peers[idx];
-	struct list_head    *ptmp;
-	kib_peer_t    *peer;
-	kib_conn_t    *conn;
+	struct list_head *peers = &amp;kiblnd_data.kib_peers[idx];
+	struct list_head *ptmp;
+	kib_peer_t *peer;
+	kib_conn_t *conn;
 	kib_conn_t *tmp;
-	struct list_head    *ctmp;
-	unsigned long  flags;
+	struct list_head *ctmp;
+	unsigned long flags;
 
 	/* NB. We expect to have a look at all the peers and not find any
 	 * RDMAs to time out, so we just use a shared lock while we
@@ -3114,14 +3112,14 @@ kiblnd_disconnect_conn(kib_conn_t *conn)
 int
 kiblnd_connd(void *arg)
 {
-	wait_queue_t     wait;
-	unsigned long      flags;
-	kib_conn_t	*conn;
-	int		timeout;
-	int		i;
-	int		dropped_lock;
-	int		peer_index = 0;
-	unsigned long      deadline = jiffies;
+	wait_queue_t wait;
+	unsigned long flags;
+	kib_conn_t *conn;
+	int timeout;
+	int i;
+	int dropped_lock;
+	int peer_index = 0;
+	unsigned long deadline = jiffies;
 
 	cfs_block_allsigs();
 
@@ -3169,7 +3167,7 @@ kiblnd_connd(void *arg)
 		if (timeout &lt;= 0) {
 			const int n = 4;
 			const int p = 1;
-			int       chunk = kiblnd_data.kib_peer_hash_size;
+			int chunk = kiblnd_data.kib_peer_hash_size;
 
 			spin_unlock_irqrestore(&amp;kiblnd_data.kib_connd_lock, flags);
 			dropped_lock = 1;
@@ -3273,9 +3271,9 @@ kiblnd_cq_completion(struct ib_cq *cq, void *arg)
 	 * consuming my CQ I could be called after all completions have
 	 * occurred.  But in this case, ibc_nrx == 0 &amp;&amp; ibc_nsends_posted == 0
 	 * and this CQ is about to be destroyed so I NOOP. */
-	kib_conn_t		*conn = (kib_conn_t *)arg;
-	struct kib_sched_info	*sched = conn-&gt;ibc_sched;
-	unsigned long		flags;
+	kib_conn_t *conn = (kib_conn_t *)arg;
+	struct kib_sched_info *sched = conn-&gt;ibc_sched;
+	unsigned long flags;
 
 	LASSERT(cq == conn-&gt;ibc_cq);
 
@@ -3309,15 +3307,15 @@ kiblnd_cq_event(struct ib_event *event, void *arg)
 int
 kiblnd_scheduler(void *arg)
 {
-	long			id = (long)arg;
-	struct kib_sched_info	*sched;
-	kib_conn_t		*conn;
-	wait_queue_t		wait;
-	unsigned long		flags;
-	struct ib_wc		wc;
-	int			did_something;
-	int			busy_loops = 0;
-	int			rc;
+	long id = (long)arg;
+	struct kib_sched_info *sched;
+	kib_conn_t *conn;
+	wait_queue_t wait;
+	unsigned long flags;
+	struct ib_wc wc;
+	int did_something;
+	int busy_loops = 0;
+	int rc;
 
 	cfs_block_allsigs();
 
@@ -3432,11 +3430,11 @@ kiblnd_scheduler(void *arg)
 int
 kiblnd_failover_thread(void *arg)
 {
-	rwlock_t		*glock = &amp;kiblnd_data.kib_global_lock;
-	kib_dev_t	 *dev;
-	wait_queue_t     wait;
-	unsigned long      flags;
-	int		rc;
+	rwlock_t *glock = &amp;kiblnd_data.kib_global_lock;
+	kib_dev_t *dev;
+	wait_queue_t wait;
+	unsigned long flags;
+	int rc;
 
 	LASSERT(*kiblnd_tunables.kib_dev_failover != 0);
 
@@ -3446,8 +3444,8 @@ kiblnd_failover_thread(void *arg)
 	write_lock_irqsave(glock, flags);
 
 	while (!kiblnd_data.kib_shutdown) {
-		int     do_failover = 0;
-		int     long_sleep;
+		int do_failover = 0;
+		int long_sleep;
 
 		list_for_each_entry(dev, &amp;kiblnd_data.kib_failed_devs,
 				    ibd_fail_list) {
diff --git a/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_modparams.c b/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_modparams.c
index eedf01afd57f..b0e00361cfce 100644
--- a/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_modparams.c
+++ b/drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_modparams.c
@@ -150,30 +150,30 @@ module_param(use_privileged_port, int, 0644);
 MODULE_PARM_DESC(use_privileged_port, "use privileged port when initiating connection");
 
 kib_tunables_t kiblnd_tunables = {
-	.kib_dev_failover	   = &amp;dev_failover,
-	.kib_service		= &amp;service,
-	.kib_cksum		  = &amp;cksum,
-	.kib_timeout		= &amp;timeout,
-	.kib_keepalive	      = &amp;keepalive,
-	.kib_ntx		    = &amp;ntx,
-	.kib_credits		= &amp;credits,
-	.kib_peertxcredits	  = &amp;peer_credits,
-	.kib_peercredits_hiw	= &amp;peer_credits_hiw,
-	.kib_peerrtrcredits	 = &amp;peer_buffer_credits,
-	.kib_peertimeout	    = &amp;peer_timeout,
-	.kib_default_ipif	   = &amp;ipif_name,
-	.kib_retry_count	    = &amp;retry_count,
-	.kib_rnr_retry_count	= &amp;rnr_retry_count,
-	.kib_concurrent_sends       = &amp;concurrent_sends,
-	.kib_ib_mtu		 = &amp;ib_mtu,
-	.kib_map_on_demand	  = &amp;map_on_demand,
-	.kib_fmr_pool_size	  = &amp;fmr_pool_size,
-	.kib_fmr_flush_trigger      = &amp;fmr_flush_trigger,
-	.kib_fmr_cache	      = &amp;fmr_cache,
-	.kib_pmr_pool_size	  = &amp;pmr_pool_size,
-	.kib_require_priv_port      = &amp;require_privileged_port,
-	.kib_use_priv_port	    = &amp;use_privileged_port,
-	.kib_nscheds		    = &amp;nscheds
+	.kib_dev_failover      = &amp;dev_failover,
+	.kib_service           = &amp;service,
+	.kib_cksum             = &amp;cksum,
+	.kib_timeout           = &amp;timeout,
+	.kib_keepalive         = &amp;keepalive,
+	.kib_ntx               = &amp;ntx,
+	.kib_credits           = &amp;credits,
+	.kib_peertxcredits     = &amp;peer_credits,
+	.kib_peercredits_hiw   = &amp;peer_credits_hiw,
+	.kib_peerrtrcredits    = &amp;peer_buffer_credits,
+	.kib_peertimeout       = &amp;peer_timeout,
+	.kib_default_ipif      = &amp;ipif_name,
+	.kib_retry_count       = &amp;retry_count,
+	.kib_rnr_retry_count   = &amp;rnr_retry_count,
+	.kib_concurrent_sends  = &amp;concurrent_sends,
+	.kib_ib_mtu            = &amp;ib_mtu,
+	.kib_map_on_demand     = &amp;map_on_demand,
+	.kib_fmr_pool_size     = &amp;fmr_pool_size,
+	.kib_fmr_flush_trigger = &amp;fmr_flush_trigger,
+	.kib_fmr_cache         = &amp;fmr_cache,
+	.kib_pmr_pool_size     = &amp;pmr_pool_size,
+	.kib_require_priv_port = &amp;require_privileged_port,
+	.kib_use_priv_port     = &amp;use_privileged_port,
+	.kib_nscheds           = &amp;nscheds
 };
 
 int</pre><hr><pre>commit 92980ff946fc2cdf160db735751bdbf01e5be39e
Author: Mike Shuey &lt;shuey@purdue.edu&gt;
Date:   Tue May 19 10:14:35 2015 -0400

    staging: lustre: lnet: lnet: Module is LNet, not Portals
    
    Fix the module version to match upstream development.
    
    Signed-off-by: Mike Shuey &lt;shuey@purdue.edu&gt;
    Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;

diff --git a/drivers/staging/lustre/lnet/lnet/module.c b/drivers/staging/lustre/lnet/lnet/module.c
index ff3f83172b14..f73d64468396 100644
--- a/drivers/staging/lustre/lnet/lnet/module.c
+++ b/drivers/staging/lustre/lnet/lnet/module.c
@@ -147,7 +147,7 @@ fini_lnet(void)
 }
 
 MODULE_AUTHOR("Peter J. Braam &lt;braam@clusterfs.com&gt;");
-MODULE_DESCRIPTION("Portals v3.1");
+MODULE_DESCRIPTION("LNet v3.1");
 MODULE_LICENSE("GPL");
 MODULE_VERSION("1.0.0");
 </pre>
    <div class="pagination">
        <span>[1]</span><a href='40_2.html'>2</a><a href='40_2.html'>Next&gt;&gt;</a>
    <div>
</body>

<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_152.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><span>[153]</span><a href='1_154.html'>154</a><a href='1_154.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit c22e464022f935b0cbd8724b1d99d800d49518a9
Author: David Ward &lt;david.ward@ll.mit.edu&gt;
Date:   Thu Sep 13 05:22:33 2012 +0000

    net_sched: gred: eliminate redundant DP prio comparisons
    
    Each pair of DPs only needs to be compared once when searching for
    a non-unique prio value.
    
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Acked-by: Jamal Hadi Salim &lt;jhs@mojatatu.com&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/sched/sch_gred.c b/net/sched/sch_gred.c
index fca73cdf44d9..e19d4ebfea1c 100644
--- a/net/sched/sch_gred.c
+++ b/net/sched/sch_gred.c
@@ -102,9 +102,8 @@ static inline int gred_wred_mode_check(struct Qdisc *sch)
 		if (q == NULL)
 			continue;
 
-		for (n = 0; n &lt; table-&gt;DPs; n++)
-			if (table-&gt;tab[n] &amp;&amp; table-&gt;tab[n] != q &amp;&amp;
-			    table-&gt;tab[n]-&gt;prio == q-&gt;prio)
+		for (n = i + 1; n &lt; table-&gt;DPs; n++)
+			if (table-&gt;tab[n] &amp;&amp; table-&gt;tab[n]-&gt;prio == q-&gt;prio)
 				return 1;
 	}
 </pre><hr><pre>commit e29fe837bfa3ca0a9f4ef1d4a90e6e0a74b6b8a0
Author: David Ward &lt;david.ward@ll.mit.edu&gt;
Date:   Thu Sep 13 05:22:32 2012 +0000

    net_sched: gred: correct comment about qavg calculation in RIO mode
    
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Acked-by: Jamal Hadi Salim &lt;jhs@mojatatu.com&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/sched/sch_gred.c b/net/sched/sch_gred.c
index e901583e4ea5..fca73cdf44d9 100644
--- a/net/sched/sch_gred.c
+++ b/net/sched/sch_gred.c
@@ -176,7 +176,7 @@ static int gred_enqueue(struct sk_buff *skb, struct Qdisc *sch)
 		skb-&gt;tc_index = (skb-&gt;tc_index &amp; ~GRED_VQ_MASK) | dp;
 	}
 
-	/* sum up all the qaves of prios &lt;= to ours to get the new qave */
+	/* sum up all the qaves of prios &lt; ours to get the new qave */
 	if (!gred_wred_mode(t) &amp;&amp; gred_rio_mode(t)) {
 		int i;
 </pre><hr><pre>commit 4362aaf6054b9760652c7047cdf6fa852acb6cf7
Author: David Ward &lt;david.ward@ll.mit.edu&gt;
Date:   Mon Apr 16 03:17:22 2012 +0000

    net_sched: red: Make minor corrections to comments
    
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/include/net/red.h b/include/net/red.h
index 77d4c3745cb5..ef46058d35bf 100644
--- a/include/net/red.h
+++ b/include/net/red.h
@@ -245,7 +245,7 @@ static inline unsigned long red_calc_qavg_from_idle_time(const struct red_parms
 	 *
 	 * dummy packets as a burst after idle time, i.e.
 	 *
-	 * 	p-&gt;qavg *= (1-W)^m
+	 * 	v-&gt;qavg *= (1-W)^m
 	 *
 	 * This is an apparently overcomplicated solution (f.e. we have to
 	 * precompute a table to make this calculation in reasonable time)
@@ -279,7 +279,7 @@ static inline unsigned long red_calc_qavg_no_idle_time(const struct red_parms *p
 						       unsigned int backlog)
 {
 	/*
-	 * NOTE: p-&gt;qavg is fixed point number with point at Wlog.
+	 * NOTE: v-&gt;qavg is fixed point number with point at Wlog.
 	 * The formula below is equvalent to floating point
 	 * version:
 	 *
@@ -390,7 +390,7 @@ static inline void red_adaptative_algo(struct red_parms *p, struct red_vars *v)
 	if (red_is_idling(v))
 		qavg = red_calc_qavg_from_idle_time(p, v);
 
-	/* p-&gt;qavg is fixed point number with point at Wlog */
+	/* v-&gt;qavg is fixed point number with point at Wlog */
 	qavg &gt;&gt;= p-&gt;Wlog;
 
 	if (qavg &gt; p-&gt;target_max &amp;&amp; p-&gt;max_P &lt;= MAX_P_MAX)</pre><hr><pre>commit 244b65dbfede788f2fa3fe2463c44d0809e97c6b
Author: David Ward &lt;david.ward@ll.mit.edu&gt;
Date:   Sun Apr 15 12:31:45 2012 +0000

    net_sched: gred: Fix oops in gred_dump() in WRED mode
    
    A parameter set exists for WRED mode, called wred_set, to hold the same
    values for qavg and qidlestart across all VQs. The WRED mode values had
    been previously held in the VQ for the default DP. After these values
    were moved to wred_set, the VQ for the default DP was no longer created
    automatically (so that it could be omitted on purpose, to have packets
    in the default DP enqueued directly to the device without using RED).
    
    However, gred_dump() was overlooked during that change; in WRED mode it
    still reads qavg/qidlestart from the VQ for the default DP, which might
    not even exist. As a result, this command sequence will cause an oops:
    
    tc qdisc add dev $DEV handle $HANDLE parent $PARENT gred setup \
        DPs 3 default 2 grio
    tc qdisc change dev $DEV handle $HANDLE gred DP 0 prio 8 $RED_OPTIONS
    tc qdisc change dev $DEV handle $HANDLE gred DP 1 prio 8 $RED_OPTIONS
    
    This fixes gred_dump() in WRED mode to use the values held in wred_set.
    
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/sched/sch_gred.c b/net/sched/sch_gred.c
index 0b15236be7b6..8179494c269a 100644
--- a/net/sched/sch_gred.c
+++ b/net/sched/sch_gred.c
@@ -565,11 +565,8 @@ static int gred_dump(struct Qdisc *sch, struct sk_buff *skb)
 		opt.packets	= q-&gt;packetsin;
 		opt.bytesin	= q-&gt;bytesin;
 
-		if (gred_wred_mode(table)) {
-			q-&gt;vars.qidlestart =
-				table-&gt;tab[table-&gt;def]-&gt;vars.qidlestart;
-			q-&gt;vars.qavg = table-&gt;tab[table-&gt;def]-&gt;vars.qavg;
-		}
+		if (gred_wred_mode(table))
+			gred_load_wred_set(table, q);
 
 		opt.qave = red_calc_qavg(&amp;q-&gt;parms, &amp;q-&gt;vars, q-&gt;vars.qavg);
 </pre><hr><pre>commit 6f66cdc3e5d3d5ccbb7ee9265b8211cdc24aa401
Author: David Ward &lt;david.ward@ll.mit.edu&gt;
Date:   Mon Apr 9 04:13:53 2012 +0000

    net/garp: fix GID rbtree ordering
    
    The comparison operators were backwards in both garp_attr_lookup and
    garp_attr_create, so the entire GID rbtree was in reverse order.
    (There was no practical side effect to this though, except that PDUs
    were sent with attributes listed in reverse order, which is still
    valid by the protocol. This change is only for clarity.)
    
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/802/garp.c b/net/802/garp.c
index a5c224830439..8456f5d98b85 100644
--- a/net/802/garp.c
+++ b/net/802/garp.c
@@ -157,9 +157,9 @@ static struct garp_attr *garp_attr_lookup(const struct garp_applicant *app,
 	while (parent) {
 		attr = rb_entry(parent, struct garp_attr, node);
 		d = garp_attr_cmp(attr, data, len, type);
-		if (d &lt; 0)
+		if (d &gt; 0)
 			parent = parent-&gt;rb_left;
-		else if (d &gt; 0)
+		else if (d &lt; 0)
 			parent = parent-&gt;rb_right;
 		else
 			return attr;
@@ -178,9 +178,9 @@ static struct garp_attr *garp_attr_create(struct garp_applicant *app,
 		parent = *p;
 		attr = rb_entry(parent, struct garp_attr, node);
 		d = garp_attr_cmp(attr, data, len, type);
-		if (d &lt; 0)
+		if (d &gt; 0)
 			p = &amp;parent-&gt;rb_left;
-		else if (d &gt; 0)
+		else if (d &lt; 0)
 			p = &amp;parent-&gt;rb_right;
 		else {
 			/* The attribute already exists; re-use it. */</pre><hr><pre>commit 67378563df2e168d32a4054616f244a91aec462d
Author: David Ward &lt;david.ward@ll.mit.edu&gt;
Date:   Tue Mar 27 09:01:52 2012 +0000

    net/garp: avoid infinite loop if attribute already exists
    
    An infinite loop occurred if garp_attr_create was called with the values
    of an existing attribute. This might happen if a previous leave request
    for the attribute has not yet been followed by a PDU transmission (or,
    if the application previously issued a join request for the attribute
    and is now issuing another one, without having issued a leave request).
    
    If garp_attr_create finds an existing attribute having the same values,
    return the address to it. Its state will then get updated (i.e., if it
    was in a leaving state, it will move into a non-leaving state and not
    get deleted during the next PDU transmission).
    
    To accomplish this fix, collapse garp_attr_insert into garp_attr_create
    (which is its only caller).
    
    Thanks to Jorge Boncompte [DTI2] &lt;jorge@dti2.net&gt; for contributing to
    this fix.
    
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Acked-by: Jorge Boncompte [DTI2] &lt;jorge@dti2.net&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/802/garp.c b/net/802/garp.c
index 8e21b6db3981..a5c224830439 100644
--- a/net/802/garp.c
+++ b/net/802/garp.c
@@ -167,7 +167,8 @@ static struct garp_attr *garp_attr_lookup(const struct garp_applicant *app,
 	return NULL;
 }
 
-static void garp_attr_insert(struct garp_applicant *app, struct garp_attr *new)
+static struct garp_attr *garp_attr_create(struct garp_applicant *app,
+					  const void *data, u8 len, u8 type)
 {
 	struct rb_node *parent = NULL, **p = &amp;app-&gt;gid.rb_node;
 	struct garp_attr *attr;
@@ -176,21 +177,16 @@ static void garp_attr_insert(struct garp_applicant *app, struct garp_attr *new)
 	while (*p) {
 		parent = *p;
 		attr = rb_entry(parent, struct garp_attr, node);
-		d = garp_attr_cmp(attr, new-&gt;data, new-&gt;dlen, new-&gt;type);
+		d = garp_attr_cmp(attr, data, len, type);
 		if (d &lt; 0)
 			p = &amp;parent-&gt;rb_left;
 		else if (d &gt; 0)
 			p = &amp;parent-&gt;rb_right;
+		else {
+			/* The attribute already exists; re-use it. */
+			return attr;
+		}
 	}
-	rb_link_node(&amp;new-&gt;node, parent, p);
-	rb_insert_color(&amp;new-&gt;node, &amp;app-&gt;gid);
-}
-
-static struct garp_attr *garp_attr_create(struct garp_applicant *app,
-					  const void *data, u8 len, u8 type)
-{
-	struct garp_attr *attr;
-
 	attr = kmalloc(sizeof(*attr) + len, GFP_ATOMIC);
 	if (!attr)
 		return attr;
@@ -198,7 +194,9 @@ static struct garp_attr *garp_attr_create(struct garp_applicant *app,
 	attr-&gt;type  = type;
 	attr-&gt;dlen  = len;
 	memcpy(attr-&gt;data, data, len);
-	garp_attr_insert(app, attr);
+
+	rb_link_node(&amp;attr-&gt;node, parent, p);
+	rb_insert_color(&amp;attr-&gt;node, &amp;app-&gt;gid);
 	return attr;
 }
 </pre><hr><pre>commit cb2d0f3e968bff7c6d262aca3e3ab8d4184e69b2
Author: David Ward &lt;david.ward@ll.mit.edu&gt;
Date:   Sun Sep 18 12:53:20 2011 +0000

    macvlan/macvtap: Fix unicast between macvtap interfaces in bridge mode
    
    Packets should always be forwarded to the lowerdev using dev_forward_skb.
    vlan-&gt;forward is for packets being forwarded directly to another macvlan/
    macvtap device (used for multicast in bridge mode).
    
    Reported-and-tested-by: Shlomo Pongratz &lt;shlomop@mellanox.com&gt;
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/drivers/net/macvlan.c b/drivers/net/macvlan.c
index 05172c39a0ce..376e3e94bae0 100644
--- a/drivers/net/macvlan.c
+++ b/drivers/net/macvlan.c
@@ -239,7 +239,7 @@ static int macvlan_queue_xmit(struct sk_buff *skb, struct net_device *dev)
 		dest = macvlan_hash_lookup(port, eth-&gt;h_dest);
 		if (dest &amp;&amp; dest-&gt;mode == MACVLAN_MODE_BRIDGE) {
 			/* send to lowerdev first for its network taps */
-			vlan-&gt;forward(vlan-&gt;lowerdev, skb);
+			dev_forward_skb(vlan-&gt;lowerdev, skb);
 
 			return NET_XMIT_SUCCESS;
 		}</pre><hr><pre>commit aa1c366e4febc7f5c2b84958a2dd7cd70e28f9d0
Author: dpward &lt;david.ward@ll.mit.edu&gt;
Date:   Mon Sep 5 16:47:24 2011 +0000

    net: Handle different key sizes between address families in flow cache
    
    With the conversion of struct flowi to a union of AF-specific structs, some
    operations on the flow cache need to account for the exact size of the key.
    
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/include/net/flow.h b/include/net/flow.h
index 2ec377d9ab9f..a09447749e2d 100644
--- a/include/net/flow.h
+++ b/include/net/flow.h
@@ -7,6 +7,7 @@
 #ifndef _NET_FLOW_H
 #define _NET_FLOW_H
 
+#include &lt;linux/socket.h&gt;
 #include &lt;linux/in6.h&gt;
 #include &lt;linux/atomic.h&gt;
 
@@ -161,6 +162,24 @@ static inline struct flowi *flowidn_to_flowi(struct flowidn *fldn)
 	return container_of(fldn, struct flowi, u.dn);
 }
 
+typedef unsigned long flow_compare_t;
+
+static inline size_t flow_key_size(u16 family)
+{
+	switch (family) {
+	case AF_INET:
+		BUILD_BUG_ON(sizeof(struct flowi4) % sizeof(flow_compare_t));
+		return sizeof(struct flowi4) / sizeof(flow_compare_t);
+	case AF_INET6:
+		BUILD_BUG_ON(sizeof(struct flowi6) % sizeof(flow_compare_t));
+		return sizeof(struct flowi6) / sizeof(flow_compare_t);
+	case AF_DECnet:
+		BUILD_BUG_ON(sizeof(struct flowidn) % sizeof(flow_compare_t));
+		return sizeof(struct flowidn) / sizeof(flow_compare_t);
+	}
+	return 0;
+}
+
 #define FLOW_DIR_IN	0
 #define FLOW_DIR_OUT	1
 #define FLOW_DIR_FWD	2
diff --git a/net/core/flow.c b/net/core/flow.c
index 47b6d26c2afb..555a456efb07 100644
--- a/net/core/flow.c
+++ b/net/core/flow.c
@@ -173,29 +173,26 @@ static void flow_new_hash_rnd(struct flow_cache *fc,
 
 static u32 flow_hash_code(struct flow_cache *fc,
 			  struct flow_cache_percpu *fcp,
-			  const struct flowi *key)
+			  const struct flowi *key,
+			  size_t keysize)
 {
 	const u32 *k = (const u32 *) key;
+	const u32 length = keysize * sizeof(flow_compare_t) / sizeof(u32);
 
-	return jhash2(k, (sizeof(*key) / sizeof(u32)), fcp-&gt;hash_rnd)
+	return jhash2(k, length, fcp-&gt;hash_rnd)
 		&amp; (flow_cache_hash_size(fc) - 1);
 }
 
-typedef unsigned long flow_compare_t;
-
 /* I hear what you're saying, use memcmp.  But memcmp cannot make
- * important assumptions that we can here, such as alignment and
- * constant size.
+ * important assumptions that we can here, such as alignment.
  */
-static int flow_key_compare(const struct flowi *key1, const struct flowi *key2)
+static int flow_key_compare(const struct flowi *key1, const struct flowi *key2,
+			    size_t keysize)
 {
 	const flow_compare_t *k1, *k1_lim, *k2;
-	const int n_elem = sizeof(struct flowi) / sizeof(flow_compare_t);
-
-	BUILD_BUG_ON(sizeof(struct flowi) % sizeof(flow_compare_t));
 
 	k1 = (const flow_compare_t *) key1;
-	k1_lim = k1 + n_elem;
+	k1_lim = k1 + keysize;
 
 	k2 = (const flow_compare_t *) key2;
 
@@ -216,6 +213,7 @@ flow_cache_lookup(struct net *net, const struct flowi *key, u16 family, u8 dir,
 	struct flow_cache_entry *fle, *tfle;
 	struct hlist_node *entry;
 	struct flow_cache_object *flo;
+	size_t keysize;
 	unsigned int hash;
 
 	local_bh_disable();
@@ -223,6 +221,11 @@ flow_cache_lookup(struct net *net, const struct flowi *key, u16 family, u8 dir,
 
 	fle = NULL;
 	flo = NULL;
+
+	keysize = flow_key_size(family);
+	if (!keysize)
+		goto nocache;
+
 	/* Packet really early in init?  Making flow_cache_init a
 	 * pre-smp initcall would solve this.  --RR */
 	if (!fcp-&gt;hash_table)
@@ -231,12 +234,12 @@ flow_cache_lookup(struct net *net, const struct flowi *key, u16 family, u8 dir,
 	if (fcp-&gt;hash_rnd_recalc)
 		flow_new_hash_rnd(fc, fcp);
 
-	hash = flow_hash_code(fc, fcp, key);
+	hash = flow_hash_code(fc, fcp, key, keysize);
 	hlist_for_each_entry(tfle, entry, &amp;fcp-&gt;hash_table[hash], u.hlist) {
 		if (tfle-&gt;net == net &amp;&amp;
 		    tfle-&gt;family == family &amp;&amp;
 		    tfle-&gt;dir == dir &amp;&amp;
-		    flow_key_compare(key, &amp;tfle-&gt;key) == 0) {
+		    flow_key_compare(key, &amp;tfle-&gt;key, keysize) == 0) {
 			fle = tfle;
 			break;
 		}
@@ -251,7 +254,7 @@ flow_cache_lookup(struct net *net, const struct flowi *key, u16 family, u8 dir,
 			fle-&gt;net = net;
 			fle-&gt;family = family;
 			fle-&gt;dir = dir;
-			memcpy(&amp;fle-&gt;key, key, sizeof(*key));
+			memcpy(&amp;fle-&gt;key, key, keysize * sizeof(flow_compare_t));
 			fle-&gt;object = NULL;
 			hlist_add_head(&amp;fle-&gt;u.hlist, &amp;fcp-&gt;hash_table[hash]);
 			fcp-&gt;hash_count++;</pre><hr><pre>commit 728871bc05afc8ff310b17dba3e57a2472792b13
Author: David Ward &lt;david.ward@ll.mit.edu&gt;
Date:   Mon Sep 5 16:47:23 2011 +0000

    net: Align AF-specific flowi structs to long
    
    AF-specific flowi structs are now passed to flow_key_compare, which must
    also be aligned to a long.
    
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/include/net/flow.h b/include/net/flow.h
index 78113daadd63..2ec377d9ab9f 100644
--- a/include/net/flow.h
+++ b/include/net/flow.h
@@ -68,7 +68,7 @@ struct flowi4 {
 #define fl4_ipsec_spi		uli.spi
 #define fl4_mh_type		uli.mht.type
 #define fl4_gre_key		uli.gre_key
-};
+} __attribute__((__aligned__(BITS_PER_LONG/8)));
 
 static inline void flowi4_init_output(struct flowi4 *fl4, int oif,
 				      __u32 mark, __u8 tos, __u8 scope,
@@ -112,7 +112,7 @@ struct flowi6 {
 #define fl6_ipsec_spi		uli.spi
 #define fl6_mh_type		uli.mht.type
 #define fl6_gre_key		uli.gre_key
-};
+} __attribute__((__aligned__(BITS_PER_LONG/8)));
 
 struct flowidn {
 	struct flowi_common	__fl_common;
@@ -127,7 +127,7 @@ struct flowidn {
 	union flowi_uli		uli;
 #define fld_sport		uli.ports.sport
 #define fld_dport		uli.ports.dport
-};
+} __attribute__((__aligned__(BITS_PER_LONG/8)));
 
 struct flowi {
 	union {</pre><hr><pre>commit 0542b69e2c57fc9668ce6a03155bea6e1f557901
Author: dpward &lt;david.ward@ll.mit.edu&gt;
Date:   Wed Aug 31 06:05:27 2011 +0000

    net: Make flow cache namespace-aware
    
    flow_cache_lookup will return a cached object (or null pointer) that the
    resolver (i.e. xfrm_policy_lookup) previously found for another namespace
    using the same key/family/dir.  Instead, make the namespace part of what
    identifies entries in the cache.
    
    As before, flow_entry_valid will return 0 for entries where the namespace
    has been deleted, and they will be removed from the cache the next time
    flow_cache_gc_task is run.
    
    Reported-by: Andrew Dickinson &lt;whydna@whydna.net&gt;
    Signed-off-by: David Ward &lt;david.ward@ll.mit.edu&gt;
    Signed-off-by: David S. Miller &lt;davem@davemloft.net&gt;

diff --git a/net/core/flow.c b/net/core/flow.c
index bf32c33cad3b..47b6d26c2afb 100644
--- a/net/core/flow.c
+++ b/net/core/flow.c
@@ -30,6 +30,7 @@ struct flow_cache_entry {
 		struct hlist_node	hlist;
 		struct list_head	gc_list;
 	} u;
+	struct net			*net;
 	u16				family;
 	u8				dir;
 	u32				genid;
@@ -232,7 +233,8 @@ flow_cache_lookup(struct net *net, const struct flowi *key, u16 family, u8 dir,
 
 	hash = flow_hash_code(fc, fcp, key);
 	hlist_for_each_entry(tfle, entry, &amp;fcp-&gt;hash_table[hash], u.hlist) {
-		if (tfle-&gt;family == family &amp;&amp;
+		if (tfle-&gt;net == net &amp;&amp;
+		    tfle-&gt;family == family &amp;&amp;
 		    tfle-&gt;dir == dir &amp;&amp;
 		    flow_key_compare(key, &amp;tfle-&gt;key) == 0) {
 			fle = tfle;
@@ -246,6 +248,7 @@ flow_cache_lookup(struct net *net, const struct flowi *key, u16 family, u8 dir,
 
 		fle = kmem_cache_alloc(flow_cachep, GFP_ATOMIC);
 		if (fle) {
+			fle-&gt;net = net;
 			fle-&gt;family = family;
 			fle-&gt;dir = dir;
 			memcpy(&amp;fle-&gt;key, key, sizeof(*key));</pre>
    <div class="pagination">
        <a href='1_152.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><span>[153]</span><a href='1_154.html'>154</a><a href='1_154.html'>Next&gt;&gt;</a>
    <div>
</body>

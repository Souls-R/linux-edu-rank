<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Columbia University</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Columbia University</h1>
    <div class="pagination">
        <span>[1]</span><a href='23_2.html'>2</a><a href='23_3.html'>3</a><a href='23_2.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 3783e1721b650588938d28e4a084a1c9748361c8
Author: Kele Huang &lt;kele.huang@columbia.edu&gt;
Date:   Sat Dec 24 01:02:33 2022 -0500

    mm: fix comment of page table counter
    
    Commit af5b0f6a09e42 ("mm: consolidate page table accounting")
    consolidates page table accounting to a single counter in struct mm_struct
    {} as mm-&gt;pgtables_bytes.  So the meanning of this counter should be the
    size of all page tables now.
    
    Link: https://lkml.kernel.org/r/20221224060233.417827-1-kele.huang@columbia.edu
    Signed-off-by: Kele Huang &lt;kele.huang@columbia.edu&gt;
    Cc: Arnd Bergmann &lt;arnd@arndb.de&gt;
    Cc: Colin Cross &lt;ccross@google.com&gt;
    Cc: David Hildenbrand &lt;david@redhat.com&gt;
    Cc: Hugh Dickins &lt;hughd@google.com&gt;
    Cc: Liam Howlett &lt;liam.howlett@oracle.com&gt;
    Cc: Matthew Wilcox (Oracle) &lt;willy@infradead.org&gt;
    Cc: Pasha Tatashin &lt;pasha.tatashin@soleen.com&gt;
    Cc: Peter Xu &lt;peterx@redhat.com&gt;
    Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;
    Cc: Yu Zhao &lt;yuzhao@google.com&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;

diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 1118e381fcdc..10b6eb311ede 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -647,7 +647,7 @@ struct mm_struct {
 		atomic_t mm_count;
 
 #ifdef CONFIG_MMU
-		atomic_long_t pgtables_bytes;	/* PTE page table pages */
+		atomic_long_t pgtables_bytes;	/* size of all page tables */
 #endif
 		int map_count;			/* number of VMAs */
 </pre><hr><pre>commit c51acdb78f92719127995c0fe41108df0552edc3
Author: Tal Zussman &lt;tz2294@columbia.edu&gt;
Date:   Fri Dec 31 02:57:50 2021 -0500

    fs: Remove FIXME comment in generic_write_checks()
    
    This patch removes an unnecessary comment that had to do with block special
    files from `generic_write_checks()`.
    
    The comment, originally added in Linux v2.4.14.9, was to clarify that we only
    set `pos` to the file size when the file was opened with `O_APPEND` if the file
    wasn't a block special file. Prior to Linux v2.4, block special files had a
    different `write()` function which was unified into a generic `write()` function
    in Linux v2.4. This generic `write()` function called `generic_write_checks()`.
    For more details, see this earlier conversation:
    https://lore.kernel.org/linux-fsdevel/Yc4Czk5A+p5p2Y4W@mit.edu/
    
    Currently, block special devices have their own `write_iter()` function and no
    longer share the same `generic_write_checks()`, therefore rendering the comment
    irrelevant.
    
    Signed-off-by: Tal Zussman &lt;tz2294@columbia.edu&gt;
    Co-authored-by: Xijiao Li &lt;xl2950@columbia.edu&gt;
    Co-authored-by: Hans Montero &lt;hjm2133@columbia.edu&gt;
    Suggested-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Signed-off-by: Al Viro &lt;viro@zeniv.linux.org.uk&gt;

diff --git a/fs/read_write.c b/fs/read_write.c
index 0074afa7ecb3..0173dc7183c9 100644
--- a/fs/read_write.c
+++ b/fs/read_write.c
@@ -1637,7 +1637,6 @@ ssize_t generic_write_checks(struct kiocb *iocb, struct iov_iter *from)
 	if (!iov_iter_count(from))
 		return 0;
 
-	/* FIXME: this is for backwards compatibility with 2.4 */
 	if (iocb-&gt;ki_flags &amp; IOCB_APPEND)
 		iocb-&gt;ki_pos = i_size_read(inode);
 </pre><hr><pre>commit b19a888c1e9bdf12e0d8dd9aeb887ca7de91c8a5
Author: Tal Zussman &lt;tz2294@columbia.edu&gt;
Date:   Thu Nov 12 19:51:56 2020 -0500

    sched/core: Fix typos in comments
    
    Signed-off-by: Tal Zussman &lt;tz2294@columbia.edu&gt;
    Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;
    Link: https://lkml.kernel.org/r/20201113005156.GA8408@charmander

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 28d541a3c74d..a9e6d630eb83 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -97,7 +97,7 @@ int sysctl_sched_rt_runtime = 950000;
  *
  * Normal scheduling state is serialized by rq-&gt;lock. __schedule() takes the
  * local CPU's rq-&gt;lock, it optionally removes the task from the runqueue and
- * always looks at the local rq data structures to find the most elegible task
+ * always looks at the local rq data structures to find the most eligible task
  * to run next.
  *
  * Task enqueue is also under rq-&gt;lock, possibly taken from another CPU.
@@ -518,7 +518,7 @@ static bool __wake_q_add(struct wake_q_head *head, struct task_struct *task)
 
 	/*
 	 * Atomically grab the task, if -&gt;wake_q is !nil already it means
-	 * its already queued (either by us or someone else) and will get the
+	 * it's already queued (either by us or someone else) and will get the
 	 * wakeup due to that.
 	 *
 	 * In order to ensure that a pending wakeup will observe our pending
@@ -769,7 +769,7 @@ bool sched_can_stop_tick(struct rq *rq)
 		return false;
 
 	/*
-	 * If there are more than one RR tasks, we need the tick to effect the
+	 * If there are more than one RR tasks, we need the tick to affect the
 	 * actual RR behaviour.
 	 */
 	if (rq-&gt;rt.rr_nr_running) {
@@ -1187,14 +1187,14 @@ static inline void uclamp_rq_dec_id(struct rq *rq, struct task_struct *p,
 	 * accounting was performed at enqueue time and we can just return
 	 * here.
 	 *
-	 * Need to be careful of the following enqeueue/dequeue ordering
+	 * Need to be careful of the following enqueue/dequeue ordering
 	 * problem too
 	 *
 	 *	enqueue(taskA)
 	 *	// sched_uclamp_used gets enabled
 	 *	enqueue(taskB)
 	 *	dequeue(taskA)
-	 *	// Must not decrement bukcet-&gt;tasks here
+	 *	// Must not decrement bucket-&gt;tasks here
 	 *	dequeue(taskB)
 	 *
 	 * where we could end up with stale data in uc_se and
@@ -2924,7 +2924,7 @@ static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags,
 #ifdef CONFIG_SMP
 	if (p-&gt;sched_class-&gt;task_woken) {
 		/*
-		 * Our task @p is fully woken up and running; so its safe to
+		 * Our task @p is fully woken up and running; so it's safe to
 		 * drop the rq-&gt;lock, hereafter rq is only used for statistics.
 		 */
 		rq_unpin_lock(rq, rf);
@@ -3411,7 +3411,7 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 
 	/*
 	 * If the owning (remote) CPU is still in the middle of schedule() with
-	 * this task as prev, wait until its done referencing the task.
+	 * this task as prev, wait until it's done referencing the task.
 	 *
 	 * Pairs with the smp_store_release() in finish_task().
 	 *
@@ -3816,7 +3816,7 @@ void wake_up_new_task(struct task_struct *p)
 #ifdef CONFIG_SMP
 	if (p-&gt;sched_class-&gt;task_woken) {
 		/*
-		 * Nothing relies on rq-&gt;lock after this, so its fine to
+		 * Nothing relies on rq-&gt;lock after this, so it's fine to
 		 * drop it.
 		 */
 		rq_unpin_lock(rq, &amp;rf);
@@ -4343,7 +4343,7 @@ unsigned long nr_iowait_cpu(int cpu)
 }
 
 /*
- * IO-wait accounting, and how its mostly bollocks (on SMP).
+ * IO-wait accounting, and how it's mostly bollocks (on SMP).
  *
  * The idea behind IO-wait account is to account the idle time that we could
  * have spend running if it were not for IO. That is, if we were to improve the
@@ -4838,7 +4838,7 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 	/*
 	 * Optimization: we know that if all tasks are in the fair class we can
 	 * call that function directly, but only if the @prev task wasn't of a
-	 * higher scheduling class, because otherwise those loose the
+	 * higher scheduling class, because otherwise those lose the
 	 * opportunity to pull in more work from other CPUs.
 	 */
 	if (likely(prev-&gt;sched_class &lt;= &amp;fair_sched_class &amp;&amp;
@@ -5361,7 +5361,7 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 	 * right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to
 	 * ensure a task is de-boosted (pi_task is set to NULL) before the
 	 * task is allowed to run again (and can exit). This ensures the pointer
-	 * points to a blocked task -- which guaratees the task is present.
+	 * points to a blocked task -- which guarantees the task is present.
 	 */
 	p-&gt;pi_top_task = pi_task;
 
@@ -5479,7 +5479,7 @@ void set_user_nice(struct task_struct *p, long nice)
 	/*
 	 * The RT priorities are set via sched_setscheduler(), but we still
 	 * allow the 'normal' nice value to be set - but as expected
-	 * it wont have any effect on scheduling until the task is
+	 * it won't have any effect on scheduling until the task is
 	 * SCHED_DEADLINE, SCHED_FIFO or SCHED_RR:
 	 */
 	if (task_has_dl_policy(p) || task_has_rt_policy(p)) {
@@ -6668,7 +6668,7 @@ EXPORT_SYMBOL(__cond_resched_lock);
  *
  * The scheduler is at all times free to pick the calling task as the most
  * eligible task to run, if removing the yield() call from your code breaks
- * it, its already broken.
+ * it, it's already broken.
  *
  * Typical broken usage is:
  *
@@ -7042,7 +7042,7 @@ void init_idle(struct task_struct *idle, int cpu)
 
 #ifdef CONFIG_SMP
 	/*
-	 * Its possible that init_idle() gets called multiple times on a task,
+	 * It's possible that init_idle() gets called multiple times on a task,
 	 * in that case do_set_cpus_allowed() will not do the right thing.
 	 *
 	 * And since this is boot we can forgo the serialization.
@@ -8225,7 +8225,7 @@ static int cpu_cgroup_can_attach(struct cgroup_taskset *tset)
 			return -EINVAL;
 #endif
 		/*
-		 * Serialize against wake_up_new_task() such that if its
+		 * Serialize against wake_up_new_task() such that if it's
 		 * running, we're sure to observe its full state.
 		 */
 		raw_spin_lock_irq(&amp;task-&gt;pi_lock);</pre><hr><pre>commit 8d0dd23c6c78d140ed2132f523592ddb4cea839f
Author: Tal Zussman &lt;tz2294@columbia.edu&gt;
Date:   Thu Nov 12 16:56:57 2020 -0500

    syscalls: Fix file comments for syscalls implemented in kernel/sys.c
    
    The relevant syscalls were previously moved from kernel/timer.c to kernel/sys.c,
    but the comments weren't updated to reflect this change.
    
    Fixing these comments messes up the alphabetical ordering of syscalls by
    filename. This could be fixed by merging the two groups of kernel/sys.c syscalls,
    but that would require reordering the syscalls and renumbering them to maintain
    the numerical order in unistd.h.
    
    Signed-off-by: Tal Zussman &lt;tz2294@columbia.edu&gt;
    Link: https://lore.kernel.org/r/20201112215657.GA4539@charmander'
    Signed-off-by: Arnd Bergmann &lt;arnd@arndb.de&gt;

diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index 37bea07c12f2..629870fbb2c9 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -744,7 +744,7 @@ asmlinkage long sys_settimeofday(struct __kernel_old_timeval __user *tv,
 asmlinkage long sys_adjtimex(struct __kernel_timex __user *txc_p);
 asmlinkage long sys_adjtimex_time32(struct old_timex32 __user *txc_p);
 
-/* kernel/timer.c */
+/* kernel/sys.c */
 asmlinkage long sys_getpid(void);
 asmlinkage long sys_getppid(void);
 asmlinkage long sys_getuid(void);
diff --git a/include/uapi/asm-generic/unistd.h b/include/uapi/asm-generic/unistd.h
index 2056318988f7..fc48c64700eb 100644
--- a/include/uapi/asm-generic/unistd.h
+++ b/include/uapi/asm-generic/unistd.h
@@ -517,7 +517,7 @@ __SC_COMP(__NR_settimeofday, sys_settimeofday, compat_sys_settimeofday)
 __SC_3264(__NR_adjtimex, sys_adjtimex_time32, sys_adjtimex)
 #endif
 
-/* kernel/timer.c */
+/* kernel/sys.c */
 #define __NR_getpid 172
 __SYSCALL(__NR_getpid, sys_getpid)
 #define __NR_getppid 173</pre><hr><pre>commit 8d404c4c246137531f94dfee352f350d59d0e5a7
Author: Christoffer Dall &lt;cdall@cs.columbia.edu&gt;
Date:   Wed Mar 16 15:38:53 2016 +0100

    KVM: arm64: Rewrite system register accessors to read/write functions
    
    Currently we access the system registers array via the vcpu_sys_reg()
    macro.  However, we are about to change the behavior to some times
    modify the register file directly, so let's change this to two
    primitives:
    
     * Accessor macros vcpu_write_sys_reg() and vcpu_read_sys_reg()
     * Direct array access macro __vcpu_sys_reg()
    
    The accessor macros should be used in places where the code needs to
    access the currently loaded VCPU's state as observed by the guest.  For
    example, when trapping on cache related registers, a write to a system
    register should go directly to the VCPU version of the register.
    
    The direct array access macro can be used in places where the VCPU is
    known to never be running (for example userspace access) or for
    registers which are never context switched (for example all the PMU
    system registers).
    
    This rewrites all users of vcpu_sys_regs to one of the macros described
    above.
    
    No functional change.
    
    Acked-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
    Reviewed-by: Andrew Jones &lt;drjones@redhat.com&gt;
    Signed-off-by: Christoffer Dall &lt;cdall@cs.columbia.edu&gt;
    Signed-off-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;

diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
index 3cc535591bdf..d313aaae5c38 100644
--- a/arch/arm64/include/asm/kvm_emulate.h
+++ b/arch/arm64/include/asm/kvm_emulate.h
@@ -290,15 +290,18 @@ static inline int kvm_vcpu_sys_get_rt(struct kvm_vcpu *vcpu)
 
 static inline unsigned long kvm_vcpu_get_mpidr_aff(struct kvm_vcpu *vcpu)
 {
-	return vcpu_sys_reg(vcpu, MPIDR_EL1) &amp; MPIDR_HWID_BITMASK;
+	return vcpu_read_sys_reg(vcpu, MPIDR_EL1) &amp; MPIDR_HWID_BITMASK;
 }
 
 static inline void kvm_vcpu_set_be(struct kvm_vcpu *vcpu)
 {
-	if (vcpu_mode_is_32bit(vcpu))
+	if (vcpu_mode_is_32bit(vcpu)) {
 		*vcpu_cpsr(vcpu) |= COMPAT_PSR_E_BIT;
-	else
-		vcpu_sys_reg(vcpu, SCTLR_EL1) |= (1 &lt;&lt; 25);
+	} else {
+		u64 sctlr = vcpu_read_sys_reg(vcpu, SCTLR_EL1);
+		sctlr |= (1 &lt;&lt; 25);
+		vcpu_write_sys_reg(vcpu, SCTLR_EL1, sctlr);
+	}
 }
 
 static inline bool kvm_vcpu_is_be(struct kvm_vcpu *vcpu)
@@ -306,7 +309,7 @@ static inline bool kvm_vcpu_is_be(struct kvm_vcpu *vcpu)
 	if (vcpu_mode_is_32bit(vcpu))
 		return !!(*vcpu_cpsr(vcpu) &amp; COMPAT_PSR_E_BIT);
 
-	return !!(vcpu_sys_reg(vcpu, SCTLR_EL1) &amp; (1 &lt;&lt; 25));
+	return !!(vcpu_read_sys_reg(vcpu, SCTLR_EL1) &amp; (1 &lt;&lt; 25));
 }
 
 static inline unsigned long vcpu_data_guest_to_host(struct kvm_vcpu *vcpu,
diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 9001fd0890c9..179bb9d5760b 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -287,7 +287,18 @@ struct kvm_vcpu_arch {
 };
 
 #define vcpu_gp_regs(v)		(&amp;(v)-&gt;arch.ctxt.gp_regs)
-#define vcpu_sys_reg(v,r)	((v)-&gt;arch.ctxt.sys_regs[(r)])
+
+/*
+ * Only use __vcpu_sys_reg if you know you want the memory backed version of a
+ * register, and not the one most recently accessed by a running VCPU.  For
+ * example, for userspace access or for system registers that are never context
+ * switched, but only emulated.
+ */
+#define __vcpu_sys_reg(v,r)	((v)-&gt;arch.ctxt.sys_regs[(r)])
+
+#define vcpu_read_sys_reg(v,r)	__vcpu_sys_reg(v,r)
+#define vcpu_write_sys_reg(v,n,r)	do { __vcpu_sys_reg(v,r) = n; } while (0)
+
 /*
  * CP14 and CP15 live in the same array, as they are backed by the
  * same system registers.
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
index 7faed6e48b46..cffa34e23718 100644
--- a/arch/arm64/include/asm/kvm_mmu.h
+++ b/arch/arm64/include/asm/kvm_mmu.h
@@ -249,7 +249,7 @@ struct kvm;
 
 static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)
 {
-	return (vcpu_sys_reg(vcpu, SCTLR_EL1) &amp; 0b101) == 0b101;
+	return (vcpu_read_sys_reg(vcpu, SCTLR_EL1) &amp; 0b101) == 0b101;
 }
 
 static inline void __clean_dcache_guest_page(kvm_pfn_t pfn, unsigned long size)
diff --git a/arch/arm64/kvm/debug.c b/arch/arm64/kvm/debug.c
index feedb877cff8..a1f4ebdfe6d3 100644
--- a/arch/arm64/kvm/debug.c
+++ b/arch/arm64/kvm/debug.c
@@ -46,7 +46,9 @@ static DEFINE_PER_CPU(u32, mdcr_el2);
  */
 static void save_guest_debug_regs(struct kvm_vcpu *vcpu)
 {
-	vcpu-&gt;arch.guest_debug_preserved.mdscr_el1 = vcpu_sys_reg(vcpu, MDSCR_EL1);
+	u64 val = vcpu_read_sys_reg(vcpu, MDSCR_EL1);
+
+	vcpu-&gt;arch.guest_debug_preserved.mdscr_el1 = val;
 
 	trace_kvm_arm_set_dreg32("Saved MDSCR_EL1",
 				vcpu-&gt;arch.guest_debug_preserved.mdscr_el1);
@@ -54,10 +56,12 @@ static void save_guest_debug_regs(struct kvm_vcpu *vcpu)
 
 static void restore_guest_debug_regs(struct kvm_vcpu *vcpu)
 {
-	vcpu_sys_reg(vcpu, MDSCR_EL1) = vcpu-&gt;arch.guest_debug_preserved.mdscr_el1;
+	u64 val = vcpu-&gt;arch.guest_debug_preserved.mdscr_el1;
+
+	vcpu_write_sys_reg(vcpu, val, MDSCR_EL1);
 
 	trace_kvm_arm_set_dreg32("Restored MDSCR_EL1",
-				vcpu_sys_reg(vcpu, MDSCR_EL1));
+				vcpu_read_sys_reg(vcpu, MDSCR_EL1));
 }
 
 /**
@@ -108,6 +112,7 @@ void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu)
 void kvm_arm_setup_debug(struct kvm_vcpu *vcpu)
 {
 	bool trap_debug = !(vcpu-&gt;arch.debug_flags &amp; KVM_ARM64_DEBUG_DIRTY);
+	unsigned long mdscr;
 
 	trace_kvm_arm_setup_debug(vcpu, vcpu-&gt;guest_debug);
 
@@ -152,9 +157,13 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu)
 		 */
 		if (vcpu-&gt;guest_debug &amp; KVM_GUESTDBG_SINGLESTEP) {
 			*vcpu_cpsr(vcpu) |=  DBG_SPSR_SS;
-			vcpu_sys_reg(vcpu, MDSCR_EL1) |= DBG_MDSCR_SS;
+			mdscr = vcpu_read_sys_reg(vcpu, MDSCR_EL1);
+			mdscr |= DBG_MDSCR_SS;
+			vcpu_write_sys_reg(vcpu, mdscr, MDSCR_EL1);
 		} else {
-			vcpu_sys_reg(vcpu, MDSCR_EL1) &amp;= ~DBG_MDSCR_SS;
+			mdscr = vcpu_read_sys_reg(vcpu, MDSCR_EL1);
+			mdscr &amp;= ~DBG_MDSCR_SS;
+			vcpu_write_sys_reg(vcpu, mdscr, MDSCR_EL1);
 		}
 
 		trace_kvm_arm_set_dreg32("SPSR_EL2", *vcpu_cpsr(vcpu));
@@ -170,7 +179,9 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu)
 		 */
 		if (vcpu-&gt;guest_debug &amp; KVM_GUESTDBG_USE_HW) {
 			/* Enable breakpoints/watchpoints */
-			vcpu_sys_reg(vcpu, MDSCR_EL1) |= DBG_MDSCR_MDE;
+			mdscr = vcpu_read_sys_reg(vcpu, MDSCR_EL1);
+			mdscr |= DBG_MDSCR_MDE;
+			vcpu_write_sys_reg(vcpu, mdscr, MDSCR_EL1);
 
 			vcpu-&gt;arch.debug_ptr = &amp;vcpu-&gt;arch.external_debug_state;
 			vcpu-&gt;arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;
@@ -194,12 +205,11 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu)
 		vcpu-&gt;arch.mdcr_el2 |= MDCR_EL2_TDA;
 
 	/* If KDE or MDE are set, perform a full save/restore cycle. */
-	if ((vcpu_sys_reg(vcpu, MDSCR_EL1) &amp; DBG_MDSCR_KDE) ||
-	    (vcpu_sys_reg(vcpu, MDSCR_EL1) &amp; DBG_MDSCR_MDE))
+	if (vcpu_read_sys_reg(vcpu, MDSCR_EL1) &amp; (DBG_MDSCR_KDE | DBG_MDSCR_MDE))
 		vcpu-&gt;arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;
 
 	trace_kvm_arm_set_dreg32("MDCR_EL2", vcpu-&gt;arch.mdcr_el2);
-	trace_kvm_arm_set_dreg32("MDSCR_EL1", vcpu_sys_reg(vcpu, MDSCR_EL1));
+	trace_kvm_arm_set_dreg32("MDSCR_EL1", vcpu_read_sys_reg(vcpu, MDSCR_EL1));
 }
 
 void kvm_arm_clear_debug(struct kvm_vcpu *vcpu)
diff --git a/arch/arm64/kvm/inject_fault.c b/arch/arm64/kvm/inject_fault.c
index 30a3f58cdb7b..63dba401fc7d 100644
--- a/arch/arm64/kvm/inject_fault.c
+++ b/arch/arm64/kvm/inject_fault.c
@@ -58,7 +58,7 @@ static u64 get_except_vector(struct kvm_vcpu *vcpu, enum exception_type type)
 		exc_offset = LOWER_EL_AArch32_VECTOR;
 	}
 
-	return vcpu_sys_reg(vcpu, VBAR_EL1) + exc_offset + type;
+	return vcpu_read_sys_reg(vcpu, VBAR_EL1) + exc_offset + type;
 }
 
 static void inject_abt64(struct kvm_vcpu *vcpu, bool is_iabt, unsigned long addr)
@@ -73,7 +73,7 @@ static void inject_abt64(struct kvm_vcpu *vcpu, bool is_iabt, unsigned long addr
 	*vcpu_cpsr(vcpu) = PSTATE_FAULT_BITS_64;
 	*vcpu_spsr(vcpu) = cpsr;
 
-	vcpu_sys_reg(vcpu, FAR_EL1) = addr;
+	vcpu_write_sys_reg(vcpu, addr, FAR_EL1);
 
 	/*
 	 * Build an {i,d}abort, depending on the level and the
@@ -94,7 +94,7 @@ static void inject_abt64(struct kvm_vcpu *vcpu, bool is_iabt, unsigned long addr
 	if (!is_iabt)
 		esr |= ESR_ELx_EC_DABT_LOW &lt;&lt; ESR_ELx_EC_SHIFT;
 
-	vcpu_sys_reg(vcpu, ESR_EL1) = esr | ESR_ELx_FSC_EXTABT;
+	vcpu_write_sys_reg(vcpu, esr | ESR_ELx_FSC_EXTABT, ESR_EL1);
 }
 
 static void inject_undef64(struct kvm_vcpu *vcpu)
@@ -115,7 +115,7 @@ static void inject_undef64(struct kvm_vcpu *vcpu)
 	if (kvm_vcpu_trap_il_is32bit(vcpu))
 		esr |= ESR_ELx_IL;
 
-	vcpu_sys_reg(vcpu, ESR_EL1) = esr;
+	vcpu_write_sys_reg(vcpu, esr, ESR_EL1);
 }
 
 /**
diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 691f81c31018..7514db002430 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -133,14 +133,14 @@ static bool access_vm_reg(struct kvm_vcpu *vcpu,
 	if (!p-&gt;is_aarch32 || !p-&gt;is_32bit) {
 		val = p-&gt;regval;
 	} else {
-		val = vcpu_sys_reg(vcpu, reg);
+		val = vcpu_read_sys_reg(vcpu, reg);
 		if (r-&gt;reg % 2)
 			val = (p-&gt;regval &lt;&lt; 32) | (u64)lower_32_bits(val);
 		else
 			val = ((u64)upper_32_bits(val) &lt;&lt; 32) |
 				lower_32_bits(p-&gt;regval);
 	}
-	vcpu_sys_reg(vcpu, reg) = val;
+	vcpu_write_sys_reg(vcpu, val, reg);
 
 	kvm_toggle_cache(vcpu, was_enabled);
 	return true;
@@ -249,10 +249,10 @@ static bool trap_debug_regs(struct kvm_vcpu *vcpu,
 			    const struct sys_reg_desc *r)
 {
 	if (p-&gt;is_write) {
-		vcpu_sys_reg(vcpu, r-&gt;reg) = p-&gt;regval;
+		vcpu_write_sys_reg(vcpu, p-&gt;regval, r-&gt;reg);
 		vcpu-&gt;arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;
 	} else {
-		p-&gt;regval = vcpu_sys_reg(vcpu, r-&gt;reg);
+		p-&gt;regval = vcpu_read_sys_reg(vcpu, r-&gt;reg);
 	}
 
 	trace_trap_reg(__func__, r-&gt;reg, p-&gt;is_write, p-&gt;regval);
@@ -465,7 +465,8 @@ static void reset_wcr(struct kvm_vcpu *vcpu,
 
 static void reset_amair_el1(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
 {
-	vcpu_sys_reg(vcpu, AMAIR_EL1) = read_sysreg(amair_el1);
+	u64 amair = read_sysreg(amair_el1);
+	vcpu_write_sys_reg(vcpu, amair, AMAIR_EL1);
 }
 
 static void reset_mpidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
@@ -482,7 +483,7 @@ static void reset_mpidr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
 	mpidr = (vcpu-&gt;vcpu_id &amp; 0x0f) &lt;&lt; MPIDR_LEVEL_SHIFT(0);
 	mpidr |= ((vcpu-&gt;vcpu_id &gt;&gt; 4) &amp; 0xff) &lt;&lt; MPIDR_LEVEL_SHIFT(1);
 	mpidr |= ((vcpu-&gt;vcpu_id &gt;&gt; 12) &amp; 0xff) &lt;&lt; MPIDR_LEVEL_SHIFT(2);
-	vcpu_sys_reg(vcpu, MPIDR_EL1) = (1ULL &lt;&lt; 31) | mpidr;
+	vcpu_write_sys_reg(vcpu, (1ULL &lt;&lt; 31) | mpidr, MPIDR_EL1);
 }
 
 static void reset_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
@@ -496,12 +497,12 @@ static void reset_pmcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
 	 */
 	val = ((pmcr &amp; ~ARMV8_PMU_PMCR_MASK)
 	       | (ARMV8_PMU_PMCR_MASK &amp; 0xdecafbad)) &amp; (~ARMV8_PMU_PMCR_E);
-	vcpu_sys_reg(vcpu, PMCR_EL0) = val;
+	__vcpu_sys_reg(vcpu, PMCR_EL0) = val;
 }
 
 static bool check_pmu_access_disabled(struct kvm_vcpu *vcpu, u64 flags)
 {
-	u64 reg = vcpu_sys_reg(vcpu, PMUSERENR_EL0);
+	u64 reg = __vcpu_sys_reg(vcpu, PMUSERENR_EL0);
 	bool enabled = (reg &amp; flags) || vcpu_mode_priv(vcpu);
 
 	if (!enabled)
@@ -543,14 +544,14 @@ static bool access_pmcr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 
 	if (p-&gt;is_write) {
 		/* Only update writeable bits of PMCR */
-		val = vcpu_sys_reg(vcpu, PMCR_EL0);
+		val = __vcpu_sys_reg(vcpu, PMCR_EL0);
 		val &amp;= ~ARMV8_PMU_PMCR_MASK;
 		val |= p-&gt;regval &amp; ARMV8_PMU_PMCR_MASK;
-		vcpu_sys_reg(vcpu, PMCR_EL0) = val;
+		__vcpu_sys_reg(vcpu, PMCR_EL0) = val;
 		kvm_pmu_handle_pmcr(vcpu, val);
 	} else {
 		/* PMCR.P &amp; PMCR.C are RAZ */
-		val = vcpu_sys_reg(vcpu, PMCR_EL0)
+		val = __vcpu_sys_reg(vcpu, PMCR_EL0)
 		      &amp; ~(ARMV8_PMU_PMCR_P | ARMV8_PMU_PMCR_C);
 		p-&gt;regval = val;
 	}
@@ -568,10 +569,10 @@ static bool access_pmselr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 		return false;
 
 	if (p-&gt;is_write)
-		vcpu_sys_reg(vcpu, PMSELR_EL0) = p-&gt;regval;
+		__vcpu_sys_reg(vcpu, PMSELR_EL0) = p-&gt;regval;
 	else
 		/* return PMSELR.SEL field */
-		p-&gt;regval = vcpu_sys_reg(vcpu, PMSELR_EL0)
+		p-&gt;regval = __vcpu_sys_reg(vcpu, PMSELR_EL0)
 			    &amp; ARMV8_PMU_COUNTER_MASK;
 
 	return true;
@@ -604,7 +605,7 @@ static bool pmu_counter_idx_valid(struct kvm_vcpu *vcpu, u64 idx)
 {
 	u64 pmcr, val;
 
-	pmcr = vcpu_sys_reg(vcpu, PMCR_EL0);
+	pmcr = __vcpu_sys_reg(vcpu, PMCR_EL0);
 	val = (pmcr &gt;&gt; ARMV8_PMU_PMCR_N_SHIFT) &amp; ARMV8_PMU_PMCR_N_MASK;
 	if (idx &gt;= val &amp;&amp; idx != ARMV8_PMU_CYCLE_IDX) {
 		kvm_inject_undefined(vcpu);
@@ -629,7 +630,7 @@ static bool access_pmu_evcntr(struct kvm_vcpu *vcpu,
 			if (pmu_access_event_counter_el0_disabled(vcpu))
 				return false;
 
-			idx = vcpu_sys_reg(vcpu, PMSELR_EL0)
+			idx = __vcpu_sys_reg(vcpu, PMSELR_EL0)
 			      &amp; ARMV8_PMU_COUNTER_MASK;
 		} else if (r-&gt;Op2 == 0) {
 			/* PMCCNTR_EL0 */
@@ -684,7 +685,7 @@ static bool access_pmu_evtyper(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 
 	if (r-&gt;CRn == 9 &amp;&amp; r-&gt;CRm == 13 &amp;&amp; r-&gt;Op2 == 1) {
 		/* PMXEVTYPER_EL0 */
-		idx = vcpu_sys_reg(vcpu, PMSELR_EL0) &amp; ARMV8_PMU_COUNTER_MASK;
+		idx = __vcpu_sys_reg(vcpu, PMSELR_EL0) &amp; ARMV8_PMU_COUNTER_MASK;
 		reg = PMEVTYPER0_EL0 + idx;
 	} else if (r-&gt;CRn == 14 &amp;&amp; (r-&gt;CRm &amp; 12) == 12) {
 		idx = ((r-&gt;CRm &amp; 3) &lt;&lt; 3) | (r-&gt;Op2 &amp; 7);
@@ -702,9 +703,9 @@ static bool access_pmu_evtyper(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 
 	if (p-&gt;is_write) {
 		kvm_pmu_set_counter_event_type(vcpu, p-&gt;regval, idx);
-		vcpu_sys_reg(vcpu, reg) = p-&gt;regval &amp; ARMV8_PMU_EVTYPE_MASK;
+		__vcpu_sys_reg(vcpu, reg) = p-&gt;regval &amp; ARMV8_PMU_EVTYPE_MASK;
 	} else {
-		p-&gt;regval = vcpu_sys_reg(vcpu, reg) &amp; ARMV8_PMU_EVTYPE_MASK;
+		p-&gt;regval = __vcpu_sys_reg(vcpu, reg) &amp; ARMV8_PMU_EVTYPE_MASK;
 	}
 
 	return true;
@@ -726,15 +727,15 @@ static bool access_pmcnten(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 		val = p-&gt;regval &amp; mask;
 		if (r-&gt;Op2 &amp; 0x1) {
 			/* accessing PMCNTENSET_EL0 */
-			vcpu_sys_reg(vcpu, PMCNTENSET_EL0) |= val;
+			__vcpu_sys_reg(vcpu, PMCNTENSET_EL0) |= val;
 			kvm_pmu_enable_counter(vcpu, val);
 		} else {
 			/* accessing PMCNTENCLR_EL0 */
-			vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &amp;= ~val;
+			__vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &amp;= ~val;
 			kvm_pmu_disable_counter(vcpu, val);
 		}
 	} else {
-		p-&gt;regval = vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &amp; mask;
+		p-&gt;regval = __vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &amp; mask;
 	}
 
 	return true;
@@ -758,12 +759,12 @@ static bool access_pminten(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 
 		if (r-&gt;Op2 &amp; 0x1)
 			/* accessing PMINTENSET_EL1 */
-			vcpu_sys_reg(vcpu, PMINTENSET_EL1) |= val;
+			__vcpu_sys_reg(vcpu, PMINTENSET_EL1) |= val;
 		else
 			/* accessing PMINTENCLR_EL1 */
-			vcpu_sys_reg(vcpu, PMINTENSET_EL1) &amp;= ~val;
+			__vcpu_sys_reg(vcpu, PMINTENSET_EL1) &amp;= ~val;
 	} else {
-		p-&gt;regval = vcpu_sys_reg(vcpu, PMINTENSET_EL1) &amp; mask;
+		p-&gt;regval = __vcpu_sys_reg(vcpu, PMINTENSET_EL1) &amp; mask;
 	}
 
 	return true;
@@ -783,12 +784,12 @@ static bool access_pmovs(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 	if (p-&gt;is_write) {
 		if (r-&gt;CRm &amp; 0x2)
 			/* accessing PMOVSSET_EL0 */
-			vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= (p-&gt;regval &amp; mask);
+			__vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= (p-&gt;regval &amp; mask);
 		else
 			/* accessing PMOVSCLR_EL0 */
-			vcpu_sys_reg(vcpu, PMOVSSET_EL0) &amp;= ~(p-&gt;regval &amp; mask);
+			__vcpu_sys_reg(vcpu, PMOVSSET_EL0) &amp;= ~(p-&gt;regval &amp; mask);
 	} else {
-		p-&gt;regval = vcpu_sys_reg(vcpu, PMOVSSET_EL0) &amp; mask;
+		p-&gt;regval = __vcpu_sys_reg(vcpu, PMOVSSET_EL0) &amp; mask;
 	}
 
 	return true;
@@ -825,10 +826,10 @@ static bool access_pmuserenr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,
 			return false;
 		}
 
-		vcpu_sys_reg(vcpu, PMUSERENR_EL0) = p-&gt;regval
-						    &amp; ARMV8_PMU_USERENR_MASK;
+		__vcpu_sys_reg(vcpu, PMUSERENR_EL0) =
+			       p-&gt;regval &amp; ARMV8_PMU_USERENR_MASK;
 	} else {
-		p-&gt;regval = vcpu_sys_reg(vcpu, PMUSERENR_EL0)
+		p-&gt;regval = __vcpu_sys_reg(vcpu, PMUSERENR_EL0)
 			    &amp; ARMV8_PMU_USERENR_MASK;
 	}
 
@@ -2230,7 +2231,7 @@ int kvm_arm_sys_reg_get_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg
 	if (r-&gt;get_user)
 		return (r-&gt;get_user)(vcpu, r, reg, uaddr);
 
-	return reg_to_user(uaddr, &amp;vcpu_sys_reg(vcpu, r-&gt;reg), reg-&gt;id);
+	return reg_to_user(uaddr, &amp;__vcpu_sys_reg(vcpu, r-&gt;reg), reg-&gt;id);
 }
 
 int kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg)
@@ -2251,7 +2252,7 @@ int kvm_arm_sys_reg_set_reg(struct kvm_vcpu *vcpu, const struct kvm_one_reg *reg
 	if (r-&gt;set_user)
 		return (r-&gt;set_user)(vcpu, r, reg, uaddr);
 
-	return reg_from_user(&amp;vcpu_sys_reg(vcpu, r-&gt;reg), uaddr, reg-&gt;id);
+	return reg_from_user(&amp;__vcpu_sys_reg(vcpu, r-&gt;reg), uaddr, reg-&gt;id);
 }
 
 static unsigned int num_demux_regs(void)
@@ -2457,6 +2458,6 @@ void kvm_reset_sys_regs(struct kvm_vcpu *vcpu)
 	reset_sys_reg_descs(vcpu, table, num);
 
 	for (num = 1; num &lt; NR_SYS_REGS; num++)
-		if (vcpu_sys_reg(vcpu, num) == 0x4242424242424242)
-			panic("Didn't reset vcpu_sys_reg(%zi)", num);
+		if (__vcpu_sys_reg(vcpu, num) == 0x4242424242424242)
+			panic("Didn't reset __vcpu_sys_reg(%zi)", num);
 }
diff --git a/arch/arm64/kvm/sys_regs.h b/arch/arm64/kvm/sys_regs.h
index 060f5348ef25..cd710f8b63e0 100644
--- a/arch/arm64/kvm/sys_regs.h
+++ b/arch/arm64/kvm/sys_regs.h
@@ -89,14 +89,14 @@ static inline void reset_unknown(struct kvm_vcpu *vcpu,
 {
 	BUG_ON(!r-&gt;reg);
 	BUG_ON(r-&gt;reg &gt;= NR_SYS_REGS);
-	vcpu_sys_reg(vcpu, r-&gt;reg) = 0x1de7ec7edbadc0deULL;
+	__vcpu_sys_reg(vcpu, r-&gt;reg) = 0x1de7ec7edbadc0deULL;
 }
 
 static inline void reset_val(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
 {
 	BUG_ON(!r-&gt;reg);
 	BUG_ON(r-&gt;reg &gt;= NR_SYS_REGS);
-	vcpu_sys_reg(vcpu, r-&gt;reg) = r-&gt;val;
+	__vcpu_sys_reg(vcpu, r-&gt;reg) = r-&gt;val;
 }
 
 static inline int cmp_sys_reg(const struct sys_reg_desc *i1,
diff --git a/arch/arm64/kvm/sys_regs_generic_v8.c b/arch/arm64/kvm/sys_regs_generic_v8.c
index 969ade1d333d..ddb8497d18d6 100644
--- a/arch/arm64/kvm/sys_regs_generic_v8.c
+++ b/arch/arm64/kvm/sys_regs_generic_v8.c
@@ -38,13 +38,13 @@ static bool access_actlr(struct kvm_vcpu *vcpu,
 	if (p-&gt;is_write)
 		return ignore_write(vcpu, p);
 
-	p-&gt;regval = vcpu_sys_reg(vcpu, ACTLR_EL1);
+	p-&gt;regval = vcpu_read_sys_reg(vcpu, ACTLR_EL1);
 	return true;
 }
 
 static void reset_actlr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *r)
 {
-	vcpu_sys_reg(vcpu, ACTLR_EL1) = read_sysreg(actlr_el1);
+	__vcpu_sys_reg(vcpu, ACTLR_EL1) = read_sysreg(actlr_el1);
 }
 
 /*
diff --git a/virt/kvm/arm/pmu.c b/virt/kvm/arm/pmu.c
index 8a9c42366db7..1c5b76c46e26 100644
--- a/virt/kvm/arm/pmu.c
+++ b/virt/kvm/arm/pmu.c
@@ -37,7 +37,7 @@ u64 kvm_pmu_get_counter_value(struct kvm_vcpu *vcpu, u64 select_idx)
 
 	reg = (select_idx == ARMV8_PMU_CYCLE_IDX)
 	      ? PMCCNTR_EL0 : PMEVCNTR0_EL0 + select_idx;
-	counter = vcpu_sys_reg(vcpu, reg);
+	counter = __vcpu_sys_reg(vcpu, reg);
 
 	/* The real counter value is equal to the value of counter register plus
 	 * the value perf event counts.
@@ -61,7 +61,7 @@ void kvm_pmu_set_counter_value(struct kvm_vcpu *vcpu, u64 select_idx, u64 val)
 
 	reg = (select_idx == ARMV8_PMU_CYCLE_IDX)
 	      ? PMCCNTR_EL0 : PMEVCNTR0_EL0 + select_idx;
-	vcpu_sys_reg(vcpu, reg) += (s64)val - kvm_pmu_get_counter_value(vcpu, select_idx);
+	__vcpu_sys_reg(vcpu, reg) += (s64)val - kvm_pmu_get_counter_value(vcpu, select_idx);
 }
 
 /**
@@ -78,7 +78,7 @@ static void kvm_pmu_stop_counter(struct kvm_vcpu *vcpu, struct kvm_pmc *pmc)
 		counter = kvm_pmu_get_counter_value(vcpu, pmc-&gt;idx);
 		reg = (pmc-&gt;idx == ARMV8_PMU_CYCLE_IDX)
 		       ? PMCCNTR_EL0 : PMEVCNTR0_EL0 + pmc-&gt;idx;
-		vcpu_sys_reg(vcpu, reg) = counter;
+		__vcpu_sys_reg(vcpu, reg) = counter;
 		perf_event_disable(pmc-&gt;perf_event);
 		perf_event_release_kernel(pmc-&gt;perf_event);
 		pmc-&gt;perf_event = NULL;
@@ -125,7 +125,7 @@ void kvm_pmu_vcpu_destroy(struct kvm_vcpu *vcpu)
 
 u64 kvm_pmu_valid_counter_mask(struct kvm_vcpu *vcpu)
 {
-	u64 val = vcpu_sys_reg(vcpu, PMCR_EL0) &gt;&gt; ARMV8_PMU_PMCR_N_SHIFT;
+	u64 val = __vcpu_sys_reg(vcpu, PMCR_EL0) &gt;&gt; ARMV8_PMU_PMCR_N_SHIFT;
 
 	val &amp;= ARMV8_PMU_PMCR_N_MASK;
 	if (val == 0)
@@ -147,7 +147,7 @@ void kvm_pmu_enable_counter(struct kvm_vcpu *vcpu, u64 val)
 	struct kvm_pmu *pmu = &amp;vcpu-&gt;arch.pmu;
 	struct kvm_pmc *pmc;
 
-	if (!(vcpu_sys_reg(vcpu, PMCR_EL0) &amp; ARMV8_PMU_PMCR_E) || !val)
+	if (!(__vcpu_sys_reg(vcpu, PMCR_EL0) &amp; ARMV8_PMU_PMCR_E) || !val)
 		return;
 
 	for (i = 0; i &lt; ARMV8_PMU_MAX_COUNTERS; i++) {
@@ -193,10 +193,10 @@ static u64 kvm_pmu_overflow_status(struct kvm_vcpu *vcpu)
 {
 	u64 reg = 0;
 
-	if ((vcpu_sys_reg(vcpu, PMCR_EL0) &amp; ARMV8_PMU_PMCR_E)) {
-		reg = vcpu_sys_reg(vcpu, PMOVSSET_EL0);
-		reg &amp;= vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
-		reg &amp;= vcpu_sys_reg(vcpu, PMINTENSET_EL1);
+	if ((__vcpu_sys_reg(vcpu, PMCR_EL0) &amp; ARMV8_PMU_PMCR_E)) {
+		reg = __vcpu_sys_reg(vcpu, PMOVSSET_EL0);
+		reg &amp;= __vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
+		reg &amp;= __vcpu_sys_reg(vcpu, PMINTENSET_EL1);
 		reg &amp;= kvm_pmu_valid_counter_mask(vcpu);
 	}
 
@@ -295,7 +295,7 @@ static void kvm_pmu_perf_overflow(struct perf_event *perf_event,
 	struct kvm_vcpu *vcpu = kvm_pmc_to_vcpu(pmc);
 	int idx = pmc-&gt;idx;
 
-	vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(idx);
+	__vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(idx);
 
 	if (kvm_pmu_overflow_status(vcpu)) {
 		kvm_make_request(KVM_REQ_IRQ_PENDING, vcpu);
@@ -316,19 +316,19 @@ void kvm_pmu_software_increment(struct kvm_vcpu *vcpu, u64 val)
 	if (val == 0)
 		return;
 
-	enable = vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
+	enable = __vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
 	for (i = 0; i &lt; ARMV8_PMU_CYCLE_IDX; i++) {
 		if (!(val &amp; BIT(i)))
 			continue;
-		type = vcpu_sys_reg(vcpu, PMEVTYPER0_EL0 + i)
+		type = __vcpu_sys_reg(vcpu, PMEVTYPER0_EL0 + i)
 		       &amp; ARMV8_PMU_EVTYPE_EVENT;
 		if ((type == ARMV8_PMUV3_PERFCTR_SW_INCR)
 		    &amp;&amp; (enable &amp; BIT(i))) {
-			reg = vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) + 1;
+			reg = __vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) + 1;
 			reg = lower_32_bits(reg);
-			vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) = reg;
+			__vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) = reg;
 			if (!reg)
-				vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(i);
+				__vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(i);
 		}
 	}
 }
@@ -348,7 +348,7 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 	mask = kvm_pmu_valid_counter_mask(vcpu);
 	if (val &amp; ARMV8_PMU_PMCR_E) {
 		kvm_pmu_enable_counter(vcpu,
-				vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &amp; mask);
+		       __vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &amp; mask);
 	} else {
 		kvm_pmu_disable_counter(vcpu, mask);
 	}
@@ -369,8 +369,8 @@ void kvm_pmu_handle_pmcr(struct kvm_vcpu *vcpu, u64 val)
 
 static bool kvm_pmu_counter_is_enabled(struct kvm_vcpu *vcpu, u64 select_idx)
 {
-	return (vcpu_sys_reg(vcpu, PMCR_EL0) &amp; ARMV8_PMU_PMCR_E) &amp;&amp;
-	       (vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &amp; BIT(select_idx));
+	return (__vcpu_sys_reg(vcpu, PMCR_EL0) &amp; ARMV8_PMU_PMCR_E) &amp;&amp;
+	       (__vcpu_sys_reg(vcpu, PMCNTENSET_EL0) &amp; BIT(select_idx));
 }
 
 /**</pre><hr><pre>commit 35a84dec00a707aed97c1ff9ebb1cd1eb67c7052
Author: Shih-Wei Li &lt;shihwei@cs.columbia.edu&gt;
Date:   Thu Aug 3 11:45:21 2017 +0200

    KVM: arm64: Move HCR_INT_OVERRIDE to default HCR_EL2 guest flag
    
    We always set the IMO and FMO bits in the HCR_EL2 when running the
    guest, regardless if we use the vgic or not.  By moving these flags to
    HCR_GUEST_FLAGS we can avoid one of the extra save/restore operations of
    HCR_EL2 in the world switch code, and we can also soon get rid of the
    other one.
    
    This is safe, because even though the IMO and FMO bits control both
    taking the interrupts to EL2 and remapping ICC_*_EL1 to ICV_*_EL1 when
    executed at EL1, as long as we ensure that these bits are clear when
    running the EL1 host, we're OK, because we reset the HCR_EL2 to only
    have the HCR_RW bit set when returning to EL1 on non-VHE systems.
    
    Reviewed-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
    Signed-off-by: Shih-Wei Li &lt;shihwei@cs.columbia.edu&gt;
    Signed-off-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;
    Signed-off-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;

diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
index 1b438c334463..6dd285e979c9 100644
--- a/arch/arm64/include/asm/kvm_arm.h
+++ b/arch/arm64/include/asm/kvm_arm.h
@@ -83,9 +83,9 @@
  */
 #define HCR_GUEST_FLAGS (HCR_TSC | HCR_TSW | HCR_TWE | HCR_TWI | HCR_VM | \
 			 HCR_TVM | HCR_BSU_IS | HCR_FB | HCR_TAC | \
-			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW | HCR_TLOR)
+			 HCR_AMO | HCR_SWIO | HCR_TIDCP | HCR_RW | HCR_TLOR | \
+			 HCR_FMO | HCR_IMO)
 #define HCR_VIRT_EXCP_MASK (HCR_VSE | HCR_VI | HCR_VF)
-#define HCR_INT_OVERRIDE   (HCR_FMO | HCR_IMO)
 #define HCR_HOST_VHE_FLAGS (HCR_RW | HCR_TGE | HCR_E2H)
 
 /* TCR_EL2 Registers bits */
diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c
index 579d9a263853..4117717548b0 100644
--- a/arch/arm64/kvm/hyp/switch.c
+++ b/arch/arm64/kvm/hyp/switch.c
@@ -174,8 +174,6 @@ static void __hyp_text __vgic_save_state(struct kvm_vcpu *vcpu)
 		__vgic_v3_save_state(vcpu);
 	else
 		__vgic_v2_save_state(vcpu);
-
-	write_sysreg(read_sysreg(hcr_el2) &amp; ~HCR_INT_OVERRIDE, hcr_el2);
 }
 
 static void __hyp_text __vgic_restore_state(struct kvm_vcpu *vcpu)
@@ -183,7 +181,6 @@ static void __hyp_text __vgic_restore_state(struct kvm_vcpu *vcpu)
 	u64 val;
 
 	val = read_sysreg(hcr_el2);
-	val |= 	HCR_INT_OVERRIDE;
 	val |= vcpu-&gt;arch.irq_lines;
 	write_sysreg(val, hcr_el2);
 </pre><hr><pre>commit f6769581e90ba2535b3e587fe15b74f6cbc4aaab
Author: Shih-Wei Li &lt;shihwei@cs.columbia.edu&gt;
Date:   Wed Oct 19 18:12:34 2016 +0000

    KVM: arm/arm64: vgic: Avoid flushing vgic state when there's no pending IRQ
    
    We do not need to flush vgic states in each world switch unless
    there is pending IRQ queued to the vgic's ap list. We can thus reduce
    the overhead by not grabbing the spinlock and not making the extra
    function call to vgic_flush_lr_state.
    
    Note: list_empty is a single atomic read (uses READ_ONCE) and can
    therefore check if a list is empty or not without the need to take the
    spinlock protecting the list.
    
    Reviewed-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
    Signed-off-by: Shih-Wei Li &lt;shihwei@cs.columbia.edu&gt;
    Signed-off-by: Christoffer Dall &lt;cdall@linaro.org&gt;

diff --git a/virt/kvm/arm/vgic/vgic.c b/virt/kvm/arm/vgic/vgic.c
index 2ac0def57424..104329139f24 100644
--- a/virt/kvm/arm/vgic/vgic.c
+++ b/virt/kvm/arm/vgic/vgic.c
@@ -637,12 +637,17 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 /* Sync back the hardware VGIC state into our emulation after a guest's run. */
 void kvm_vgic_sync_hwstate(struct kvm_vcpu *vcpu)
 {
+	struct vgic_cpu *vgic_cpu = &amp;vcpu-&gt;arch.vgic_cpu;
+
 	if (unlikely(!vgic_initialized(vcpu-&gt;kvm)))
 		return;
 
 	vgic_process_maintenance_interrupt(vcpu);
 	vgic_fold_lr_state(vcpu);
 	vgic_prune_ap_list(vcpu);
+
+	/* Make sure we can fast-path in flush_hwstate */
+	vgic_cpu-&gt;used_lrs = 0;
 }
 
 /* Flush our emulation state into the GIC hardware before entering the guest. */
@@ -651,6 +656,18 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 	if (unlikely(!vgic_initialized(vcpu-&gt;kvm)))
 		return;
 
+	/*
+	 * If there are no virtual interrupts active or pending for this
+	 * VCPU, then there is no work to do and we can bail out without
+	 * taking any lock.  There is a potential race with someone injecting
+	 * interrupts to the VCPU, but it is a benign race as the VCPU will
+	 * either observe the new interrupt before or after doing this check,
+	 * and introducing additional synchronization mechanism doesn't change
+	 * this.
+	 */
+	if (list_empty(&amp;vcpu-&gt;arch.vgic_cpu.ap_list_head))
+		return;
+
 	spin_lock(&amp;vcpu-&gt;arch.vgic_cpu.ap_list_lock);
 	vgic_flush_lr_state(vcpu);
 	spin_unlock(&amp;vcpu-&gt;arch.vgic_cpu.ap_list_lock);</pre><hr><pre>commit 328e566479449194979d64685ae6d74c989599bb
Author: Christoffer Dall &lt;cdall@cs.columbia.edu&gt;
Date:   Thu Mar 24 11:21:04 2016 +0100

    KVM: arm/arm64: vgic: Defer touching GICH_VMCR to vcpu_load/put
    
    We don't have to save/restore the VMCR on every entry to/from the guest,
    since on GICv2 we can access the control interface from EL1 and on VHE
    systems with GICv3 we can access the control interface from KVM running
    in EL2.
    
    GICv3 systems without VHE becomes the rare case, which has to
    save/restore the register on each round trip.
    
    Note that userspace accesses may see out-of-date values if the VCPU is
    running while accessing the VGIC state via the KVM device API, but this
    is already the case and it is up to userspace to quiesce the CPUs before
    reading the CPU registers from the GIC for an up-to-date view.
    
    Reviewed-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
    Signed-off-by: Christoffer Dall &lt;cdall@cs.columbia.edu&gt;
    Signed-off-by: Christoffer Dall &lt;cdall@linaro.org&gt;

diff --git a/arch/arm/include/asm/kvm_asm.h b/arch/arm/include/asm/kvm_asm.h
index 8ef05381984b..dd16044b34b6 100644
--- a/arch/arm/include/asm/kvm_asm.h
+++ b/arch/arm/include/asm/kvm_asm.h
@@ -75,7 +75,10 @@ extern void __init_stage2_translation(void);
 extern void __kvm_hyp_reset(unsigned long);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
+extern u64 __vgic_v3_read_vmcr(void);
+extern void __vgic_v3_write_vmcr(u32 vmcr);
 extern void __vgic_v3_init_lrs(void);
+
 #endif
 
 #endif /* __ARM_KVM_ASM_H__ */
diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
index 96dba7cd8be7..46fd37578693 100644
--- a/arch/arm/kvm/arm.c
+++ b/arch/arm/kvm/arm.c
@@ -351,15 +351,14 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	vcpu-&gt;arch.host_cpu_context = this_cpu_ptr(kvm_host_cpu_state);
 
 	kvm_arm_set_running_vcpu(vcpu);
+
+	kvm_vgic_load(vcpu);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
-	/*
-	 * The arch-generic KVM code expects the cpu field of a vcpu to be -1
-	 * if the vcpu is no longer assigned to a cpu.  This is used for the
-	 * optimized make_all_cpus_request path.
-	 */
+	kvm_vgic_put(vcpu);
+
 	vcpu-&gt;cpu = -1;
 
 	kvm_arm_set_running_vcpu(NULL);
@@ -633,7 +632,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		 * non-preemptible context.
 		 */
 		preempt_disable();
+
 		kvm_pmu_flush_hwstate(vcpu);
+
 		kvm_timer_flush_hwstate(vcpu);
 		kvm_vgic_flush_hwstate(vcpu);
 
diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
index ec3553eb9349..49f99cd02613 100644
--- a/arch/arm64/include/asm/kvm_asm.h
+++ b/arch/arm64/include/asm/kvm_asm.h
@@ -59,6 +59,8 @@ extern void __kvm_tlb_flush_local_vmid(struct kvm_vcpu *vcpu);
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
 extern u64 __vgic_v3_get_ich_vtr_el2(void);
+extern u64 __vgic_v3_read_vmcr(void);
+extern void __vgic_v3_write_vmcr(u32 vmcr);
 extern void __vgic_v3_init_lrs(void);
 
 extern u32 __kvm_get_mdcr_el2(void);
diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
index b72dd2ad5f44..f7a2e31eb4c1 100644
--- a/include/kvm/arm_vgic.h
+++ b/include/kvm/arm_vgic.h
@@ -306,6 +306,9 @@ bool kvm_vgic_map_is_active(struct kvm_vcpu *vcpu, unsigned int virt_irq);
 
 int kvm_vgic_vcpu_pending_irq(struct kvm_vcpu *vcpu);
 
+void kvm_vgic_load(struct kvm_vcpu *vcpu);
+void kvm_vgic_put(struct kvm_vcpu *vcpu);
+
 #define irqchip_in_kernel(k)	(!!((k)-&gt;arch.vgic.in_kernel))
 #define vgic_initialized(k)	((k)-&gt;arch.vgic.initialized)
 #define vgic_ready(k)		((k)-&gt;arch.vgic.ready)
diff --git a/virt/kvm/arm/hyp/vgic-v2-sr.c b/virt/kvm/arm/hyp/vgic-v2-sr.c
index c8aeb7b91ec8..d3d3b9b0c2c3 100644
--- a/virt/kvm/arm/hyp/vgic-v2-sr.c
+++ b/virt/kvm/arm/hyp/vgic-v2-sr.c
@@ -114,8 +114,6 @@ void __hyp_text __vgic_v2_save_state(struct kvm_vcpu *vcpu)
 	if (!base)
 		return;
 
-	cpu_if-&gt;vgic_vmcr = readl_relaxed(base + GICH_VMCR);
-
 	if (vcpu-&gt;arch.vgic_cpu.live_lrs) {
 		cpu_if-&gt;vgic_apr = readl_relaxed(base + GICH_APR);
 
@@ -165,7 +163,6 @@ void __hyp_text __vgic_v2_restore_state(struct kvm_vcpu *vcpu)
 		}
 	}
 
-	writel_relaxed(cpu_if-&gt;vgic_vmcr, base + GICH_VMCR);
 	vcpu-&gt;arch.vgic_cpu.live_lrs = live_lrs;
 }
 
diff --git a/virt/kvm/arm/hyp/vgic-v3-sr.c b/virt/kvm/arm/hyp/vgic-v3-sr.c
index 3947095cc0a1..e51ee7edf953 100644
--- a/virt/kvm/arm/hyp/vgic-v3-sr.c
+++ b/virt/kvm/arm/hyp/vgic-v3-sr.c
@@ -159,8 +159,6 @@ void __hyp_text __vgic_v3_save_state(struct kvm_vcpu *vcpu)
 	if (!cpu_if-&gt;vgic_sre)
 		dsb(st);
 
-	cpu_if-&gt;vgic_vmcr  = read_gicreg(ICH_VMCR_EL2);
-
 	if (vcpu-&gt;arch.vgic_cpu.live_lrs) {
 		int i;
 		u32 max_lr_idx, nr_pri_bits;
@@ -261,8 +259,6 @@ void __hyp_text __vgic_v3_restore_state(struct kvm_vcpu *vcpu)
 			live_lrs |= (1 &lt;&lt; i);
 	}
 
-	write_gicreg(cpu_if-&gt;vgic_vmcr, ICH_VMCR_EL2);
-
 	if (live_lrs) {
 		write_gicreg(cpu_if-&gt;vgic_hcr, ICH_HCR_EL2);
 
@@ -326,3 +322,13 @@ u64 __hyp_text __vgic_v3_get_ich_vtr_el2(void)
 {
 	return read_gicreg(ICH_VTR_EL2);
 }
+
+u64 __hyp_text __vgic_v3_read_vmcr(void)
+{
+	return read_gicreg(ICH_VMCR_EL2);
+}
+
+void __hyp_text __vgic_v3_write_vmcr(u32 vmcr)
+{
+	write_gicreg(vmcr, ICH_VMCR_EL2);
+}
diff --git a/virt/kvm/arm/vgic/vgic-init.c b/virt/kvm/arm/vgic/vgic-init.c
index 276139a24e6f..e8e973b72ca5 100644
--- a/virt/kvm/arm/vgic/vgic-init.c
+++ b/virt/kvm/arm/vgic/vgic-init.c
@@ -262,6 +262,18 @@ int vgic_init(struct kvm *kvm)
 	vgic_debug_init(kvm);
 
 	dist-&gt;initialized = true;
+
+	/*
+	 * If we're initializing GICv2 on-demand when first running the VCPU
+	 * then we need to load the VGIC state onto the CPU.  We can detect
+	 * this easily by checking if we are in between vcpu_load and vcpu_put
+	 * when we just initialized the VGIC.
+	 */
+	preempt_disable();
+	vcpu = kvm_arm_get_running_vcpu();
+	if (vcpu)
+		kvm_vgic_load(vcpu);
+	preempt_enable();
 out:
 	return ret;
 }
diff --git a/virt/kvm/arm/vgic/vgic-v2.c b/virt/kvm/arm/vgic/vgic-v2.c
index b834ecdf3225..2f241e026c8f 100644
--- a/virt/kvm/arm/vgic/vgic-v2.c
+++ b/virt/kvm/arm/vgic/vgic-v2.c
@@ -184,6 +184,7 @@ void vgic_v2_clear_lr(struct kvm_vcpu *vcpu, int lr)
 
 void vgic_v2_set_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcrp)
 {
+	struct vgic_v2_cpu_if *cpu_if = &amp;vcpu-&gt;arch.vgic_cpu.vgic_v2;
 	u32 vmcr;
 
 	vmcr  = (vmcrp-&gt;ctlr &lt;&lt; GICH_VMCR_CTRL_SHIFT) &amp; GICH_VMCR_CTRL_MASK;
@@ -194,12 +195,15 @@ void vgic_v2_set_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcrp)
 	vmcr |= (vmcrp-&gt;pmr &lt;&lt; GICH_VMCR_PRIMASK_SHIFT) &amp;
 		GICH_VMCR_PRIMASK_MASK;
 
-	vcpu-&gt;arch.vgic_cpu.vgic_v2.vgic_vmcr = vmcr;
+	cpu_if-&gt;vgic_vmcr = vmcr;
 }
 
 void vgic_v2_get_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcrp)
 {
-	u32 vmcr = vcpu-&gt;arch.vgic_cpu.vgic_v2.vgic_vmcr;
+	struct vgic_v2_cpu_if *cpu_if = &amp;vcpu-&gt;arch.vgic_cpu.vgic_v2;
+	u32 vmcr;
+
+	vmcr = cpu_if-&gt;vgic_vmcr;
 
 	vmcrp-&gt;ctlr = (vmcr &amp; GICH_VMCR_CTRL_MASK) &gt;&gt;
 			GICH_VMCR_CTRL_SHIFT;
@@ -375,3 +379,19 @@ int vgic_v2_probe(const struct gic_kvm_info *info)
 
 	return ret;
 }
+
+void vgic_v2_load(struct kvm_vcpu *vcpu)
+{
+	struct vgic_v2_cpu_if *cpu_if = &amp;vcpu-&gt;arch.vgic_cpu.vgic_v2;
+	struct vgic_dist *vgic = &amp;vcpu-&gt;kvm-&gt;arch.vgic;
+
+	writel_relaxed(cpu_if-&gt;vgic_vmcr, vgic-&gt;vctrl_base + GICH_VMCR);
+}
+
+void vgic_v2_put(struct kvm_vcpu *vcpu)
+{
+	struct vgic_v2_cpu_if *cpu_if = &amp;vcpu-&gt;arch.vgic_cpu.vgic_v2;
+	struct vgic_dist *vgic = &amp;vcpu-&gt;kvm-&gt;arch.vgic;
+
+	cpu_if-&gt;vgic_vmcr = readl_relaxed(vgic-&gt;vctrl_base + GICH_VMCR);
+}
diff --git a/virt/kvm/arm/vgic/vgic-v3.c b/virt/kvm/arm/vgic/vgic-v3.c
index be0f4c3e0142..99213d744e4f 100644
--- a/virt/kvm/arm/vgic/vgic-v3.c
+++ b/virt/kvm/arm/vgic/vgic-v3.c
@@ -173,6 +173,7 @@ void vgic_v3_clear_lr(struct kvm_vcpu *vcpu, int lr)
 
 void vgic_v3_set_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcrp)
 {
+	struct vgic_v3_cpu_if *cpu_if = &amp;vcpu-&gt;arch.vgic_cpu.vgic_v3;
 	u32 vmcr;
 
 	/*
@@ -188,12 +189,15 @@ void vgic_v3_set_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcrp)
 	vmcr |= (vmcrp-&gt;grpen0 &lt;&lt; ICH_VMCR_ENG0_SHIFT) &amp; ICH_VMCR_ENG0_MASK;
 	vmcr |= (vmcrp-&gt;grpen1 &lt;&lt; ICH_VMCR_ENG1_SHIFT) &amp; ICH_VMCR_ENG1_MASK;
 
-	vcpu-&gt;arch.vgic_cpu.vgic_v3.vgic_vmcr = vmcr;
+	cpu_if-&gt;vgic_vmcr = vmcr;
 }
 
 void vgic_v3_get_vmcr(struct kvm_vcpu *vcpu, struct vgic_vmcr *vmcrp)
 {
-	u32 vmcr = vcpu-&gt;arch.vgic_cpu.vgic_v3.vgic_vmcr;
+	struct vgic_v3_cpu_if *cpu_if = &amp;vcpu-&gt;arch.vgic_cpu.vgic_v3;
+	u32 vmcr;
+
+	vmcr = cpu_if-&gt;vgic_vmcr;
 
 	/*
 	 * Ignore the FIQen bit, because GIC emulation always implies
@@ -386,3 +390,17 @@ int vgic_v3_probe(const struct gic_kvm_info *info)
 
 	return 0;
 }
+
+void vgic_v3_load(struct kvm_vcpu *vcpu)
+{
+	struct vgic_v3_cpu_if *cpu_if = &amp;vcpu-&gt;arch.vgic_cpu.vgic_v3;
+
+	kvm_call_hyp(__vgic_v3_write_vmcr, cpu_if-&gt;vgic_vmcr);
+}
+
+void vgic_v3_put(struct kvm_vcpu *vcpu)
+{
+	struct vgic_v3_cpu_if *cpu_if = &amp;vcpu-&gt;arch.vgic_cpu.vgic_v3;
+
+	cpu_if-&gt;vgic_vmcr = kvm_call_hyp(__vgic_v3_read_vmcr);
+}
diff --git a/virt/kvm/arm/vgic/vgic.c b/virt/kvm/arm/vgic/vgic.c
index 654dfd40e449..2ac0def57424 100644
--- a/virt/kvm/arm/vgic/vgic.c
+++ b/virt/kvm/arm/vgic/vgic.c
@@ -656,6 +656,28 @@ void kvm_vgic_flush_hwstate(struct kvm_vcpu *vcpu)
 	spin_unlock(&amp;vcpu-&gt;arch.vgic_cpu.ap_list_lock);
 }
 
+void kvm_vgic_load(struct kvm_vcpu *vcpu)
+{
+	if (unlikely(!vgic_initialized(vcpu-&gt;kvm)))
+		return;
+
+	if (kvm_vgic_global_state.type == VGIC_V2)
+		vgic_v2_load(vcpu);
+	else
+		vgic_v3_load(vcpu);
+}
+
+void kvm_vgic_put(struct kvm_vcpu *vcpu)
+{
+	if (unlikely(!vgic_initialized(vcpu-&gt;kvm)))
+		return;
+
+	if (kvm_vgic_global_state.type == VGIC_V2)
+		vgic_v2_put(vcpu);
+	else
+		vgic_v3_put(vcpu);
+}
+
 int kvm_vgic_vcpu_pending_irq(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &amp;vcpu-&gt;arch.vgic_cpu;
diff --git a/virt/kvm/arm/vgic/vgic.h b/virt/kvm/arm/vgic/vgic.h
index db28f7cadab2..9afb4557c7e8 100644
--- a/virt/kvm/arm/vgic/vgic.h
+++ b/virt/kvm/arm/vgic/vgic.h
@@ -130,6 +130,9 @@ int vgic_v2_map_resources(struct kvm *kvm);
 int vgic_register_dist_iodev(struct kvm *kvm, gpa_t dist_base_address,
 			     enum vgic_type);
 
+void vgic_v2_load(struct kvm_vcpu *vcpu);
+void vgic_v2_put(struct kvm_vcpu *vcpu);
+
 static inline void vgic_get_irq_kref(struct vgic_irq *irq)
 {
 	if (irq-&gt;intid &lt; VGIC_MIN_LPI)
@@ -150,6 +153,9 @@ int vgic_v3_probe(const struct gic_kvm_info *info);
 int vgic_v3_map_resources(struct kvm *kvm);
 int vgic_register_redist_iodevs(struct kvm *kvm, gpa_t dist_base_address);
 
+void vgic_v3_load(struct kvm_vcpu *vcpu);
+void vgic_v3_put(struct kvm_vcpu *vcpu);
+
 int vgic_register_its_iodevs(struct kvm *kvm);
 bool vgic_has_its(struct kvm *kvm);
 int kvm_vgic_register_its_device(void);</pre><hr><pre>commit 370a0ec1819990f8e2a93df7cc9c0146980ed45f
Author: Jintack Lim &lt;jintack@cs.columbia.edu&gt;
Date:   Mon Mar 6 05:42:37 2017 -0800

    KVM: arm/arm64: Let vcpu thread modify its own active state
    
    Currently, if a vcpu thread tries to change the active state of an
    interrupt which is already on the same vcpu's AP list, it will loop
    forever. Since the VGIC mmio handler is called after a vcpu has
    already synced back the LR state to the struct vgic_irq, we can just
    let it proceed safely.
    
    Cc: stable@vger.kernel.org
    Reviewed-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
    Signed-off-by: Jintack Lim &lt;jintack@cs.columbia.edu&gt;
    Signed-off-by: Christoffer Dall &lt;cdall@linaro.org&gt;
    Signed-off-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;

diff --git a/virt/kvm/arm/vgic/vgic-mmio.c b/virt/kvm/arm/vgic/vgic-mmio.c
index 3654b4c835ef..2a5db1352722 100644
--- a/virt/kvm/arm/vgic/vgic-mmio.c
+++ b/virt/kvm/arm/vgic/vgic-mmio.c
@@ -180,21 +180,37 @@ unsigned long vgic_mmio_read_active(struct kvm_vcpu *vcpu,
 static void vgic_mmio_change_active(struct kvm_vcpu *vcpu, struct vgic_irq *irq,
 				    bool new_active_state)
 {
+	struct kvm_vcpu *requester_vcpu;
 	spin_lock(&amp;irq-&gt;irq_lock);
+
+	/*
+	 * The vcpu parameter here can mean multiple things depending on how
+	 * this function is called; when handling a trap from the kernel it
+	 * depends on the GIC version, and these functions are also called as
+	 * part of save/restore from userspace.
+	 *
+	 * Therefore, we have to figure out the requester in a reliable way.
+	 *
+	 * When accessing VGIC state from user space, the requester_vcpu is
+	 * NULL, which is fine, because we guarantee that no VCPUs are running
+	 * when accessing VGIC state from user space so irq-&gt;vcpu-&gt;cpu is
+	 * always -1.
+	 */
+	requester_vcpu = kvm_arm_get_running_vcpu();
+
 	/*
 	 * If this virtual IRQ was written into a list register, we
 	 * have to make sure the CPU that runs the VCPU thread has
-	 * synced back LR state to the struct vgic_irq.  We can only
-	 * know this for sure, when either this irq is not assigned to
-	 * anyone's AP list anymore, or the VCPU thread is not
-	 * running on any CPUs.
+	 * synced back the LR state to the struct vgic_irq.
 	 *
-	 * In the opposite case, we know the VCPU thread may be on its
-	 * way back from the guest and still has to sync back this
-	 * IRQ, so we release and re-acquire the spin_lock to let the
-	 * other thread sync back the IRQ.
+	 * As long as the conditions below are true, we know the VCPU thread
+	 * may be on its way back from the guest (we kicked the VCPU thread in
+	 * vgic_change_active_prepare)  and still has to sync back this IRQ,
+	 * so we release and re-acquire the spin_lock to let the other thread
+	 * sync back the IRQ.
 	 */
 	while (irq-&gt;vcpu &amp;&amp; /* IRQ may have state in an LR somewhere */
+	       irq-&gt;vcpu != requester_vcpu &amp;&amp; /* Current thread is not the VCPU thread */
 	       irq-&gt;vcpu-&gt;cpu != -1) /* VCPU thread is running */
 		cond_resched_lock(&amp;irq-&gt;irq_lock);
 </pre><hr><pre>commit 7b6b46311a8562fb3a9e035ed6ffab6d49c28886
Author: Jintack Lim &lt;jintack@cs.columbia.edu&gt;
Date:   Fri Feb 3 10:20:08 2017 -0500

    KVM: arm/arm64: Emulate the EL1 phys timer registers
    
    Emulate read and write operations to CNTP_TVAL, CNTP_CVAL and CNTP_CTL.
    Now VMs are able to use the EL1 physical timer.
    
    Signed-off-by: Jintack Lim &lt;jintack@cs.columbia.edu&gt;
    Reviewed-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;
    Signed-off-by: Marc Zyngier &lt;marc.zyngier@arm.com&gt;

diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
index 1cd3464ff88d..0e26f8c2b56f 100644
--- a/arch/arm64/kvm/sys_regs.c
+++ b/arch/arm64/kvm/sys_regs.c
@@ -824,7 +824,14 @@ static bool access_cntp_tval(struct kvm_vcpu *vcpu,
 		struct sys_reg_params *p,
 		const struct sys_reg_desc *r)
 {
-	kvm_inject_undefined(vcpu);
+	struct arch_timer_context *ptimer = vcpu_ptimer(vcpu);
+	u64 now = kvm_phys_timer_read();
+
+	if (p-&gt;is_write)
+		ptimer-&gt;cnt_cval = p-&gt;regval + now;
+	else
+		p-&gt;regval = ptimer-&gt;cnt_cval - now;
+
 	return true;
 }
 
@@ -832,7 +839,25 @@ static bool access_cntp_ctl(struct kvm_vcpu *vcpu,
 		struct sys_reg_params *p,
 		const struct sys_reg_desc *r)
 {
-	kvm_inject_undefined(vcpu);
+	struct arch_timer_context *ptimer = vcpu_ptimer(vcpu);
+
+	if (p-&gt;is_write) {
+		/* ISTATUS bit is read-only */
+		ptimer-&gt;cnt_ctl = p-&gt;regval &amp; ~ARCH_TIMER_CTRL_IT_STAT;
+	} else {
+		u64 now = kvm_phys_timer_read();
+
+		p-&gt;regval = ptimer-&gt;cnt_ctl;
+		/*
+		 * Set ISTATUS bit if it's expired.
+		 * Note that according to ARMv8 ARM Issue A.k, ISTATUS bit is
+		 * UNKNOWN when ENABLE bit is 0, so we chose to set ISTATUS bit
+		 * regardless of ENABLE bit for our implementation convenience.
+		 */
+		if (ptimer-&gt;cnt_cval &lt;= now)
+			p-&gt;regval |= ARCH_TIMER_CTRL_IT_STAT;
+	}
+
 	return true;
 }
 
@@ -840,7 +865,13 @@ static bool access_cntp_cval(struct kvm_vcpu *vcpu,
 		struct sys_reg_params *p,
 		const struct sys_reg_desc *r)
 {
-	kvm_inject_undefined(vcpu);
+	struct arch_timer_context *ptimer = vcpu_ptimer(vcpu);
+
+	if (p-&gt;is_write)
+		ptimer-&gt;cnt_cval = p-&gt;regval;
+	else
+		p-&gt;regval = ptimer-&gt;cnt_cval;
+
 	return true;
 }
 
diff --git a/include/kvm/arm_arch_timer.h b/include/kvm/arm_arch_timer.h
index f1d2fba0b9c6..fe797d6ef89d 100644
--- a/include/kvm/arm_arch_timer.h
+++ b/include/kvm/arm_arch_timer.h
@@ -72,6 +72,8 @@ bool kvm_timer_should_fire(struct arch_timer_context *timer_ctx);
 void kvm_timer_schedule(struct kvm_vcpu *vcpu);
 void kvm_timer_unschedule(struct kvm_vcpu *vcpu);
 
+u64 kvm_phys_timer_read(void);
+
 void kvm_timer_vcpu_put(struct kvm_vcpu *vcpu);
 
 void kvm_timer_init_vhe(void);
diff --git a/virt/kvm/arm/arch_timer.c b/virt/kvm/arm/arch_timer.c
index 33257b560f74..35d7100e0815 100644
--- a/virt/kvm/arm/arch_timer.c
+++ b/virt/kvm/arm/arch_timer.c
@@ -40,7 +40,7 @@ void kvm_timer_vcpu_put(struct kvm_vcpu *vcpu)
 	vcpu_vtimer(vcpu)-&gt;active_cleared_last = false;
 }
 
-static u64 kvm_phys_timer_read(void)
+u64 kvm_phys_timer_read(void)
 {
 	return timecounter-&gt;cc-&gt;read(timecounter-&gt;cc);
 }</pre>
    <div class="pagination">
        <span>[1]</span><a href='23_2.html'>2</a><a href='23_3.html'>3</a><a href='23_2.html'>Next&gt;&gt;</a>
    <div>
</body>

<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by The Computer Engineering Research Group,University of Toronto</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by The Computer Engineering Research Group,University of Toronto</h1>
    <div class="pagination">
        <span>[1]</span>
    </div>
    <hr>
    <pre>commit e768aee89c687a50e6a2110e30c5cae1fbf0d2da
Author: Livio Soares &lt;livio@eecg.toronto.edu&gt;
Date:   Thu Jun 3 15:00:31 2010 -0400

    perf, x86: Small fix to cpuid10_edx
    
    Fixes to 'cpuid10_edx' to comply with Intel documentation.
    According to the Intel Manual, Volume 2A, Table 3-12, the cpuid for
    architecture performance monitoring returns, in EDX, two pieces of
    information:
    
      1) Number of fixed-function counters (5 bits, not 4)
      2) Width of fixed-function counters (8 bits)
    
    Signed-off-by: Livio Soares &lt;livio@eecg.toronto.edu&gt;
    Acked-by: Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;
    Cc: Arnaldo Carvalho de Melo &lt;acme@redhat.com&gt;
    Cc: Arjan van de Ven &lt;arjan@linux.intel.com&gt;
    Cc: "H. Peter Anvin" &lt;hpa@zytor.com&gt;
    LKML-Reference: &lt;new-submission&gt;
    Signed-off-by: Ingo Molnar &lt;mingo@elte.hu&gt;

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 254883d0c7e0..6ed3ae4f5482 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -68,8 +68,9 @@ union cpuid10_eax {
 
 union cpuid10_edx {
 	struct {
-		unsigned int num_counters_fixed:4;
-		unsigned int reserved:28;
+		unsigned int num_counters_fixed:5;
+		unsigned int bit_width_fixed:8;
+		unsigned int reserved:19;
 	} split;
 	unsigned int full;
 };</pre><hr><pre>commit c7af77b584b02d3e321b00203a618a9c93782121
Author: Livio Soares &lt;livio@eecg.toronto.edu&gt;
Date:   Tue Dec 18 15:21:13 2007 +0100

    sched: mark rwsem functions as __sched for wchan/profiling
    
    This following commit
    
    http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commitdiff;h=fdf8cb0909b531f9ae8f9b9d7e4eb35ba3505f07
    
    un-inlined a low-level rwsem function, but did not mark it as __sched.
    The result is that it now shows up as thread wchan (which also affects
    /proc/profile stats).  The following simple patch fixes this by properly
    marking rwsem_down_failed_common() as a __sched function.
    
    Also in this patch, which is up for discussion, marks down_read() and
    down_write() proper as __sched.  For profiling, it is pretty much
    useless to know that a semaphore is beig help - it is necessary to know
    _which_ one.  By going up another frame on the stack, the information
    becomes much more useful.
    
    In summary, the below change to lib/rwsem.c should be applied; the
    changes to kernel/rwsem.c could be applied if other kernel hackers agree
    with my proposal that down_read()/down_write() in the profile is not
    enough.
    
    [ akpm@linux-foundation.org: build fix ]
    
    Signed-off-by: Livio Soares &lt;livio@eecg.toronto.edu&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Signed-off-by: Ingo Molnar &lt;mingo@elte.hu&gt;

diff --git a/kernel/rwsem.c b/kernel/rwsem.c
index 1ec620c03064..cae050b05f5e 100644
--- a/kernel/rwsem.c
+++ b/kernel/rwsem.c
@@ -6,6 +6,7 @@
 
 #include &lt;linux/types.h&gt;
 #include &lt;linux/kernel.h&gt;
+#include &lt;linux/sched.h&gt;
 #include &lt;linux/module.h&gt;
 #include &lt;linux/rwsem.h&gt;
 
@@ -15,7 +16,7 @@
 /*
  * lock for reading
  */
-void down_read(struct rw_semaphore *sem)
+void __sched down_read(struct rw_semaphore *sem)
 {
 	might_sleep();
 	rwsem_acquire_read(&amp;sem-&gt;dep_map, 0, 0, _RET_IP_);
@@ -42,7 +43,7 @@ EXPORT_SYMBOL(down_read_trylock);
 /*
  * lock for writing
  */
-void down_write(struct rw_semaphore *sem)
+void __sched down_write(struct rw_semaphore *sem)
 {
 	might_sleep();
 	rwsem_acquire(&amp;sem-&gt;dep_map, 0, 0, _RET_IP_);
diff --git a/lib/rwsem.c b/lib/rwsem.c
index cdb4e3d05607..7d02700a4b0e 100644
--- a/lib/rwsem.c
+++ b/lib/rwsem.c
@@ -146,7 +146,7 @@ __rwsem_do_wake(struct rw_semaphore *sem, int downgrading)
 /*
  * wait for a lock to be granted
  */
-static struct rw_semaphore *
+static struct rw_semaphore __sched *
 rwsem_down_failed_common(struct rw_semaphore *sem,
 			struct rwsem_waiter *waiter, signed long adjustment)
 {</pre><hr><pre>commit 449d846dbcbf61bdf7d50a923e4791102168c292
Author: Livio Soares &lt;livio@eecg.toronto.edu&gt;
Date:   Wed Feb 7 12:51:36 2007 +1100

    [POWERPC] Fix performance monitor exception
    
    To the issue: some point during 2.6.20 development, Paul Mackerras
    introduced the "lazy IRQ  disabling" patch (very cool work,  BTW).
    In that patch, the performance monitor unit exception was marked as
    "maskable", in the sense that if interrupts were soft-disabled, that
    exception could be ignored.  This broke my PowerPC profiling code.
    The symptom that I see is that a varying number of interrupts
    (from 0 to $n$, typically closer to 0) get delivered, when, in
    reality, it should always be very close to $n$.
    
    The issue stems from the way masking is being done.   Masking in
    this fashion seems to  work well with the decrementer and external
    interrupts, because they are raised again until "really"  handled.
    For the PMU, however, this does not apply (at least on my Xserver
    machine with a 970FX processor).  If the PMU exception is not handled,
    it will _not_ be re-raised (at least on my machine).  The documentation
    states that the PMXE bit in MMCR0 is set to 0 when the PMU exception
    is raised.  However, software must re-set the bit to re-enable PMU
    exceptions.  If the exception is ignored (as currently) not only is
    that interrupt lost, but because software does not re-set PMXE, the
    PMU registers are "frozen" forever.
    
    [This patch means that performance monitor exceptions are taken and
    handled even if irqs are off, as long as some other interrupt hasn't
    come along and caused interrupts to be hard-disabled.  In this sense
    the PMU exception becomes like an NMI.  The oprofile code for most
    powerpc processors does nothing that is unsafe in an NMI context, but
    the Cell oprofile code does a spin_lock_irqsave.  However, that turns
    out to be OK because Cell doesn't actually use the performance
    monitor exception; performance monitor interrupts come in as a
    regular interrupt on Cell, so will be disabled when irqs are off.
     -- paulus.]
    
    Signed-off-by: Paul Mackerras &lt;paulus@samba.org&gt;

diff --git a/arch/powerpc/kernel/head_64.S b/arch/powerpc/kernel/head_64.S
index 71b1fe58e9e4..97cedcd6c9b4 100644
--- a/arch/powerpc/kernel/head_64.S
+++ b/arch/powerpc/kernel/head_64.S
@@ -613,7 +613,7 @@ system_call_pSeries:
 /*** pSeries interrupt support ***/
 
 	/* moved from 0xf00 */
-	MASKABLE_EXCEPTION_PSERIES(., performance_monitor)
+	STD_EXCEPTION_PSERIES(., performance_monitor)
 
 /*
  * An interrupt came in while soft-disabled; clear EE in SRR1,</pre>
    <div class="pagination">
        <span>[1]</span>
    <div>
</body>

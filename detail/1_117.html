<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_116.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><span>[117]</span><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_118.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit c8502eb2d43b6b9b1dc382299a4d37031be63876
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Fri Jul 17 15:45:26 2020 -0400

    efi/x86: Mark kernel rodata non-executable for mixed mode
    
    When remapping the kernel rodata section RO in the EFI pagetables, the
    protection flags that were used for the text section are being reused,
    but the rodata section should not be marked executable.
    
    Cc: &lt;stable@vger.kernel.org&gt;
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Link: https://lore.kernel.org/r/20200717194526.3452089-1-nivedita@alum.mit.edu
    Signed-off-by: Ard Biesheuvel &lt;ardb@kernel.org&gt;

diff --git a/arch/x86/platform/efi/efi_64.c b/arch/x86/platform/efi/efi_64.c
index 413583f904a6..6af4da1149ba 100644
--- a/arch/x86/platform/efi/efi_64.c
+++ b/arch/x86/platform/efi/efi_64.c
@@ -259,6 +259,8 @@ int __init efi_setup_page_tables(unsigned long pa_memmap, unsigned num_pages)
 	npages = (__end_rodata - __start_rodata) &gt;&gt; PAGE_SHIFT;
 	rodata = __pa(__start_rodata);
 	pfn = rodata &gt;&gt; PAGE_SHIFT;
+
+	pf = _PAGE_NX | _PAGE_ENC;
 	if (kernel_map_pages_in_pgd(pgd, pfn, rodata, npages, pf)) {
 		pr_err("Failed to map kernel rodata 1:1\n");
 		return 1;</pre><hr><pre>commit 33d0f96ffd7394ffb208bb366be312d12dfd24a4
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Wed Aug 19 10:08:16 2020 -0400

    lib/string.c: Use freestanding environment
    
    gcc can transform the loop in a naive implementation of memset/memcpy
    etc into a call to the function itself.  This optimization is enabled by
    -ftree-loop-distribute-patterns.
    
    This has been the case for a while, but gcc-10.x enables this option at
    -O2 rather than -O3 as in previous versions.
    
    Add -ffreestanding, which implicitly disables this optimization with
    gcc.  It is unclear whether clang performs such optimizations, but
    hopefully it will also not do so in a freestanding environment.
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Link: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=56888
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/lib/Makefile b/lib/Makefile
index e290fc5707ea..a4a4c6864f51 100644
--- a/lib/Makefile
+++ b/lib/Makefile
@@ -15,11 +15,16 @@ KCOV_INSTRUMENT_debugobjects.o := n
 KCOV_INSTRUMENT_dynamic_debug.o := n
 KCOV_INSTRUMENT_fault-inject.o := n
 
+# string.o implements standard library functions like memset/memcpy etc.
+# Use -ffreestanding to ensure that the compiler does not try to "optimize"
+# them into calls to themselves.
+CFLAGS_string.o := -ffreestanding
+
 # Early boot use of cmdline, don't instrument it
 ifdef CONFIG_AMD_MEM_ENCRYPT
 KASAN_SANITIZE_string.o := n
 
-CFLAGS_string.o := -fno-stack-protector
+CFLAGS_string.o += -fno-stack-protector
 endif
 
 # Used by KCSAN while enabled, avoid recursion.</pre><hr><pre>commit 394b19d6cb58ae292c0e1ad6b893fed8ece477ce
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Tue Aug 4 19:48:17 2020 -0400

    x86/boot/compressed: Use builtin mem functions for decompressor
    
    Since commits
    
      c041b5ad8640 ("x86, boot: Create a separate string.h file to provide standard string functions")
      fb4cac573ef6 ("x86, boot: Move memcmp() into string.h and string.c")
    
    the decompressor stub has been using the compiler's builtin memcpy,
    memset and memcmp functions, _except_ where it would likely have the
    largest impact, in the decompression code itself.
    
    Remove the #undef's of memcpy and memset in misc.c so that the
    decompressor code also uses the compiler builtins.
    
    The rationale given in the comment doesn't really apply: just because
    some functions use the out-of-line version is no reason to not use the
    builtin version in the rest.
    
    Replace the comment with an explanation of why memzero and memmove are
    being #define'd.
    
    Drop the suggestion to #undef in boot/string.h as well: the out-of-line
    versions are not really optimized versions, they're generic code that's
    good enough for the preboot environment. The compiler will likely
    generate better code for constant-size memcpy/memset/memcmp if it is
    allowed to.
    
    Most decompressors' performance is unchanged, with the exception of LZ4
    and 64-bit ZSTD.
    
            Before  After ARCH
    LZ4       73ms   10ms   32
    LZ4      120ms   10ms   64
    ZSTD      90ms   74ms   64
    
    Measurements on QEMU on 2.2GHz Broadwell Xeon, using defconfig kernels.
    
    Decompressor code size has small differences, with the largest being
    that 64-bit ZSTD decreases just over 2k. The largest code size increase
    was on 64-bit XZ, of about 400 bytes.
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Suggested-by: Nick Terrell &lt;nickrterrell@gmail.com&gt;
    Tested-by: Nick Terrell &lt;nickrterrell@gmail.com&gt;
    Acked-by: Kees Cook &lt;keescook@chromium.org&gt;
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/arch/x86/boot/compressed/misc.c b/arch/x86/boot/compressed/misc.c
index 39e592d0e0b4..e478e40fbe5a 100644
--- a/arch/x86/boot/compressed/misc.c
+++ b/arch/x86/boot/compressed/misc.c
@@ -30,12 +30,9 @@
 #define STATIC		static
 
 /*
- * Use normal definitions of mem*() from string.c. There are already
- * included header files which expect a definition of memset() and by
- * the time we define memset macro, it is too late.
+ * Provide definitions of memzero and memmove as some of the decompressors will
+ * try to define their own functions if these are not defined as macros.
  */
-#undef memcpy
-#undef memset
 #define memzero(s, n)	memset((s), 0, (n))
 #define memmove		memmove
 
diff --git a/arch/x86/boot/string.h b/arch/x86/boot/string.h
index 995f7b7ad512..a232da487cd2 100644
--- a/arch/x86/boot/string.h
+++ b/arch/x86/boot/string.h
@@ -11,10 +11,7 @@ void *memcpy(void *dst, const void *src, size_t len);
 void *memset(void *dst, int c, size_t len);
 int memcmp(const void *s1, const void *s2, size_t len);
 
-/*
- * Access builtin version by default. If one needs to use optimized version,
- * do "undef memcpy" in .c file and link against right string.c
- */
+/* Access builtin version by default. */
 #define memcpy(d,s,l) __builtin_memcpy(d,s,l)
 #define memset(d,c,l) __builtin_memset(d,c,l)
 #define memcmp	__builtin_memcmp</pre><hr><pre>commit 527afc212231ea9d585b7709c0ab73263ecf0c85
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Fri Jul 31 16:07:51 2020 -0700

    x86/boot: Check that there are no run-time relocations
    
    Add a linker script check that there are no run-time relocations, and
    remove the old one that tries to check via looking for specially-named
    sections in the object files.
    
    Drop the tests for -fPIE compiler option and -pie linker option, as they
    are available in all supported gcc and binutils versions (as well as
    clang and lld).
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Kees Cook &lt;keescook@chromium.org&gt;
    Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;
    Tested-by: Nick Desaulniers &lt;ndesaulniers@google.com&gt;
    Tested-by: Sedat Dilek &lt;sedat.dilek@gmail.com&gt;
    Reviewed-by: Kees Cook &lt;keescook@chromium.org&gt;
    Reviewed-by: Ard Biesheuvel &lt;ardb@kernel.org&gt;
    Reviewed-by: Fangrui Song &lt;maskray@google.com&gt;
    Reviewed-by: Sedat Dilek &lt;sedat.dilek@gmail.com&gt;
    Link: https://lore.kernel.org/r/20200731230820.1742553-8-keescook@chromium.org

diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 7d25089c5a7b..753d57266757 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -29,7 +29,7 @@ targets := vmlinux vmlinux.bin vmlinux.bin.gz vmlinux.bin.bz2 vmlinux.bin.lzma \
 	vmlinux.bin.xz vmlinux.bin.lzo vmlinux.bin.lz4 vmlinux.bin.zst
 
 KBUILD_CFLAGS := -m$(BITS) -O2
-KBUILD_CFLAGS += -fno-strict-aliasing $(call cc-option, -fPIE, -fPIC)
+KBUILD_CFLAGS += -fno-strict-aliasing -fPIE
 KBUILD_CFLAGS += -DDISABLE_BRANCH_PROFILING
 cflags-$(CONFIG_X86_32) := -march=i386
 cflags-$(CONFIG_X86_64) := -mcmodel=small
@@ -52,7 +52,7 @@ UBSAN_SANITIZE :=n
 KBUILD_LDFLAGS := -m elf_$(UTS_MACHINE)
 # Compressed kernel should be built as PIE since it may be loaded at any
 # address by the bootloader.
-LDFLAGS_vmlinux := $(call ld-option, -pie) $(call ld-option, --no-dynamic-linker)
+LDFLAGS_vmlinux := -pie $(call ld-option, --no-dynamic-linker)
 LDFLAGS_vmlinux += -T
 
 hostprogs	:= mkpiggy
@@ -87,30 +87,8 @@ vmlinux-objs-$(CONFIG_ACPI) += $(obj)/acpi.o
 vmlinux-objs-$(CONFIG_EFI_MIXED) += $(obj)/efi_thunk_$(BITS).o
 efi-obj-$(CONFIG_EFI_STUB) = $(objtree)/drivers/firmware/efi/libstub/lib.a
 
-# The compressed kernel is built with -fPIC/-fPIE so that a boot loader
-# can place it anywhere in memory and it will still run. However, since
-# it is executed as-is without any ELF relocation processing performed
-# (and has already had all relocation sections stripped from the binary),
-# none of the code can use data relocations (e.g. static assignments of
-# pointer values), since they will be meaningless at runtime. This check
-# will refuse to link the vmlinux if any of these relocations are found.
-quiet_cmd_check_data_rel = DATAREL $@
-define cmd_check_data_rel
-	for obj in $(filter %.o,$^); do \
-		$(READELF) -S $$obj | grep -qF .rel.local &amp;&amp; { \
-			echo "error: $$obj has data relocations!" &gt;&amp;2; \
-			exit 1; \
-		} || true; \
-	done
-endef
-
-# We need to run two commands under "if_changed", so merge them into a
-# single invocation.
-quiet_cmd_check-and-link-vmlinux = LD      $@
-      cmd_check-and-link-vmlinux = $(cmd_check_data_rel); $(cmd_ld)
-
 $(obj)/vmlinux: $(vmlinux-objs-y) $(efi-obj-y) FORCE
-	$(call if_changed,check-and-link-vmlinux)
+	$(call if_changed,ld)
 
 OBJCOPYFLAGS_vmlinux.bin :=  -R .comment -S
 $(obj)/vmlinux.bin: vmlinux FORCE
diff --git a/arch/x86/boot/compressed/vmlinux.lds.S b/arch/x86/boot/compressed/vmlinux.lds.S
index a4a4a59a2628..29df99b6cc64 100644
--- a/arch/x86/boot/compressed/vmlinux.lds.S
+++ b/arch/x86/boot/compressed/vmlinux.lds.S
@@ -42,6 +42,12 @@ SECTIONS
 		*(.rodata.*)
 		_erodata = . ;
 	}
+	.rel.dyn : {
+		*(.rel.*)
+	}
+	.rela.dyn : {
+		*(.rela.*)
+	}
 	.got : {
 		*(.got)
 	}
@@ -85,3 +91,5 @@ ASSERT(SIZEOF(.got.plt) == 0 || SIZEOF(.got.plt) == 0x18, "Unexpected GOT/PLT en
 #else
 ASSERT(SIZEOF(.got.plt) == 0 || SIZEOF(.got.plt) == 0xc, "Unexpected GOT/PLT entries detected!")
 #endif
+
+ASSERT(SIZEOF(.rel.dyn) == 0 &amp;&amp; SIZEOF(.rela.dyn) == 0, "Unexpected run-time relocations detected!")</pre><hr><pre>commit 3f086189cd3641d212949ff044d8e4486c93d55e
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Fri Jul 31 16:07:50 2020 -0700

    x86/boot: Remove run-time relocations from head_{32,64}.S
    
    The BFD linker generates run-time relocations for z_input_len and
    z_output_len, even though they are absolute symbols.
    
    This is fixed for binutils-2.35 [1]. Work around this for earlier
    versions by defining two variables input_len and output_len in addition
    to the symbols, and use them via position-independent references.
    
    This eliminates the last two run-time relocations in the head code and
    allows us to drop the -z noreloc-overflow flag to the linker.
    
    Move the -pie and --no-dynamic-linker LDFLAGS to LDFLAGS_vmlinux instead
    of KBUILD_LDFLAGS. There shouldn't be anything else getting linked, but
    this is the more logical location for these flags, and modversions might
    call the linker if an EXPORT_SYMBOL is left over accidentally in one of
    the decompressors.
    
    [1] https://sourceware.org/bugzilla/show_bug.cgi?id=25754
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Kees Cook &lt;keescook@chromium.org&gt;
    Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;
    Tested-by: Nick Desaulniers &lt;ndesaulniers@google.com&gt;
    Tested-by: Sedat Dilek &lt;sedat.dilek@gmail.com&gt;
    Reviewed-by: Kees Cook &lt;keescook@chromium.org&gt;
    Reviewed-by: Ard Biesheuvel &lt;ardb@kernel.org&gt;
    Reviewed-by: Fangrui Song &lt;maskray@google.com&gt;
    Link: https://lore.kernel.org/r/20200731230820.1742553-7-keescook@chromium.org

diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 7c687a770537..7d25089c5a7b 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -52,16 +52,8 @@ UBSAN_SANITIZE :=n
 KBUILD_LDFLAGS := -m elf_$(UTS_MACHINE)
 # Compressed kernel should be built as PIE since it may be loaded at any
 # address by the bootloader.
-ifeq ($(CONFIG_X86_32),y)
-KBUILD_LDFLAGS += $(call ld-option, -pie) $(call ld-option, --no-dynamic-linker)
-else
-# To build 64-bit compressed kernel as PIE, we disable relocation
-# overflow check to avoid relocation overflow error with a new linker
-# command-line option, -z noreloc-overflow.
-KBUILD_LDFLAGS += $(shell $(LD) --help 2&gt;&amp;1 | grep -q "\-z noreloc-overflow" \
-	&amp;&amp; echo "-z noreloc-overflow -pie --no-dynamic-linker")
-endif
-LDFLAGS_vmlinux := -T
+LDFLAGS_vmlinux := $(call ld-option, -pie) $(call ld-option, --no-dynamic-linker)
+LDFLAGS_vmlinux += -T
 
 hostprogs	:= mkpiggy
 HOST_EXTRACFLAGS += -I$(srctree)/tools/include
diff --git a/arch/x86/boot/compressed/head_32.S b/arch/x86/boot/compressed/head_32.S
index 8c1a4f5610f5..659fad53ca82 100644
--- a/arch/x86/boot/compressed/head_32.S
+++ b/arch/x86/boot/compressed/head_32.S
@@ -178,18 +178,17 @@ SYM_FUNC_START_LOCAL_NOALIGN(.Lrelocated)
 /*
  * Do the extraction, and jump to the new kernel..
  */
-				/* push arguments for extract_kernel: */
-	pushl	$z_output_len	/* decompressed length, end of relocs */
+	/* push arguments for extract_kernel: */
 
-	pushl	%ebp		/* output address */
-
-	pushl	$z_input_len	/* input_len */
+	pushl	output_len@GOTOFF(%ebx)	/* decompressed length, end of relocs */
+	pushl	%ebp			/* output address */
+	pushl	input_len@GOTOFF(%ebx)	/* input_len */
 	leal	input_data@GOTOFF(%ebx), %eax
-	pushl	%eax		/* input_data */
+	pushl	%eax			/* input_data */
 	leal	boot_heap@GOTOFF(%ebx), %eax
-	pushl	%eax		/* heap area */
-	pushl	%esi		/* real mode pointer */
-	call	extract_kernel	/* returns kernel location in %eax */
+	pushl	%eax			/* heap area */
+	pushl	%esi			/* real mode pointer */
+	call	extract_kernel		/* returns kernel location in %eax */
 	addl	$24, %esp
 
 /*
diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S
index 11429092c224..9e46729cf162 100644
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@ -534,9 +534,9 @@ SYM_FUNC_START_LOCAL_NOALIGN(.Lrelocated)
 	movq	%rsi, %rdi		/* real mode address */
 	leaq	boot_heap(%rip), %rsi	/* malloc area for uncompression */
 	leaq	input_data(%rip), %rdx  /* input_data */
-	movl	$z_input_len, %ecx	/* input_len */
+	movl	input_len(%rip), %ecx	/* input_len */
 	movq	%rbp, %r8		/* output target address */
-	movl	$z_output_len, %r9d	/* decompressed length, end of relocs */
+	movl	output_len(%rip), %r9d	/* decompressed length, end of relocs */
 	call	extract_kernel		/* returns kernel location in %rax */
 	popq	%rsi
 
diff --git a/arch/x86/boot/compressed/mkpiggy.c b/arch/x86/boot/compressed/mkpiggy.c
index 7e01248765b2..52aa56cdbacc 100644
--- a/arch/x86/boot/compressed/mkpiggy.c
+++ b/arch/x86/boot/compressed/mkpiggy.c
@@ -60,6 +60,12 @@ int main(int argc, char *argv[])
 	printf(".incbin \"%s\"\n", argv[1]);
 	printf("input_data_end:\n");
 
+	printf(".section \".rodata\",\"a\",@progbits\n");
+	printf(".globl input_len\n");
+	printf("input_len:\n\t.long %lu\n", ilen);
+	printf(".globl output_len\n");
+	printf("output_len:\n\t.long %lu\n", (unsigned long)olen);
+
 	retval = 0;
 bail:
 	if (f)</pre><hr><pre>commit a2c4fc4d4e2c40b07534094810d915c7354d84a7
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Fri Jul 31 16:07:49 2020 -0700

    x86/boot: Remove run-time relocations from .head.text code
    
    The assembly code in head_{32,64}.S, while meant to be
    position-independent, generates run-time relocations because it uses
    instructions such as:
    
            leal    gdt(%edx), %eax
    
    which make the assembler and linker think that the code is using %edx as
    an index into gdt, and hence gdt needs to be relocated to its run-time
    address.
    
    On 32-bit, with lld Dmitry Golovin reports that this results in a
    link-time error with default options (i.e. unless -z notext is
    explicitly passed):
    
      LD      arch/x86/boot/compressed/vmlinux
      ld.lld: error: can't create dynamic relocation R_386_32 against local
      symbol in readonly segment; recompile object files with -fPIC or pass
      '-Wl,-z,notext' to allow text relocations in the output
    
    With the BFD linker, this generates a warning during the build, if
    --warn-shared-textrel is enabled, which at least Gentoo enables by
    default:
    
      LD      arch/x86/boot/compressed/vmlinux
      ld: arch/x86/boot/compressed/head_32.o: warning: relocation in read-only section `.head.text'
      ld: warning: creating a DT_TEXTREL in object
    
    On 64-bit, it is not possible to link the kernel as -pie with lld, and
    it is only possible with a BFD linker that supports -z noreloc-overflow,
    i.e. versions &gt;2.26. This is because these instructions cannot really be
    relocated: the displacement field is only 32-bits wide, and thus cannot
    be relocated for a 64-bit load address. The -z noreloc-overflow option
    simply overrides the linker error, and results in R_X86_64_RELATIVE
    relocations that apply a 64-bit relocation to a 32-bit field anyway.
    This happens to work because nothing will process these run-time
    relocations.
    
    Start fixing this by removing relocations from .head.text:
    
    - On 32-bit, use a base register that holds the address of the GOT and
      reference symbol addresses using @GOTOFF, i.e.
            leal    gdt@GOTOFF(%edx), %eax
    
    - On 64-bit, most of the code can (and already does) use %rip-relative
      addressing, however the .code32 bits can't, and the 64-bit code also
      needs to reference symbol addresses as they will be after moving the
      compressed kernel to the end of the decompression buffer.
      For these cases, reference the symbols as an offset to startup_32 to
      avoid creating relocations, i.e.:
    
            leal    (gdt-startup_32)(%bp), %eax
    
      This only works in .head.text as the subtraction cannot be represented
      as a PC-relative relocation unless startup_32 is in the same section
      as the code. Move efi32_pe_entry into .head.text so that it can use
      the same method to avoid relocations.
    
    Reported-by: Dmitry Golovin &lt;dima@golovin.in&gt;
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Kees Cook &lt;keescook@chromium.org&gt;
    Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;
    Tested-by: Nick Desaulniers &lt;ndesaulniers@google.com&gt;
    Tested-by: Sedat Dilek &lt;sedat.dilek@gmail.com&gt;
    Reviewed-by: Kees Cook &lt;keescook@chromium.org&gt;
    Reviewed-by: Ard Biesheuvel &lt;ardb@kernel.org&gt;
    Reviewed-by: Fangrui Song &lt;maskray@google.com&gt;
    Link: https://lore.kernel.org/r/20200731230820.1742553-6-keescook@chromium.org

diff --git a/arch/x86/boot/compressed/head_32.S b/arch/x86/boot/compressed/head_32.S
index 39f0bb43218f..8c1a4f5610f5 100644
--- a/arch/x86/boot/compressed/head_32.S
+++ b/arch/x86/boot/compressed/head_32.S
@@ -33,26 +33,10 @@
 #include &lt;asm/bootparam.h&gt;
 
 /*
- * The 32-bit x86 assembler in binutils 2.26 will generate R_386_GOT32X
- * relocation to get the symbol address in PIC.  When the compressed x86
- * kernel isn't built as PIC, the linker optimizes R_386_GOT32X
- * relocations to their fixed symbol addresses.  However, when the
- * compressed x86 kernel is loaded at a different address, it leads
- * to the following load failure:
- *
- *   Failed to allocate space for phdrs
- *
- * during the decompression stage.
- *
- * If the compressed x86 kernel is relocatable at run-time, it should be
- * compiled with -fPIE, instead of -fPIC, if possible and should be built as
- * Position Independent Executable (PIE) so that linker won't optimize
- * R_386_GOT32X relocation to its fixed symbol address.  Older
- * linkers generate R_386_32 relocations against locally defined symbols,
- * _bss, _ebss and _end, in PIE.  It isn't wrong, just less optimal than
- * R_386_RELATIVE.  But the x86 kernel fails to properly handle R_386_32
- * relocations when relocating the kernel.  To generate R_386_RELATIVE
- * relocations, we mark _bss, _ebss and _end as hidden:
+ * These symbols needed to be marked as .hidden to prevent the BFD linker from
+ * generating R_386_32 (rather than R_386_RELATIVE) relocations for them when
+ * the 32-bit compressed kernel is linked as PIE. This is no longer necessary,
+ * but it doesn't hurt to keep them .hidden.
  */
 	.hidden _bss
 	.hidden _ebss
@@ -74,10 +58,10 @@ SYM_FUNC_START(startup_32)
 	leal	(BP_scratch+4)(%esi), %esp
 	call	1f
 1:	popl	%edx
-	subl	$1b, %edx
+	addl	$_GLOBAL_OFFSET_TABLE_+(.-1b), %edx
 
 	/* Load new GDT */
-	leal	gdt(%edx), %eax
+	leal	gdt@GOTOFF(%edx), %eax
 	movl	%eax, 2(%eax)
 	lgdt	(%eax)
 
@@ -90,14 +74,16 @@ SYM_FUNC_START(startup_32)
 	movl	%eax, %ss
 
 /*
- * %edx contains the address we are loaded at by the boot loader and %ebx
- * contains the address where we should move the kernel image temporarily
- * for safe in-place decompression. %ebp contains the address that the kernel
- * will be decompressed to.
+ * %edx contains the address we are loaded at by the boot loader (plus the
+ * offset to the GOT).  The below code calculates %ebx to be the address where
+ * we should move the kernel image temporarily for safe in-place decompression
+ * (again, plus the offset to the GOT).
+ *
+ * %ebp is calculated to be the address that the kernel will be decompressed to.
  */
 
 #ifdef CONFIG_RELOCATABLE
-	movl	%edx, %ebx
+	leal	startup_32@GOTOFF(%edx), %ebx
 
 #ifdef CONFIG_EFI_STUB
 /*
@@ -108,7 +94,7 @@ SYM_FUNC_START(startup_32)
  *	image_offset = startup_32 - image_base
  * Otherwise image_offset will be zero and has no effect on the calculations.
  */
-	subl    image_offset(%edx), %ebx
+	subl    image_offset@GOTOFF(%edx), %ebx
 #endif
 
 	movl	BP_kernel_alignment(%esi), %eax
@@ -125,10 +111,10 @@ SYM_FUNC_START(startup_32)
 	movl	%ebx, %ebp	// Save the output address for later
 	/* Target address to relocate to for decompression */
 	addl    BP_init_size(%esi), %ebx
-	subl    $_end, %ebx
+	subl    $_end@GOTOFF, %ebx
 
 	/* Set up the stack */
-	leal	boot_stack_end(%ebx), %esp
+	leal	boot_stack_end@GOTOFF(%ebx), %esp
 
 	/* Zero EFLAGS */
 	pushl	$0
@@ -139,8 +125,8 @@ SYM_FUNC_START(startup_32)
  * where decompression in place becomes safe.
  */
 	pushl	%esi
-	leal	(_bss-4)(%edx), %esi
-	leal	(_bss-4)(%ebx), %edi
+	leal	(_bss@GOTOFF-4)(%edx), %esi
+	leal	(_bss@GOTOFF-4)(%ebx), %edi
 	movl	$(_bss - startup_32), %ecx
 	shrl	$2, %ecx
 	std
@@ -153,14 +139,14 @@ SYM_FUNC_START(startup_32)
 	 * during extract_kernel below. To avoid any issues, repoint the GDTR
 	 * to the new copy of the GDT.
 	 */
-	leal	gdt(%ebx), %eax
+	leal	gdt@GOTOFF(%ebx), %eax
 	movl	%eax, 2(%eax)
 	lgdt	(%eax)
 
 /*
  * Jump to the relocated address.
  */
-	leal	.Lrelocated(%ebx), %eax
+	leal	.Lrelocated@GOTOFF(%ebx), %eax
 	jmp	*%eax
 SYM_FUNC_END(startup_32)
 
@@ -170,7 +156,7 @@ SYM_FUNC_START_ALIAS(efi_stub_entry)
 	add	$0x4, %esp
 	movl	8(%esp), %esi	/* save boot_params pointer */
 	call	efi_main
-	leal	startup_32(%eax), %eax
+	/* efi_main returns the possibly relocated address of startup_32 */
 	jmp	*%eax
 SYM_FUNC_END(efi32_stub_entry)
 SYM_FUNC_END_ALIAS(efi_stub_entry)
@@ -183,8 +169,8 @@ SYM_FUNC_START_LOCAL_NOALIGN(.Lrelocated)
  * Clear BSS (stack is currently empty)
  */
 	xorl	%eax, %eax
-	leal	_bss(%ebx), %edi
-	leal	_ebss(%ebx), %ecx
+	leal	_bss@GOTOFF(%ebx), %edi
+	leal	_ebss@GOTOFF(%ebx), %ecx
 	subl	%edi, %ecx
 	shrl	$2, %ecx
 	rep	stosl
@@ -198,9 +184,9 @@ SYM_FUNC_START_LOCAL_NOALIGN(.Lrelocated)
 	pushl	%ebp		/* output address */
 
 	pushl	$z_input_len	/* input_len */
-	leal	input_data(%ebx), %eax
+	leal	input_data@GOTOFF(%ebx), %eax
 	pushl	%eax		/* input_data */
-	leal	boot_heap(%ebx), %eax
+	leal	boot_heap@GOTOFF(%ebx), %eax
 	pushl	%eax		/* heap area */
 	pushl	%esi		/* real mode pointer */
 	call	extract_kernel	/* returns kernel location in %eax */
diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S
index bf1ab30acc5b..11429092c224 100644
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@ -43,6 +43,32 @@
 	.hidden _end
 
 	__HEAD
+
+/*
+ * This macro gives the relative virtual address of X, i.e. the offset of X
+ * from startup_32. This is the same as the link-time virtual address of X,
+ * since startup_32 is at 0, but defining it this way tells the
+ * assembler/linker that we do not want the actual run-time address of X. This
+ * prevents the linker from trying to create unwanted run-time relocation
+ * entries for the reference when the compressed kernel is linked as PIE.
+ *
+ * A reference X(%reg) will result in the link-time VA of X being stored with
+ * the instruction, and a run-time R_X86_64_RELATIVE relocation entry that
+ * adds the 64-bit base address where the kernel is loaded.
+ *
+ * Replacing it with (X-startup_32)(%reg) results in the offset being stored,
+ * and no run-time relocation.
+ *
+ * The macro should be used as a displacement with a base register containing
+ * the run-time address of startup_32 [i.e. rva(X)(%reg)], or as an immediate
+ * [$ rva(X)].
+ *
+ * This macro can only be used from within the .head.text section, since the
+ * expression requires startup_32 to be in the same section as the code being
+ * assembled.
+ */
+#define rva(X) ((X) - startup_32)
+
 	.code32
 SYM_FUNC_START(startup_32)
 	/*
@@ -65,10 +91,10 @@ SYM_FUNC_START(startup_32)
 	leal	(BP_scratch+4)(%esi), %esp
 	call	1f
 1:	popl	%ebp
-	subl	$1b, %ebp
+	subl	$ rva(1b), %ebp
 
 	/* Load new GDT with the 64bit segments using 32bit descriptor */
-	leal	gdt(%ebp), %eax
+	leal	rva(gdt)(%ebp), %eax
 	movl	%eax, 2(%eax)
 	lgdt	(%eax)
 
@@ -81,7 +107,7 @@ SYM_FUNC_START(startup_32)
 	movl	%eax, %ss
 
 /* setup a stack and make sure cpu supports long mode. */
-	leal	boot_stack_end(%ebp), %esp
+	leal	rva(boot_stack_end)(%ebp), %esp
 
 	call	verify_cpu
 	testl	%eax, %eax
@@ -108,7 +134,7 @@ SYM_FUNC_START(startup_32)
  *	image_offset = startup_32 - image_base
  * Otherwise image_offset will be zero and has no effect on the calculations.
  */
-	subl    image_offset(%ebp), %ebx
+	subl    rva(image_offset)(%ebp), %ebx
 #endif
 
 	movl	BP_kernel_alignment(%esi), %eax
@@ -124,7 +150,7 @@ SYM_FUNC_START(startup_32)
 
 	/* Target address to relocate to for decompression */
 	addl	BP_init_size(%esi), %ebx
-	subl	$_end, %ebx
+	subl	$ rva(_end), %ebx
 
 /*
  * Prepare for entering 64 bit mode
@@ -152,19 +178,19 @@ SYM_FUNC_START(startup_32)
 1:
 
 	/* Initialize Page tables to 0 */
-	leal	pgtable(%ebx), %edi
+	leal	rva(pgtable)(%ebx), %edi
 	xorl	%eax, %eax
 	movl	$(BOOT_INIT_PGT_SIZE/4), %ecx
 	rep	stosl
 
 	/* Build Level 4 */
-	leal	pgtable + 0(%ebx), %edi
+	leal	rva(pgtable + 0)(%ebx), %edi
 	leal	0x1007 (%edi), %eax
 	movl	%eax, 0(%edi)
 	addl	%edx, 4(%edi)
 
 	/* Build Level 3 */
-	leal	pgtable + 0x1000(%ebx), %edi
+	leal	rva(pgtable + 0x1000)(%ebx), %edi
 	leal	0x1007(%edi), %eax
 	movl	$4, %ecx
 1:	movl	%eax, 0x00(%edi)
@@ -175,7 +201,7 @@ SYM_FUNC_START(startup_32)
 	jnz	1b
 
 	/* Build Level 2 */
-	leal	pgtable + 0x2000(%ebx), %edi
+	leal	rva(pgtable + 0x2000)(%ebx), %edi
 	movl	$0x00000183, %eax
 	movl	$2048, %ecx
 1:	movl	%eax, 0(%edi)
@@ -186,7 +212,7 @@ SYM_FUNC_START(startup_32)
 	jnz	1b
 
 	/* Enable the boot page tables */
-	leal	pgtable(%ebx), %eax
+	leal	rva(pgtable)(%ebx), %eax
 	movl	%eax, %cr3
 
 	/* Enable Long mode in EFER (Extended Feature Enable Register) */
@@ -211,14 +237,14 @@ SYM_FUNC_START(startup_32)
 	 * We place all of the values on our mini stack so lret can
 	 * used to perform that far jump.
 	 */
-	leal	startup_64(%ebp), %eax
+	leal	rva(startup_64)(%ebp), %eax
 #ifdef CONFIG_EFI_MIXED
-	movl	efi32_boot_args(%ebp), %edi
+	movl	rva(efi32_boot_args)(%ebp), %edi
 	cmp	$0, %edi
 	jz	1f
-	leal	efi64_stub_entry(%ebp), %eax
-	movl	efi32_boot_args+4(%ebp), %esi
-	movl	efi32_boot_args+8(%ebp), %edx	// saved bootparams pointer
+	leal	rva(efi64_stub_entry)(%ebp), %eax
+	movl	rva(efi32_boot_args+4)(%ebp), %esi
+	movl	rva(efi32_boot_args+8)(%ebp), %edx	// saved bootparams pointer
 	cmpl	$0, %edx
 	jnz	1f
 	/*
@@ -229,7 +255,7 @@ SYM_FUNC_START(startup_32)
 	 * the correct stack alignment for entry.
 	 */
 	subl	$40, %esp
-	leal	efi_pe_entry(%ebp), %eax
+	leal	rva(efi_pe_entry)(%ebp), %eax
 	movl	%edi, %ecx			// MS calling convention
 	movl	%esi, %edx
 1:
@@ -255,18 +281,18 @@ SYM_FUNC_START(efi32_stub_entry)
 
 	call	1f
 1:	pop	%ebp
-	subl	$1b, %ebp
+	subl	$ rva(1b), %ebp
 
-	movl	%esi, efi32_boot_args+8(%ebp)
+	movl	%esi, rva(efi32_boot_args+8)(%ebp)
 SYM_INNER_LABEL(efi32_pe_stub_entry, SYM_L_LOCAL)
-	movl	%ecx, efi32_boot_args(%ebp)
-	movl	%edx, efi32_boot_args+4(%ebp)
-	movb	$0, efi_is64(%ebp)
+	movl	%ecx, rva(efi32_boot_args)(%ebp)
+	movl	%edx, rva(efi32_boot_args+4)(%ebp)
+	movb	$0, rva(efi_is64)(%ebp)
 
 	/* Save firmware GDTR and code/data selectors */
-	sgdtl	efi32_boot_gdt(%ebp)
-	movw	%cs, efi32_boot_cs(%ebp)
-	movw	%ds, efi32_boot_ds(%ebp)
+	sgdtl	rva(efi32_boot_gdt)(%ebp)
+	movw	%cs, rva(efi32_boot_cs)(%ebp)
+	movw	%ds, rva(efi32_boot_ds)(%ebp)
 
 	/* Disable paging */
 	movl	%cr0, %eax
@@ -345,11 +371,11 @@ SYM_CODE_START(startup_64)
 
 	/* Target address to relocate to for decompression */
 	movl	BP_init_size(%rsi), %ebx
-	subl	$_end, %ebx
+	subl	$ rva(_end), %ebx
 	addq	%rbp, %rbx
 
 	/* Set up the stack */
-	leaq	boot_stack_end(%rbx), %rsp
+	leaq	rva(boot_stack_end)(%rbx), %rsp
 
 	/*
 	 * At this point we are in long mode with 4-level paging enabled,
@@ -423,7 +449,7 @@ SYM_CODE_START(startup_64)
 	lretq
 trampoline_return:
 	/* Restore the stack, the 32-bit trampoline uses its own stack */
-	leaq	boot_stack_end(%rbx), %rsp
+	leaq	rva(boot_stack_end)(%rbx), %rsp
 
 	/*
 	 * cleanup_trampoline() would restore trampoline memory.
@@ -435,7 +461,7 @@ trampoline_return:
 	 * this function call.
 	 */
 	pushq	%rsi
-	leaq	top_pgtable(%rbx), %rdi
+	leaq	rva(top_pgtable)(%rbx), %rdi
 	call	cleanup_trampoline
 	popq	%rsi
 
@@ -449,9 +475,9 @@ trampoline_return:
  */
 	pushq	%rsi
 	leaq	(_bss-8)(%rip), %rsi
-	leaq	(_bss-8)(%rbx), %rdi
-	movq	$_bss /* - $startup_32 */, %rcx
-	shrq	$3, %rcx
+	leaq	rva(_bss-8)(%rbx), %rdi
+	movl	$(_bss - startup_32), %ecx
+	shrl	$3, %ecx
 	std
 	rep	movsq
 	cld
@@ -462,15 +488,15 @@ trampoline_return:
 	 * during extract_kernel below. To avoid any issues, repoint the GDTR
 	 * to the new copy of the GDT.
 	 */
-	leaq	gdt64(%rbx), %rax
-	leaq	gdt(%rbx), %rdx
+	leaq	rva(gdt64)(%rbx), %rax
+	leaq	rva(gdt)(%rbx), %rdx
 	movq	%rdx, 2(%rax)
 	lgdt	(%rax)
 
 /*
  * Jump to the relocated address.
  */
-	leaq	.Lrelocated(%rbx), %rax
+	leaq	rva(.Lrelocated)(%rbx), %rax
 	jmp	*%rax
 SYM_CODE_END(startup_64)
 
@@ -482,7 +508,7 @@ SYM_FUNC_START_ALIAS(efi_stub_entry)
 	movq	%rdx, %rbx			/* save boot_params pointer */
 	call	efi_main
 	movq	%rbx,%rsi
-	leaq	startup_64(%rax), %rax
+	leaq	rva(startup_64)(%rax), %rax
 	jmp	*%rax
 SYM_FUNC_END(efi64_stub_entry)
 SYM_FUNC_END_ALIAS(efi_stub_entry)
@@ -645,7 +671,7 @@ SYM_DATA(efi_is64, .byte 1)
 #define BS32_handle_protocol	88 // offsetof(efi_boot_services_32_t, handle_protocol)
 #define LI32_image_base		32 // offsetof(efi_loaded_image_32_t, image_base)
 
-	.text
+	__HEAD
 	.code32
 SYM_FUNC_START(efi32_pe_entry)
 /*
@@ -667,12 +693,12 @@ SYM_FUNC_START(efi32_pe_entry)
 
 	call	1f
 1:	pop	%ebx
-	subl	$1b, %ebx
+	subl	$ rva(1b), %ebx
 
 	/* Get the loaded image protocol pointer from the image handle */
 	leal	-4(%ebp), %eax
 	pushl	%eax				// &amp;loaded_image
-	leal	loaded_image_proto(%ebx), %eax
+	leal	rva(loaded_image_proto)(%ebx), %eax
 	pushl	%eax				// pass the GUID address
 	pushl	8(%ebp)				// pass the image handle
 
@@ -707,7 +733,7 @@ SYM_FUNC_START(efi32_pe_entry)
 	 * use it before we get to the 64-bit efi_pe_entry() in C code.
 	 */
 	subl	%esi, %ebx
-	movl	%ebx, image_offset(%ebp)	// save image_offset
+	movl	%ebx, rva(image_offset)(%ebp)	// save image_offset
 	jmp	efi32_pe_stub_entry
 
 2:	popl	%edi				// restore callee-save registers</pre><hr><pre>commit 2e7a858ba843d2e6ceab1ba996805411de51b340
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Fri Jul 31 16:07:48 2020 -0700

    x86/boot: Add .text.* to setup.ld
    
    GCC puts the main function into .text.startup when compiled with -Os (or
    -O2). This results in arch/x86/boot/main.c having a .text.startup
    section which is currently not included explicitly in the linker script
    setup.ld in the same directory.
    
    The BFD linker places this orphan section immediately after .text, so
    this still works. However, LLD git, since [1], is choosing to place it
    immediately after the .bstext section instead (this is the first code
    section). This plays havoc with the section layout that setup.elf
    requires to create the setup header, for eg on 64-bit:
    
        LD      arch/x86/boot/setup.elf
      ld.lld: error: section .text.startup file range overlaps with .header
      &gt;&gt;&gt; .text.startup range is [0x200040, 0x2001FE]
      &gt;&gt;&gt; .header range is [0x2001EF, 0x20026B]
    
      ld.lld: error: section .header file range overlaps with .bsdata
      &gt;&gt;&gt; .header range is [0x2001EF, 0x20026B]
      &gt;&gt;&gt; .bsdata range is [0x2001FF, 0x200398]
    
      ld.lld: error: section .bsdata file range overlaps with .entrytext
      &gt;&gt;&gt; .bsdata range is [0x2001FF, 0x200398]
      &gt;&gt;&gt; .entrytext range is [0x20026C, 0x2002D3]
    
      ld.lld: error: section .text.startup virtual address range overlaps
      with .header
      &gt;&gt;&gt; .text.startup range is [0x40, 0x1FE]
      &gt;&gt;&gt; .header range is [0x1EF, 0x26B]
    
      ld.lld: error: section .header virtual address range overlaps with
      .bsdata
      &gt;&gt;&gt; .header range is [0x1EF, 0x26B]
      &gt;&gt;&gt; .bsdata range is [0x1FF, 0x398]
    
      ld.lld: error: section .bsdata virtual address range overlaps with
      .entrytext
      &gt;&gt;&gt; .bsdata range is [0x1FF, 0x398]
      &gt;&gt;&gt; .entrytext range is [0x26C, 0x2D3]
    
      ld.lld: error: section .text.startup load address range overlaps with
      .header
      &gt;&gt;&gt; .text.startup range is [0x40, 0x1FE]
      &gt;&gt;&gt; .header range is [0x1EF, 0x26B]
    
      ld.lld: error: section .header load address range overlaps with
      .bsdata
      &gt;&gt;&gt; .header range is [0x1EF, 0x26B]
      &gt;&gt;&gt; .bsdata range is [0x1FF, 0x398]
    
      ld.lld: error: section .bsdata load address range overlaps with
      .entrytext
      &gt;&gt;&gt; .bsdata range is [0x1FF, 0x398]
      &gt;&gt;&gt; .entrytext range is [0x26C, 0x2D3]
    
    Add .text.* to the .text output section to fix this, and also prevent
    any future surprises if the compiler decides to create other such
    sections.
    
    [1] https://reviews.llvm.org/D75225
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Kees Cook &lt;keescook@chromium.org&gt;
    Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;
    Tested-by: Nick Desaulniers &lt;ndesaulniers@google.com&gt;
    Tested-by: Sedat Dilek &lt;sedat.dilek@gmail.com&gt;
    Reviewed-by: Kees Cook &lt;keescook@chromium.org&gt;
    Reviewed-by: Ard Biesheuvel &lt;ardb@kernel.org&gt;
    Reviewed-by: Fangrui Song &lt;maskray@google.com&gt;
    Link: https://lore.kernel.org/r/20200731230820.1742553-5-keescook@chromium.org

diff --git a/arch/x86/boot/setup.ld b/arch/x86/boot/setup.ld
index 24c95522f231..49546c247ae2 100644
--- a/arch/x86/boot/setup.ld
+++ b/arch/x86/boot/setup.ld
@@ -20,7 +20,7 @@ SECTIONS
 	.initdata	: { *(.initdata) }
 	__end_init = .;
 
-	.text		: { *(.text) }
+	.text		: { *(.text .text.*) }
 	.text32		: { *(.text32) }
 
 	. = ALIGN(16);</pre><hr><pre>commit 376653435dacf84a8aca87e66aff94079a817cf2
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Tue Aug 11 18:34:16 2020 -0700

    kernel.h: remove duplicate include of asm/div64.h
    
    This seems to have been added inadvertently in commit
      72deb455b5ec ("block: remove CONFIG_LBDAF")
    
    Fixes: 72deb455b5ec ("block: remove CONFIG_LBDAF")
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Reviewed-by: Christoph Hellwig &lt;hch@lst.de&gt;
    Link: http://lkml.kernel.org/r/20200727034852.2813453-1-nivedita@alum.mit.edu
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/include/linux/kernel.h b/include/linux/kernel.h
index 7339a00c895e..e19c13616666 100644
--- a/include/linux/kernel.h
+++ b/include/linux/kernel.h
@@ -17,7 +17,6 @@
 #include &lt;asm/byteorder.h&gt;
 #include &lt;asm/div64.h&gt;
 #include &lt;uapi/linux/kernel.h&gt;
-#include &lt;asm/div64.h&gt;
 
 #define STACK_MAGIC	0xdeadbeef
 </pre><hr><pre>commit 6f8c00ff5aa88c847af9af35131d72fae480b566
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Tue Aug 11 18:32:46 2020 -0700

    sparc: drop unused MAX_PHYSADDR_BITS
    
    The macro is not used anywhere, so remove the definition.
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Reviewed-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Reviewed-by: David Hildenbrand &lt;david@redhat.com&gt;
    Acked-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
    Acked-by: David S. Miller &lt;davem@davemloft.net&gt;
    Acked-by: Mike Rapoport &lt;rppt@linux.ibm.com&gt;
    Link: http://lkml.kernel.org/r/20200723231544.17274-4-nivedita@alum.mit.edu
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/arch/sparc/include/asm/sparsemem.h b/arch/sparc/include/asm/sparsemem.h
index 1dd1b61432db..aa9a676bc341 100644
--- a/arch/sparc/include/asm/sparsemem.h
+++ b/arch/sparc/include/asm/sparsemem.h
@@ -7,7 +7,6 @@
 #include &lt;asm/page.h&gt;
 
 #define SECTION_SIZE_BITS       30
-#define MAX_PHYSADDR_BITS       MAX_PHYS_ADDRESS_BITS
 #define MAX_PHYSMEM_BITS        MAX_PHYS_ADDRESS_BITS
 
 #endif /* !(__KERNEL__) */</pre><hr><pre>commit c56771b3095d4f1af82847081b6781fc03a13904
Author: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
Date:   Tue Aug 11 18:32:43 2020 -0700

    sh/mm: drop unused MAX_PHYSADDR_BITS
    
    The macro is not used anywhere, so remove the definition.
    
    Signed-off-by: Arvind Sankar &lt;nivedita@alum.mit.edu&gt;
    Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Reviewed-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;
    Reviewed-by: David Hildenbrand &lt;david@redhat.com&gt;
    Acked-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
    Acked-by: Mike Rapoport &lt;rppt@linux.ibm.com&gt;
    Link: http://lkml.kernel.org/r/20200723231544.17274-3-nivedita@alum.mit.edu
    Signed-off-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

diff --git a/arch/sh/include/asm/sparsemem.h b/arch/sh/include/asm/sparsemem.h
index 4eb899751e45..084706bb8cca 100644
--- a/arch/sh/include/asm/sparsemem.h
+++ b/arch/sh/include/asm/sparsemem.h
@@ -5,11 +5,9 @@
 #ifdef __KERNEL__
 /*
  * SECTION_SIZE_BITS		2^N: how big each section will be
- * MAX_PHYSADDR_BITS		2^N: how much physical address space we have
- * MAX_PHYSMEM_BITS		2^N: how much memory we can have in that space
+ * MAX_PHYSMEM_BITS		2^N: how much physical address space we have
  */
 #define SECTION_SIZE_BITS	26
-#define MAX_PHYSADDR_BITS	32
 #define MAX_PHYSMEM_BITS	32
 
 #endif</pre>
    <div class="pagination">
        <a href='1_116.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><a href='1_5.html'>5</a><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><span>[117]</span><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_118.html'>Next&gt;&gt;</a>
    <div>
</body>

<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Patches contributed by Massachusetts Institute of Technology</title>
    <style>
    .pagination {
        border-top: 1px solid #ddd;
        border-bottom: 1px solid #ddd;
        overflow-wrap: break-word;
    }
    .pagination a, .pagination span {
        margin: 0 4px;
    }

    </style>
</head>
<body>
    <h1>Patches contributed by Massachusetts Institute of Technology</h1>
    <div class="pagination">
        <a href='1_4.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><span>[5]</span><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_6.html'>Next&gt;&gt;</a>
    </div>
    <hr>
    <pre>commit 85d825dbf4899a69407338bae462a59aa9a37326
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Apr 14 21:57:49 2022 -0400

    ext4: force overhead calculation if the s_overhead_cluster makes no sense
    
    If the file system does not use bigalloc, calculating the overhead is
    cheap, so force the recalculation of the overhead so we don't have to
    trust the precalculated overhead in the superblock.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: stable@kernel.org

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index 23a9b2c086ed..d08820fdfdee 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -5289,9 +5289,18 @@ static int __ext4_fill_super(struct fs_context *fc, struct super_block *sb)
 	 * Get the # of file system overhead blocks from the
 	 * superblock if present.
 	 */
-	if (es-&gt;s_overhead_clusters)
-		sbi-&gt;s_overhead = le32_to_cpu(es-&gt;s_overhead_clusters);
-	else {
+	sbi-&gt;s_overhead = le32_to_cpu(es-&gt;s_overhead_clusters);
+	/* ignore the precalculated value if it is ridiculous */
+	if (sbi-&gt;s_overhead &gt; ext4_blocks_count(es))
+		sbi-&gt;s_overhead = 0;
+	/*
+	 * If the bigalloc feature is not enabled recalculating the
+	 * overhead doesn't take long, so we might as well just redo
+	 * it to make sure we are using the correct value.
+	 */
+	if (!ext4_has_feature_bigalloc(sb))
+		sbi-&gt;s_overhead = 0;
+	if (sbi-&gt;s_overhead == 0) {
 		err = ext4_calculate_overhead(sb);
 		if (err)
 			goto failed_mount_wq;</pre><hr><pre>commit 10b01ee92df52c8d7200afead4d5e5f55a5c58b1
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Apr 14 21:31:27 2022 -0400

    ext4: fix overhead calculation to account for the reserved gdt blocks
    
    The kernel calculation was underestimating the overhead by not taking
    into account the reserved gdt blocks.  With this change, the overhead
    calculated by the kernel matches the overhead calculation in mke2fs.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Cc: stable@kernel.org

diff --git a/fs/ext4/super.c b/fs/ext4/super.c
index f2a5e78f93a9..23a9b2c086ed 100644
--- a/fs/ext4/super.c
+++ b/fs/ext4/super.c
@@ -4177,9 +4177,11 @@ static int count_overhead(struct super_block *sb, ext4_group_t grp,
 	ext4_fsblk_t		first_block, last_block, b;
 	ext4_group_t		i, ngroups = ext4_get_groups_count(sb);
 	int			s, j, count = 0;
+	int			has_super = ext4_bg_has_super(sb, grp);
 
 	if (!ext4_has_feature_bigalloc(sb))
-		return (ext4_bg_has_super(sb, grp) + ext4_bg_num_gdb(sb, grp) +
+		return (has_super + ext4_bg_num_gdb(sb, grp) +
+			(has_super ? le16_to_cpu(sbi-&gt;s_es-&gt;s_reserved_gdt_blocks) : 0) +
 			sbi-&gt;s_itb_per_group + 2);
 
 	first_block = le32_to_cpu(sbi-&gt;s_es-&gt;s_first_data_block) +</pre><hr><pre>commit 919adbfec29d5b89b3e45620653cbeeb0d42e6fd
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Mar 12 21:39:35 2022 -0500

    ext4: fix kernel doc warnings
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/balloc.c b/fs/ext4/balloc.c
index a0fb0c4bdc7c..78ee3ef795ae 100644
--- a/fs/ext4/balloc.c
+++ b/fs/ext4/balloc.c
@@ -411,6 +411,7 @@ static int ext4_validate_block_bitmap(struct super_block *sb,
  * ext4_read_block_bitmap_nowait()
  * @sb:			super block
  * @block_group:	given block group
+ * @ignore_locked:	ignore locked buffers
  *
  * Read the bitmap for a given block_group,and validate the
  * bits for block/inode/inode tables are set in the bitmaps
diff --git a/fs/ext4/ioctl.c b/fs/ext4/ioctl.c
index a8022c2c6a58..992229ca2d83 100644
--- a/fs/ext4/ioctl.c
+++ b/fs/ext4/ioctl.c
@@ -269,7 +269,7 @@ int ext4_update_superblocks_fn(struct super_block *sb,
 	return err ? err : 0;
 }
 
-/**
+/*
  * Swap memory between @a and @b for @len bytes.
  *
  * @a:          pointer to first memory area
@@ -290,7 +290,7 @@ static void memswap(void *a, void *b, size_t len)
 	}
 }
 
-/**
+/*
  * Swap i_data and associated attributes between @inode1 and @inode2.
  * This function is used for the primary swap between inode1 and inode2
  * and also to revert this primary swap in case of errors.
@@ -344,7 +344,7 @@ void ext4_reset_inode_seed(struct inode *inode)
 	ei-&gt;i_csum_seed = ext4_chksum(sbi, csum, (__u8 *)&amp;gen, sizeof(gen));
 }
 
-/**
+/*
  * Swap the information from the given @inode and the inode
  * EXT4_BOOT_LOADER_INO. It will basically swap i_data and all other
  * important fields of the inodes.</pre><hr><pre>commit cc5095747edfb054ca2068d01af20be3fcc3634f
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Mar 3 09:38:47 2022 -0500

    ext4: don't BUG if someone dirty pages without asking ext4 first
    
    [un]pin_user_pages_remote is dirtying pages without properly warning
    the file system in advance.  A related race was noted by Jan Kara in
    2018[1]; however, more recently instead of it being a very hard-to-hit
    race, it could be reliably triggered by process_vm_writev(2) which was
    discovered by Syzbot[2].
    
    This is technically a bug in mm/gup.c, but arguably ext4 is fragile in
    that if some other kernel subsystem dirty pages without properly
    notifying the file system using page_mkwrite(), ext4 will BUG, while
    other file systems will not BUG (although data will still be lost).
    
    So instead of crashing with a BUG, issue a warning (since there may be
    potential data loss) and just mark the page as clean to avoid
    unprivileged denial of service attacks until the problem can be
    properly fixed.  More discussion and background can be found in the
    thread starting at [2].
    
    [1] https://lore.kernel.org/linux-mm/20180103100430.GE4911@quack2.suse.cz
    [2] https://lore.kernel.org/r/Yg0m6IjcNmfaSokM@google.com
    
    Reported-by: syzbot+d59332e2db681cf18f0318a06e994ebbb529a8db@syzkaller.appspotmail.com
    Reported-by: Lee Jones &lt;lee.jones@linaro.org&gt;
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Link: https://lore.kernel.org/r/YiDS9wVfq4mM2jGK@mit.edu

diff --git a/fs/ext4/inode.c b/fs/ext4/inode.c
index 01c9e4f743ba..531a94f48637 100644
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@ -1993,6 +1993,15 @@ static int ext4_writepage(struct page *page,
 	else
 		len = PAGE_SIZE;
 
+	/* Should never happen but for bugs in other kernel subsystems */
+	if (!page_has_buffers(page)) {
+		ext4_warning_inode(inode,
+		   "page %lu does not have buffers attached", page-&gt;index);
+		ClearPageDirty(page);
+		unlock_page(page);
+		return 0;
+	}
+
 	page_bufs = page_buffers(page);
 	/*
 	 * We cannot do block allocation or other extent handling in this
@@ -2594,6 +2603,22 @@ static int mpage_prepare_extent_to_map(struct mpage_da_data *mpd)
 			wait_on_page_writeback(page);
 			BUG_ON(PageWriteback(page));
 
+			/*
+			 * Should never happen but for buggy code in
+			 * other subsystems that call
+			 * set_page_dirty() without properly warning
+			 * the file system first.  See [1] for more
+			 * information.
+			 *
+			 * [1] https://lore.kernel.org/linux-mm/20180103100430.GE4911@quack2.suse.cz
+			 */
+			if (!page_has_buffers(page)) {
+				ext4_warning_inode(mpd-&gt;inode, "page %lu does not have buffers attached", page-&gt;index);
+				ClearPageDirty(page);
+				unlock_page(page);
+				continue;
+			}
+
 			if (mpd-&gt;map.m_len == 0)
 				mpd-&gt;first_page = page-&gt;index;
 			mpd-&gt;next_page = page-&gt;index + 1;</pre><hr><pre>commit 6eeaf88fd586f05aaf1d48cb3a139d2a5c6eb055
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Wed Jan 5 23:59:56 2022 -0500

    ext4: don't use the orphan list when migrating an inode
    
    We probably want to remove the indirect block to extents migration
    feature after a deprecation window, but until then, let's fix a
    potential data loss problem caused by the fact that we put the
    tmp_inode on the orphan list.  In the unlikely case where we crash and
    do a journal recovery, the data blocks belonging to the inode being
    migrated are also represented in the tmp_inode on the orphan list ---
    and so its data blocks will get marked unallocated, and available for
    reuse.
    
    Instead, stop putting the tmp_inode on the oprhan list.  So in the
    case where we crash while migrating the inode, we'll leak an inode,
    which is not a disaster.  It will be easily fixed the next time we run
    fsck, and it's better than potentially having blocks getting claimed
    by two different files, and losing data as a result.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;
    Reviewed-by: Lukas Czerner &lt;lczerner@redhat.com&gt;
    Cc: stable@kernel.org

diff --git a/fs/ext4/migrate.c b/fs/ext4/migrate.c
index 36dfc88ce05b..ff8916e1d38e 100644
--- a/fs/ext4/migrate.c
+++ b/fs/ext4/migrate.c
@@ -437,12 +437,12 @@ int ext4_ext_migrate(struct inode *inode)
 	percpu_down_write(&amp;sbi-&gt;s_writepages_rwsem);
 
 	/*
-	 * Worst case we can touch the allocation bitmaps, a bgd
-	 * block, and a block to link in the orphan list.  We do need
-	 * need to worry about credits for modifying the quota inode.
+	 * Worst case we can touch the allocation bitmaps and a block
+	 * group descriptor block.  We do need need to worry about
+	 * credits for modifying the quota inode.
 	 */
 	handle = ext4_journal_start(inode, EXT4_HT_MIGRATE,
-		4 + EXT4_MAXQUOTAS_TRANS_BLOCKS(inode-&gt;i_sb));
+		3 + EXT4_MAXQUOTAS_TRANS_BLOCKS(inode-&gt;i_sb));
 
 	if (IS_ERR(handle)) {
 		retval = PTR_ERR(handle);
@@ -463,10 +463,6 @@ int ext4_ext_migrate(struct inode *inode)
 	 * Use the correct seed for checksum (i.e. the seed from 'inode').  This
 	 * is so that the metadata blocks will have the correct checksum after
 	 * the migration.
-	 *
-	 * Note however that, if a crash occurs during the migration process,
-	 * the recovery process is broken because the tmp_inode checksums will
-	 * be wrong and the orphans cleanup will fail.
 	 */
 	ei = EXT4_I(inode);
 	EXT4_I(tmp_inode)-&gt;i_csum_seed = ei-&gt;i_csum_seed;
@@ -478,7 +474,6 @@ int ext4_ext_migrate(struct inode *inode)
 	clear_nlink(tmp_inode);
 
 	ext4_ext_tree_init(handle, tmp_inode);
-	ext4_orphan_add(handle, tmp_inode);
 	ext4_journal_stop(handle);
 
 	/*
@@ -503,12 +498,6 @@ int ext4_ext_migrate(struct inode *inode)
 
 	handle = ext4_journal_start(inode, EXT4_HT_MIGRATE, 1);
 	if (IS_ERR(handle)) {
-		/*
-		 * It is impossible to update on-disk structures without
-		 * a handle, so just rollback in-core changes and live other
-		 * work to orphan_list_cleanup()
-		 */
-		ext4_orphan_del(NULL, tmp_inode);
 		retval = PTR_ERR(handle);
 		goto out_tmp_inode;
 	}</pre><hr><pre>commit 11ef08c9eb52a808b8903004cba0733df6902a43
Merge: 1fd95c05d8f7 cc883236b792
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Sep 4 23:46:32 2021 -0400

    Merge branch 'delalloc-buffer-write' into dev
    
    Fix a bug in how we update i_disksize, and the error path in
    inline_data_end.  Finally, drop an unnecessary creation of a journal
    handle which was only needed for inline data, which can give us a
    large performance gain in delayed allocation writes.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --cc fs/ext4/inline.c
index 82bf4ff6be28,d30709d42a27..50a3031bf466
--- a/fs/ext4/inline.c
+++ b/fs/ext4/inline.c
@@@ -733,45 -729,76 +733,83 @@@ convert
  int ext4_write_inline_data_end(struct inode *inode, loff_t pos, unsigned len,
  			       unsigned copied, struct page *page)
  {
- 	int ret, no_expand;
+ 	handle_t *handle = ext4_journal_current_handle();
+ 	int no_expand;
  	void *kaddr;
  	struct ext4_iloc iloc;
+ 	int ret = 0, ret2;
+ 
+ 	if (unlikely(copied &lt; len) &amp;&amp; !PageUptodate(page))
+ 		copied = 0;
  
- 	if (unlikely(copied &lt; len)) {
- 		if (!PageUptodate(page)) {
- 			copied = 0;
+ 	if (likely(copied)) {
+ 		ret = ext4_get_inode_loc(inode, &amp;iloc);
+ 		if (ret) {
+ 			unlock_page(page);
+ 			put_page(page);
+ 			ext4_std_error(inode-&gt;i_sb, ret);
  			goto out;
  		}
- 	}
+ 		ext4_write_lock_xattr(inode, &amp;no_expand);
+ 		BUG_ON(!ext4_has_inline_data(inode));
  
- 	ret = ext4_get_inode_loc(inode, &amp;iloc);
- 	if (ret) {
- 		ext4_std_error(inode-&gt;i_sb, ret);
- 		copied = 0;
- 		goto out;
- 	}
++		/*
++		 * ei-&gt;i_inline_off may have changed since
++		 * ext4_write_begin() called
++		 * ext4_try_to_write_inline_data()
++		 */
++		(void) ext4_find_inline_data_nolock(inode);
 +
- 	ext4_write_lock_xattr(inode, &amp;no_expand);
- 	BUG_ON(!ext4_has_inline_data(inode));
+ 		kaddr = kmap_atomic(page);
+ 		ext4_write_inline_data(inode, &amp;iloc, kaddr, pos, copied);
+ 		kunmap_atomic(kaddr);
+ 		SetPageUptodate(page);
+ 		/* clear page dirty so that writepages wouldn't work for us. */
+ 		ClearPageDirty(page);
  
- 	/*
- 	 * ei-&gt;i_inline_off may have changed since ext4_write_begin()
- 	 * called ext4_try_to_write_inline_data()
- 	 */
- 	(void) ext4_find_inline_data_nolock(inode);
+ 		ext4_write_unlock_xattr(inode, &amp;no_expand);
+ 		brelse(iloc.bh);
  
- 	kaddr = kmap_atomic(page);
- 	ext4_write_inline_data(inode, &amp;iloc, kaddr, pos, len);
- 	kunmap_atomic(kaddr);
- 	SetPageUptodate(page);
- 	/* clear page dirty so that writepages wouldn't work for us. */
- 	ClearPageDirty(page);
+ 		/*
+ 		 * It's important to update i_size while still holding page
+ 		 * lock: page writeout could otherwise come in and zero
+ 		 * beyond i_size.
+ 		 */
+ 		ext4_update_inode_size(inode, pos + copied);
+ 	}
+ 	unlock_page(page);
+ 	put_page(page);
  
- 	ext4_write_unlock_xattr(inode, &amp;no_expand);
- 	brelse(iloc.bh);
- 	mark_inode_dirty(inode);
+ 	/*
+ 	 * Don't mark the inode dirty under page lock. First, it unnecessarily
+ 	 * makes the holding time of page lock longer. Second, it forces lock
+ 	 * ordering of page lock and transaction start for journaling
+ 	 * filesystems.
+ 	 */
+ 	if (likely(copied))
+ 		mark_inode_dirty(inode);
  out:
- 	return copied;
+ 	/*
+ 	 * If we didn't copy as much data as expected, we need to trim back
+ 	 * size of xattr containing inline data.
+ 	 */
+ 	if (pos + len &gt; inode-&gt;i_size &amp;&amp; ext4_can_truncate(inode))
+ 		ext4_orphan_add(handle, inode);
+ 
+ 	ret2 = ext4_journal_stop(handle);
+ 	if (!ret)
+ 		ret = ret2;
+ 	if (pos + len &gt; inode-&gt;i_size) {
+ 		ext4_truncate_failed_write(inode);
+ 		/*
+ 		 * If truncate failed early the inode might still be
+ 		 * on the orphan list; we need to make sure the inode
+ 		 * is removed from the orphan list in that case.
+ 		 */
+ 		if (inode-&gt;i_nlink)
+ 			ext4_orphan_del(NULL, inode);
+ 	}
+ 	return ret ? ret : copied;
  }
  
  struct buffer_head *
diff --cc fs/ext4/inode.c
index 62e9165bc69c,502f60621bad..8204176256c8
--- a/fs/ext4/inode.c
+++ b/fs/ext4/inode.c
@@@ -1404,24 -1392,18 +1395,18 @@@ static int ext4_journalled_write_end(st
  
  	BUG_ON(!ext4_handle_valid(handle));
  
- 	if (inline_data) {
- 		ret = ext4_write_inline_data_end(inode, pos, len,
- 						 copied, page);
- 		if (ret &lt; 0) {
- 			unlock_page(page);
- 			put_page(page);
- 			goto errout;
- 		}
- 		copied = ret;
- 	} else if (unlikely(copied &lt; len) &amp;&amp; !PageUptodate(page)) {
+ 	if (ext4_has_inline_data(inode))
+ 		return ext4_write_inline_data_end(inode, pos, len, copied, page);
+ 
+ 	if (unlikely(copied &lt; len) &amp;&amp; !PageUptodate(page)) {
  		copied = 0;
 -		ext4_journalled_zero_new_buffers(handle, page, from, to);
 +		ext4_journalled_zero_new_buffers(handle, inode, page, from, to);
  	} else {
  		if (unlikely(copied &lt; len))
 -			ext4_journalled_zero_new_buffers(handle, page,
 +			ext4_journalled_zero_new_buffers(handle, inode, page,
  							 from + copied, to);
 -		ret = ext4_walk_page_buffers(handle, page_buffers(page), from,
 -					     from + copied, &amp;partial,
 +		ret = ext4_walk_page_buffers(handle, inode, page_buffers(page),
 +					     from, from + copied, &amp;partial,
  					     write_end_fn);
  		if (!partial)
  			SetPageUptodate(page);</pre><hr><pre>commit 1fd95c05d8f742abfe906620780aee4dbe1a2db0
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Thu Sep 2 11:36:01 2021 -0400

    ext4: add error checking to ext4_ext_replay_set_iblocks()
    
    If the call to ext4_map_blocks() fails due to an corrupted file
    system, ext4_ext_replay_set_iblocks() can get stuck in an infinite
    loop.  This could be reproduced by running generic/526 with a file
    system that has inline_data and fast_commit enabled.  The system will
    repeatedly log to the console:
    
    EXT4-fs warning (device dm-3): ext4_block_to_path:105: block 1074800922 &gt; max in inode 131076
    
    and the stack that it gets stuck in is:
    
       ext4_block_to_path+0xe3/0x130
       ext4_ind_map_blocks+0x93/0x690
       ext4_map_blocks+0x100/0x660
       skip_hole+0x47/0x70
       ext4_ext_replay_set_iblocks+0x223/0x440
       ext4_fc_replay_inode+0x29e/0x3b0
       ext4_fc_replay+0x278/0x550
       do_one_pass+0x646/0xc10
       jbd2_journal_recover+0x14a/0x270
       jbd2_journal_load+0xc4/0x150
       ext4_load_journal+0x1f3/0x490
       ext4_fill_super+0x22d4/0x2c00
    
    With this patch, generic/526 still fails, but system is no longer
    locking up in a tight loop.  It's likely the root casue is that
    fast_commit replay is corrupting file systems with inline_data, and we
    probably need to add better error handling in the fast commit replay
    code path beyond what is done here, which essentially just breaks the
    infinite loop without reporting the to the higher levels of the code.
    
    Fixes: 8016E29F4362 ("ext4: fast commit recovery path")
    Cc: stable@kernel.org
    Cc: Harshad Shirwadkar &lt;harshadshirwadkar@gmail.com&gt;
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/extents.c b/fs/ext4/extents.c
index eb1dd4f024f2..e57019cc3601 100644
--- a/fs/ext4/extents.c
+++ b/fs/ext4/extents.c
@@ -5913,7 +5913,7 @@ void ext4_ext_replay_shrink_inode(struct inode *inode, ext4_lblk_t end)
 }
 
 /* Check if *cur is a hole and if it is, skip it */
-static void skip_hole(struct inode *inode, ext4_lblk_t *cur)
+static int skip_hole(struct inode *inode, ext4_lblk_t *cur)
 {
 	int ret;
 	struct ext4_map_blocks map;
@@ -5922,9 +5922,12 @@ static void skip_hole(struct inode *inode, ext4_lblk_t *cur)
 	map.m_len = ((inode-&gt;i_size) &gt;&gt; inode-&gt;i_sb-&gt;s_blocksize_bits) - *cur;
 
 	ret = ext4_map_blocks(NULL, inode, &amp;map, 0);
+	if (ret &lt; 0)
+		return ret;
 	if (ret != 0)
-		return;
+		return 0;
 	*cur = *cur + map.m_len;
+	return 0;
 }
 
 /* Count number of blocks used by this inode and update i_blocks */
@@ -5973,7 +5976,9 @@ int ext4_ext_replay_set_iblocks(struct inode *inode)
 	 * iblocks by total number of differences found.
 	 */
 	cur = 0;
-	skip_hole(inode, &amp;cur);
+	ret = skip_hole(inode, &amp;cur);
+	if (ret &lt; 0)
+		goto out;
 	path = ext4_find_extent(inode, cur, NULL, 0);
 	if (IS_ERR(path))
 		goto out;
@@ -5992,8 +5997,12 @@ int ext4_ext_replay_set_iblocks(struct inode *inode)
 		}
 		cur = max(cur + 1, le32_to_cpu(ex-&gt;ee_block) +
 					ext4_ext_get_actual_len(ex));
-		skip_hole(inode, &amp;cur);
-
+		ret = skip_hole(inode, &amp;cur);
+		if (ret &lt; 0) {
+			ext4_ext_drop_refs(path);
+			kfree(path);
+			break;
+		}
 		path2 = ext4_find_extent(inode, cur, NULL, 0);
 		if (IS_ERR(path2)) {
 			ext4_ext_drop_refs(path);</pre><hr><pre>commit b33d9f5909c8d30f1429fb9aefbb32760901a023
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Aug 14 10:54:09 2021 -0400

    jbd2: add sparse annotations for add_transaction_credits()
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/jbd2/transaction.c b/fs/jbd2/transaction.c
index 8804e126805f..5347411ae13e 100644
--- a/fs/jbd2/transaction.c
+++ b/fs/jbd2/transaction.c
@@ -223,9 +223,15 @@ static void sub_reserved_credits(journal_t *journal, int blocks)
  * with j_state_lock held for reading. Returns 0 if handle joined the running
  * transaction. Returns 1 if we had to wait, j_state_lock is dropped, and
  * caller must retry.
+ *
+ * Note: because j_state_lock may be dropped depending on the return
+ * value, we need to fake out sparse so ti doesn't complain about a
+ * locking imbalance.  Callers of add_transaction_credits will need to
+ * make a similar accomodation.
  */
 static int add_transaction_credits(journal_t *journal, int blocks,
 				   int rsv_blocks)
+__must_hold(&amp;journal-&gt;j_state_lock)
 {
 	transaction_t *t = journal-&gt;j_running_transaction;
 	int needed;
@@ -238,6 +244,7 @@ static int add_transaction_credits(journal_t *journal, int blocks,
 	if (t-&gt;t_state != T_RUNNING) {
 		WARN_ON_ONCE(t-&gt;t_state &gt;= T_FLUSH);
 		wait_transaction_locked(journal);
+		__acquire(&amp;journal-&gt;j_state_lock); /* fake out sparse */
 		return 1;
 	}
 
@@ -266,10 +273,12 @@ static int add_transaction_credits(journal_t *journal, int blocks,
 			wait_event(journal-&gt;j_wait_reserved,
 				   atomic_read(&amp;journal-&gt;j_reserved_credits) + total &lt;=
 				   journal-&gt;j_max_transaction_buffers);
+			__acquire(&amp;journal-&gt;j_state_lock); /* fake out sparse */
 			return 1;
 		}
 
 		wait_transaction_locked(journal);
+		__acquire(&amp;journal-&gt;j_state_lock); /* fake out sparse */
 		return 1;
 	}
 
@@ -293,6 +302,7 @@ static int add_transaction_credits(journal_t *journal, int blocks,
 					journal-&gt;j_max_transaction_buffers)
 			__jbd2_log_wait_for_space(journal);
 		write_unlock(&amp;journal-&gt;j_state_lock);
+		__acquire(&amp;journal-&gt;j_state_lock); /* fake out sparse */
 		return 1;
 	}
 
@@ -310,6 +320,7 @@ static int add_transaction_credits(journal_t *journal, int blocks,
 		wait_event(journal-&gt;j_wait_reserved,
 			 atomic_read(&amp;journal-&gt;j_reserved_credits) + rsv_blocks
 			 &lt;= journal-&gt;j_max_transaction_buffers / 2);
+		__acquire(&amp;journal-&gt;j_state_lock); /* fake out sparse */
 		return 1;
 	}
 	return 0;
@@ -413,8 +424,14 @@ static int start_this_handle(journal_t *journal, handle_t *handle,
 
 	if (!handle-&gt;h_reserved) {
 		/* We may have dropped j_state_lock - restart in that case */
-		if (add_transaction_credits(journal, blocks, rsv_blocks))
+		if (add_transaction_credits(journal, blocks, rsv_blocks)) {
+			/*
+			 * add_transaction_credits releases
+			 * j_state_lock on a non-zero return
+			 */
+			__release(&amp;journal-&gt;j_state_lock);
 			goto repeat;
+		}
 	} else {
 		/*
 		 * We have handle reserved so we are allowed to join T_LOCKED</pre><hr><pre>commit a5fda11338180db13f3e9eec20c9deda1f7bad72
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Sat Aug 14 10:41:30 2021 -0400

    ext4: fix sparse warnings
    
    Add sparse annotations to suppress false positive context imbalance
    warnings, and use NULL instead of 0 in EXT_MAX_{EXTENT,INDEX}.
    
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/ext4_extents.h b/fs/ext4/ext4_extents.h
index 44e59881a1f0..26435f3a3094 100644
--- a/fs/ext4/ext4_extents.h
+++ b/fs/ext4/ext4_extents.h
@@ -173,10 +173,11 @@ struct partial_cluster {
 #define EXT_MAX_EXTENT(__hdr__)	\
 	((le16_to_cpu((__hdr__)-&gt;eh_max)) ? \
 	((EXT_FIRST_EXTENT((__hdr__)) + le16_to_cpu((__hdr__)-&gt;eh_max) - 1)) \
-					: 0)
+					: NULL)
 #define EXT_MAX_INDEX(__hdr__) \
 	((le16_to_cpu((__hdr__)-&gt;eh_max)) ? \
-	((EXT_FIRST_INDEX((__hdr__)) + le16_to_cpu((__hdr__)-&gt;eh_max) - 1)) : 0)
+	((EXT_FIRST_INDEX((__hdr__)) + le16_to_cpu((__hdr__)-&gt;eh_max) - 1)) \
+					: NULL)
 
 static inline struct ext4_extent_header *ext_inode_hdr(struct inode *inode)
 {
diff --git a/fs/ext4/mballoc.c b/fs/ext4/mballoc.c
index 34670cb63588..665646a12e01 100644
--- a/fs/ext4/mballoc.c
+++ b/fs/ext4/mballoc.c
@@ -2478,6 +2478,12 @@ static bool ext4_mb_good_group(struct ext4_allocation_context *ac,
  * This could return negative error code if something goes wrong
  * during ext4_mb_init_group(). This should not be called with
  * ext4_lock_group() held.
+ *
+ * Note: because we are conditionally operating with the group lock in
+ * the EXT4_MB_STRICT_CHECK case, we need to fake out sparse in this
+ * function using __acquire and __release.  This means we need to be
+ * super careful before messing with the error path handling via "goto
+ * out"!
  */
 static int ext4_mb_good_group_nolock(struct ext4_allocation_context *ac,
 				     ext4_group_t group, int cr)
@@ -2491,8 +2497,10 @@ static int ext4_mb_good_group_nolock(struct ext4_allocation_context *ac,
 
 	if (sbi-&gt;s_mb_stats)
 		atomic64_inc(&amp;sbi-&gt;s_bal_cX_groups_considered[ac-&gt;ac_criteria]);
-	if (should_lock)
+	if (should_lock) {
 		ext4_lock_group(sb, group);
+		__release(ext4_group_lock_ptr(sb, group));
+	}
 	free = grp-&gt;bb_free;
 	if (free == 0)
 		goto out;
@@ -2500,8 +2508,10 @@ static int ext4_mb_good_group_nolock(struct ext4_allocation_context *ac,
 		goto out;
 	if (unlikely(EXT4_MB_GRP_BBITMAP_CORRUPT(grp)))
 		goto out;
-	if (should_lock)
+	if (should_lock) {
+		__acquire(ext4_group_lock_ptr(sb, group));
 		ext4_unlock_group(sb, group);
+	}
 
 	/* We only do this if the grp has never been initialized */
 	if (unlikely(EXT4_MB_GRP_NEED_INIT(grp))) {
@@ -2528,12 +2538,16 @@ static int ext4_mb_good_group_nolock(struct ext4_allocation_context *ac,
 			return ret;
 	}
 
-	if (should_lock)
+	if (should_lock) {
 		ext4_lock_group(sb, group);
+		__release(ext4_group_lock_ptr(sb, group));
+	}
 	ret = ext4_mb_good_group(ac, group, cr);
 out:
-	if (should_lock)
+	if (should_lock) {
+		__acquire(ext4_group_lock_ptr(sb, group));
 		ext4_unlock_group(sb, group);
+	}
 	return ret;
 }
 
@@ -2969,6 +2983,7 @@ int ext4_seq_mb_stats_show(struct seq_file *seq, void *offset)
 }
 
 static void *ext4_mb_seq_structs_summary_start(struct seq_file *seq, loff_t *pos)
+__acquires(&amp;EXT4_SB(sb)-&gt;s_mb_rb_lock)
 {
 	struct super_block *sb = PDE_DATA(file_inode(seq-&gt;file));
 	unsigned long position;
@@ -3041,6 +3056,7 @@ static int ext4_mb_seq_structs_summary_show(struct seq_file *seq, void *v)
 }
 
 static void ext4_mb_seq_structs_summary_stop(struct seq_file *seq, void *v)
+__releases(&amp;EXT4_SB(sb)-&gt;s_mb_rb_lock)
 {
 	struct super_block *sb = PDE_DATA(file_inode(seq-&gt;file));
 
@@ -6275,6 +6291,8 @@ __acquires(bitlock)
 static int ext4_try_to_trim_range(struct super_block *sb,
 		struct ext4_buddy *e4b, ext4_grpblk_t start,
 		ext4_grpblk_t max, ext4_grpblk_t minblocks)
+__acquires(ext4_group_lock_ptr(sb, e4b-&gt;bd_group))
+__releases(ext4_group_lock_ptr(sb, e4b-&gt;bd_group))
 {
 	ext4_grpblk_t next, count, free_count;
 	void *bitmap;</pre><hr><pre>commit a54c4613dac1500b40e4ab55199f7c51f028e848
Author: Theodore Ts'o &lt;tytso@mit.edu&gt;
Date:   Fri Aug 20 23:44:17 2021 -0400

    ext4: fix race writing to an inline_data file while its xattrs are changing
    
    The location of the system.data extended attribute can change whenever
    xattr_sem is not taken.  So we need to recalculate the i_inline_off
    field since it mgiht have changed between ext4_write_begin() and
    ext4_write_end().
    
    This means that caching i_inline_off is probably not helpful, so in
    the long run we should probably get rid of it and shrink the in-memory
    ext4 inode slightly, but let's fix the race the simple way for now.
    
    Cc: stable@kernel.org
    Fixes: f19d5870cbf72 ("ext4: add normal write support for inline data")
    Reported-by: syzbot+13146364637c7363a7de@syzkaller.appspotmail.com
    Signed-off-by: Theodore Ts'o &lt;tytso@mit.edu&gt;

diff --git a/fs/ext4/inline.c b/fs/ext4/inline.c
index 70cb64db33f7..24e994e75f5c 100644
--- a/fs/ext4/inline.c
+++ b/fs/ext4/inline.c
@@ -750,6 +750,12 @@ int ext4_write_inline_data_end(struct inode *inode, loff_t pos, unsigned len,
 	ext4_write_lock_xattr(inode, &amp;no_expand);
 	BUG_ON(!ext4_has_inline_data(inode));
 
+	/*
+	 * ei-&gt;i_inline_off may have changed since ext4_write_begin()
+	 * called ext4_try_to_write_inline_data()
+	 */
+	(void) ext4_find_inline_data_nolock(inode);
+
 	kaddr = kmap_atomic(page);
 	ext4_write_inline_data(inode, &amp;iloc, kaddr, pos, len);
 	kunmap_atomic(kaddr);</pre>
    <div class="pagination">
        <a href='1_4.html'>&lt;&lt;Prev</a><a href='1.html'>1</a><a href='1_2.html'>2</a><a href='1_3.html'>3</a><a href='1_4.html'>4</a><span>[5]</span><a href='1_6.html'>6</a><a href='1_7.html'>7</a><a href='1_8.html'>8</a><a href='1_9.html'>9</a><a href='1_10.html'>10</a><a href='1_11.html'>11</a><a href='1_12.html'>12</a><a href='1_13.html'>13</a><a href='1_14.html'>14</a><a href='1_15.html'>15</a><a href='1_16.html'>16</a><a href='1_17.html'>17</a><a href='1_18.html'>18</a><a href='1_19.html'>19</a><a href='1_20.html'>20</a><a href='1_21.html'>21</a><a href='1_22.html'>22</a><a href='1_23.html'>23</a><a href='1_24.html'>24</a><a href='1_25.html'>25</a><a href='1_26.html'>26</a><a href='1_27.html'>27</a><a href='1_28.html'>28</a><a href='1_29.html'>29</a><a href='1_30.html'>30</a><a href='1_31.html'>31</a><a href='1_32.html'>32</a><a href='1_33.html'>33</a><a href='1_34.html'>34</a><a href='1_35.html'>35</a><a href='1_36.html'>36</a><a href='1_37.html'>37</a><a href='1_38.html'>38</a><a href='1_39.html'>39</a><a href='1_40.html'>40</a><a href='1_41.html'>41</a><a href='1_42.html'>42</a><a href='1_43.html'>43</a><a href='1_44.html'>44</a><a href='1_45.html'>45</a><a href='1_46.html'>46</a><a href='1_47.html'>47</a><a href='1_48.html'>48</a><a href='1_49.html'>49</a><a href='1_50.html'>50</a><a href='1_51.html'>51</a><a href='1_52.html'>52</a><a href='1_53.html'>53</a><a href='1_54.html'>54</a><a href='1_55.html'>55</a><a href='1_56.html'>56</a><a href='1_57.html'>57</a><a href='1_58.html'>58</a><a href='1_59.html'>59</a><a href='1_60.html'>60</a><a href='1_61.html'>61</a><a href='1_62.html'>62</a><a href='1_63.html'>63</a><a href='1_64.html'>64</a><a href='1_65.html'>65</a><a href='1_66.html'>66</a><a href='1_67.html'>67</a><a href='1_68.html'>68</a><a href='1_69.html'>69</a><a href='1_70.html'>70</a><a href='1_71.html'>71</a><a href='1_72.html'>72</a><a href='1_73.html'>73</a><a href='1_74.html'>74</a><a href='1_75.html'>75</a><a href='1_76.html'>76</a><a href='1_77.html'>77</a><a href='1_78.html'>78</a><a href='1_79.html'>79</a><a href='1_80.html'>80</a><a href='1_81.html'>81</a><a href='1_82.html'>82</a><a href='1_83.html'>83</a><a href='1_84.html'>84</a><a href='1_85.html'>85</a><a href='1_86.html'>86</a><a href='1_87.html'>87</a><a href='1_88.html'>88</a><a href='1_89.html'>89</a><a href='1_90.html'>90</a><a href='1_91.html'>91</a><a href='1_92.html'>92</a><a href='1_93.html'>93</a><a href='1_94.html'>94</a><a href='1_95.html'>95</a><a href='1_96.html'>96</a><a href='1_97.html'>97</a><a href='1_98.html'>98</a><a href='1_99.html'>99</a><a href='1_100.html'>100</a><a href='1_101.html'>101</a><a href='1_102.html'>102</a><a href='1_103.html'>103</a><a href='1_104.html'>104</a><a href='1_105.html'>105</a><a href='1_106.html'>106</a><a href='1_107.html'>107</a><a href='1_108.html'>108</a><a href='1_109.html'>109</a><a href='1_110.html'>110</a><a href='1_111.html'>111</a><a href='1_112.html'>112</a><a href='1_113.html'>113</a><a href='1_114.html'>114</a><a href='1_115.html'>115</a><a href='1_116.html'>116</a><a href='1_117.html'>117</a><a href='1_118.html'>118</a><a href='1_119.html'>119</a><a href='1_120.html'>120</a><a href='1_121.html'>121</a><a href='1_122.html'>122</a><a href='1_123.html'>123</a><a href='1_124.html'>124</a><a href='1_125.html'>125</a><a href='1_126.html'>126</a><a href='1_127.html'>127</a><a href='1_128.html'>128</a><a href='1_129.html'>129</a><a href='1_130.html'>130</a><a href='1_131.html'>131</a><a href='1_132.html'>132</a><a href='1_133.html'>133</a><a href='1_134.html'>134</a><a href='1_135.html'>135</a><a href='1_136.html'>136</a><a href='1_137.html'>137</a><a href='1_138.html'>138</a><a href='1_139.html'>139</a><a href='1_140.html'>140</a><a href='1_141.html'>141</a><a href='1_142.html'>142</a><a href='1_143.html'>143</a><a href='1_144.html'>144</a><a href='1_145.html'>145</a><a href='1_146.html'>146</a><a href='1_147.html'>147</a><a href='1_148.html'>148</a><a href='1_149.html'>149</a><a href='1_150.html'>150</a><a href='1_151.html'>151</a><a href='1_152.html'>152</a><a href='1_153.html'>153</a><a href='1_154.html'>154</a><a href='1_6.html'>Next&gt;&gt;</a>
    <div>
</body>
